Common classification algorithms include:
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)





Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron).
v t e
v t e
v t e
Machine learning


Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.










Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.





Common terms:
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
In the data science course that I instruct, we cover most of the data science pipeline but focus especially on machine learning. Besides teaching model evaluation procedures and metrics, we obviously teach the algorithms themselves, primarily for supervised learning.
In the data science course that I instruct, we cover most of the data science pipeline but focus especially on machine learning. Besides teaching model evaluation procedures and metrics, we obviously teach the algorithms themselves, primarily for supervised learning.
