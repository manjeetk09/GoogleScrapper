algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Discriminant analysis.;discriminant_analysis;0.6363636363636364;0.2;1.0;1.8363636363636364;0.0;1;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Nearest neighbors .;nearest_neighbors;0.6363636363636364;0.2;0.4666666666666667;1.3030303030303032;0.0;1;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.quora.com/What-is-the-difference-between-supervised-and-unsupervised-learning-algorithms;Common terms.;common_terms;0.6363636363636364;1.0;1.0;2.6363636363636362;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://en.wikipedia.org/wiki/Supervised_learning;The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k_nearest neighbor algorithm, and Neural Networks .;k_nearest_neighbor_algorithm;1.0;0.09090909090909091;1.0;2.090909090909091;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html; Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.;higher_predictive_power;1.0;0.05263157894736842;1.0;2.052631578947368;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor .;linear_unbiased_predictor;1.0;0.013071895424836602;1.0;2.0130718954248366;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 2 3  Mathematical formulation of LDA dimensionality reduction.;lda_dimensionality_reduction;1.0;0.006535947712418301;1.0;2.0065359477124183;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 6 4 4  Choice of Nearest Neighbors Algorithm.;nearest_neighbors_algorithm;1.0;0.006535947712418301;1.0;2.0065359477124183;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 2 2  Comparison of GPR and Kernel Ridge Regression.;kernel_ridge_regression;1.0;0.006535947712418301;1.0;2.0065359477124183;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 6 3 2  The best linear unbiased prediction .;linear_unbiased_prediction;1.0;0.006535947712418301;1.0;2.0065359477124183;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;venugopal says  August 11, 2015 at 6 05 am Good Summary airticle.;good_summary_airticle;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dr Venugopala Rao says  August 11, 2015 at 6 27 am Super Compilation .;dr_venugopala_rao;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;bernoulli_random_variable;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;maximum_likelihood_estimates;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Glenn Nelson says  September 10, 2015 at 7 48 pm I took the Stanford_Coursera ML class, but have not used it, and I found this to be an incredibly useful summary  I appreciate the real_world analogues, such as your mention of Jezzball  And showing the brief code snips is terrific .;stanford_coursera_ml_class;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;whystatistics says  September 29, 2015 at 10 25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful  Just, can you add Neural Network here in simple terms with example and code .;add_neural_network;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;makes_classification_easy;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;continuous_variable_bound;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;generalized_libear_models;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Bansari Shah says  January 14, 2016 at 6 27 am Thank you   reallu helpful article.;reallu_helpful_article;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Anthony says  February 16, 2016 at 8 39 am Informative and easy to follow  I ve recently started following several pages like this one and this is the best material ive seen yet .;ve_recently_started;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;worst_data_analysts;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;nd says  June 17, 2016 at 7 39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish  Hence you must always compare models, understand residuals profile and how prediction really predicts  In that sense, analysis of data is never ending  In R, use summary, plot and check for assumptions validity  .;good_information_interms;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;nd says  June 17, 2016 at 7 39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish  Hence you must always compare models, understand residuals profile and how prediction really predicts  In that sense, analysis of data is never ending  In R, use summary, plot and check for assumptions validity  .;initial_knowledge_note;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;nd says  June 17, 2016 at 7 39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish  Hence you must always compare models, understand residuals profile and how prediction really predicts  In that sense, analysis of data is never ending  In R, use summary, plot and check for assumptions validity  .;understand_residuals_profile;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;sabarikannan says  June 30, 2016 at 5 35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning  .;anomaly_dection_algorithm;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Namala Santosh Kumar says  July 15, 2016 at 2 33 am Analytics Vidhya   I am loving it.;namala_santosh_kumar;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Mohammed Abdul Kaleem says  July 16, 2016 at 8 21 am Good article  thank you for explaining with python .;mohammed_abdul_kaleem;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Baseer says  August 18, 2016 at 5 11 am Very precise quick tutorial for those who want to gain insight of machine learning.;precise_quick_tutorial;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;sanjiv says  September 8, 2016 at 4 29 am Great article  It would have become even better if you had some test data with each code snippet  Add metrics and hyper parameter tunning for each of these models.;hyper_parameter_tunning;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .;satya_swarup_dani;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Malini Ramamurthy says  October 14, 2016 at 5 53 am Very informative article  For a person new to machine learning, this article gives a good starting point .;good_starting_point;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Raghav says  November 10, 2016 at 8 09 pm All programe has error named Error in model frame default   , data   x,   invalid type for variable  as list  What is that   Gaurav says  December 8, 2016 at 1 54 pm I think that  y_train  is data frame and it cannot be converted directly to list with  as list  command  Try this instead y_train  _ as list   See if this works for you .;error_named_error;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Suman says  December 9, 2016 at 6 43 am Very nice summary  Can you tell how to get machine learning problems for practice  NSS says  January 3, 2017 at 6 20 am Analytics Vidhya has some practice datasets  Check Analytics Vidhya hackathon  Also UCI machine learning repository is a phenomenal place  Google it and enjoy .;machine_learning_problems;1.0;0.0018867924528301887;1.0;2.0018867924528303;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://www.dataschool.io/comparing-supervised-learning-algorithms/;In the data science course that I instruct, we cover most of the data science pipeline but focus especially on machine learning.;data_science_pipeline;0.8787878787878788;0.09090909090909091;1.0;1.9696969696969697;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;proper_statistical_background;0.9393939393939394;0.0018867924528301887;1.0;1.9412807318467697;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Raghav says  November 10, 2016 at 8 09 pm All programe has error named Error in model frame default   , data   x,   invalid type for variable  as list  What is that   Gaurav says  December 8, 2016 at 1 54 pm I think that  y_train  is data frame and it cannot be converted directly to list with  as list  command  Try this instead y_train  _ as list   See if this works for you .;model_frame_default;0.9393939393939394;0.0018867924528301887;1.0;1.9412807318467697;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html; Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.;larger_training_datasets;0.8787878787878788;0.05263157894736842;1.0;1.9314194577352473;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://www.dataschool.io/comparing-supervised-learning-algorithms/;In the data science course that I instruct, we cover most of the data science pipeline but focus especially on machine learning.;data_science;0.8181818181818182;0.09090909090909091;1.0;1.9090909090909092;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/; Linear regression for regression problems.;regression_problems;0.6363636363636364;0.25;1.0;1.8863636363636362;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;normal_distribution_amounts;0.8787878787878788;0.0018867924528301887;1.0;1.880674671240709;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;maximum_likelihood_estimate;0.8787878787878788;0.0018867924528301887;1.0;1.880674671240709;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;logistic_regression_output;0.7979797979793939;0.0018867924528301887;1.0;1.7998665904322242;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/;Some popular examples of supervised machine learning algorithms are.;popular_examples;0.6363636363636364;0.125;1.0;1.7613636363636362;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/; Support vector machines for classification problems.;classification_problems;0.6363636363636364;0.125;1.0;1.7613636363636362;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Supervised learning is a type of machine learning algorithm that uses a known dataset to make predictions.;make_predictions;0.6363636363636364;0.10526315789473684;1.0;1.7416267942583732;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html; The training dataset includes input data and response values.;response_values;0.6363636363636364;0.10526315789473684;1.0;1.7416267942583732;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Col  Dan Sulzinger says  March 1, 2016 at 1 21 am Good Article .;col__dan_sulzinger;0.7272727272727273;0.0018867924528301887;1.0;1.7291595197255574;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;big_flops;0.7272727272727273;0.0018867924528301887;1.0;1.7291595197255574;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;statistical_learning;0.7272727272727273;0.0018867924528301887;1.0;1.7291595197255574;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Suman says  December 9, 2016 at 6 43 am Very nice summary  Can you tell how to get machine learning problems for practice  NSS says  January 3, 2017 at 6 20 am Analytics Vidhya has some practice datasets  Check Analytics Vidhya hackathon  Also UCI machine learning repository is a phenomenal place  Google it and enjoy .;phenomenal_place__google;0.7272727272727273;0.0018867924528301887;1.0;1.7291595197255574;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://en.wikipedia.org/wiki/Supervised_learning;The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k_nearest neighbor algorithm, and Neural Networks .;learning_algorithms;0.6363636363636364;0.09090909090909091;1.0;1.7272727272727273;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html; A test dataset is often used to validate the model.;test_dataset;0.6363636363636364;0.05263157894736842;1.0;1.6889952153110048;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html; Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.;yield_models;0.6363636363636364;0.05263157894736842;1.0;1.6889952153110048;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 4  Support Vector Machines 1 4 1  Classification 1 4 1 1  Multi_class classification 1 4 1 2  Scores and probabilities 1 4 1 3  Unbalanced problems 1 4 2  Regression 1 4 3  Density estimation, novelty detection 1 4 4  Complexity 1 4 5  Tips on Practical Use 1 4 6  Kernel functions 1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel 1 4 7  Mathematical formulation 1 4 7 1  SVC 1 4 7 2  NuSVC 1 4 7 3  SVR 1 4 8  Implementation details.;python_functions;0.6363636363636364;0.0392156862745098;1.0;1.6755793226381461;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 1  Generalized Linear Models 1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity 1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation 1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection 1 1 4  Multi_task Lasso 1 1 5  Elastic Net 1 1 6  Multi_task Elastic Net 1 1 7  Least Angle Regression 1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation 1 1 9  Orthogonal Matching Pursuit 1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD 1 1 11  Logistic regression 1 1 12  Stochastic Gradient Descent _ SGD 1 1 13  Perceptron 1 1 14  Passive Aggressive Algorithms 1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes 1 1 16  Polynomial regression  extending linear models with basis functions.;basis_functions;0.6363636363636364;0.032679738562091505;1.0;1.6690433749257279;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7  Gaussian Processes 1 7 1  Gaussian Process Regression 1 7 2  GPR examples 1 7 2 1  GPR with noise_level estimation 1 7 2 2  Comparison of GPR and Kernel Ridge Regression 1 7 2 3  GPR on Mauna Loa CO2 data 1 7 3  Gaussian Process Classification 1 7 4  GPC examples 1 7 4 1  Probabilistic predictions with GPC 1 7 4 2  Illustration of GPC on the XOR dataset 1 7 4 3  Gaussian process classification on iris dataset 1 7 5  Kernels for Gaussian Processes 1 7 5 1  Gaussian Process Kernel API 1 7 5 2  Basic kernels 1 7 5 3  Kernel operators 1 7 5 4  Radial_basis function kernel 1 7 5 5  Mat rn kernel 1 7 5 6  Rational quadratic kernel 1 7 5 7  Exp_Sine_Squared kernel 1 7 5 8  Dot_Product kernel 1 7 5 9  References 1 7 6  Legacy Gaussian Processes 1 7 6 1  An introductory regression example 1 7 6 2  Fitting Noisy Data 1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor 1 7 6 4  Correlation Models 1 7 6 5  Regression Models 1 7 6 6  Implementation details.;introductory_regression;0.6363636363636364;0.032679738562091505;1.0;1.6690433749257279;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity.;squares_complexity;0.6363636363636364;0.0196078431372549;1.0;1.6559714795008913;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 4 6  Kernel functions 1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel.;rbf_kernel;0.6363636363636364;0.0196078431372549;1.0;1.6559714795008913;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 1 7  Least Angle Regression.;angle_regression;0.6363636363636364;0.013071895424836602;1.0;1.6494355317884728;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 6 1  Unsupervised Nearest Neighbors 1 6 1 1  Finding the Nearest Neighbors 1 6 1 2  KDTree and BallTree Classes.;balltree_classes;0.6363636363636364;0.013071895424836602;1.0;1.6494355317884728;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 4  GPC examples 1 7 4 1  Probabilistic predictions with GPC 1 7 4 2  Illustration of GPC on the XOR dataset 1 7 4 3  Gaussian process classification on iris dataset.;iris_dataset;0.6363636363636364;0.013071895424836602;1.0;1.6494355317884728;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;If you use the software, please consider citing scikit_learn.;citing_scikit_learn;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;User guide.;user_guide;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;PDF documentation.;pdf_documentation;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 2 2  Mathematical formulation of the LDA and QDA classifiers.;qda_classifiers;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 4 3  Density estimation, novelty detection.;novelty_detection;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 4 6 1 2  Using the Gram matrix.;gram_matrix;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 5 3  Stochastic Gradient Descent for sparse data.;sparse_data;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 2 1  GPR with noise_level estimation.;noise_level_estimation;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 4 2  Illustration of GPC on the XOR dataset.;xor_dataset;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 7 6 3 1  The initial assumption.;initial_assumption;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 11 4 4  Controlling the tree size.;tree_size;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 13 1  Removing features with low variance.;low_variance;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Col  Dan Sulzinger says  March 1, 2016 at 1 21 am Good Article .;good_article;0.6363636363636364;0.005660377358490566;1.0;1.642024013722127;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Karthikeyan says  August 11, 2015 at 3 13 am Thank you very much, A Very useful and excellent compilation  I have already bookmarked this page .;excellent_compilation;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Huzefa says  August 11, 2015 at 3 53 pm Hello, Superb information in just one blog  Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated.;superb_information;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;data_frame;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;great_article;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Debashis says  February 16, 2016 at 8 19 am This is such a wonderful article .;wonderful_article;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dung Dinh says  June 17, 2016 at 10 24 am The amazing article  I m new in data analysis  It s very useful and easy to understand  Thanks,.;amazing_article;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Namala Santosh Kumar says  July 15, 2016 at 2 33 am Analytics Vidhya   I am loving it.;analytics_vidhya;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dr Venugopala Rao says  August 11, 2015 at 6 27 am Super Compilation .;super_compilation;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Kishor Basyal says  August 11, 2015 at 7 30 am Wonderful  Really helpful.;kishor_basyal;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Brian Thomas says  August 11, 2015 at 9 24 am Very nicely done  Thanks for this .;brian_thomas;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Tesfaye says  August 11, 2015 at 10 30 am Thank you  Well presented article .;presented_article;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Sudipta Basak says  August 12, 2015 at 3 35 am Enjoyed the simplicity  Thanks for the effort .;sudipta_basak;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Sunil Ray says  August 14, 2015 at 7 36 am Hi All, Thanks for the comment  .;sunil_ray;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;max_likelihoods;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;solve__sunil_ray;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;square_method;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;good_summary;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;estimate;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;solve;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;dataset_provided;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;formula_specification;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;basically_means;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;data_table;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Chris says  August 26, 2015 at 1 01 am You did a Wonderful job  This is really helpful  Thanks .;wonderful_job;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Glenn Nelson says  September 10, 2015 at 7 48 pm I took the Stanford_Coursera ML class, but have not used it, and I found this to be an incredibly useful summary  I appreciate the real_world analogues, such as your mention of Jezzball  And showing the brief code snips is terrific .;code_snips;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Glenn Nelson says  September 10, 2015 at 7 48 pm I took the Stanford_Coursera ML class, but have not used it, and I found this to be an incredibly useful summary  I appreciate the real_world analogues, such as your mention of Jezzball  And showing the brief code snips is terrific .;real_world_analogues;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Glenn Nelson says  September 10, 2015 at 7 48 pm I took the Stanford_Coursera ML class, but have not used it, and I found this to be an incredibly useful summary  I appreciate the real_world analogues, such as your mention of Jezzball  And showing the brief code snips is terrific .;glenn_nelson;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Shankar Pandala says  September 15, 2015 at 12 09 pm This is very easy and helpful than any other courses I have completed  simple  clear  To the point .;shankar_pandala;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;whystatistics says  September 29, 2015 at 10 25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful  Just, can you add Neural Network here in simple terms with example and code .;superb_tutorial;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;whystatistics says  September 29, 2015 at 10 25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful  Just, can you add Neural Network here in simple terms with example and code .;simple_terms;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;whystatistics says  September 29, 2015 at 10 25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful  Just, can you add Neural Network here in simple terms with example and code .;good_examples;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Sayan Putatunda says  November 1, 2015 at 7 00 am Errata _ fit  _ kmeans  5 cluster solution It s a 3 cluster solution .;sayan_putatunda;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;regression;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;great_resource;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;maps_outputs;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;main_aim;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;package_hints;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;extra_step;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Ashish Yelkar says  January 6, 2016 at 6 28 am Very Nice   .;ashish_yelkar;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Bansari Shah says  January 14, 2016 at 6 27 am Thank you   reallu helpful article.;bansari_shah;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;ayushgg92 says  January 22, 2016 at 9 54 am I wanted to know if I can use rattle instead of writing the R code explicitly.;code_explicitly;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Anthony says  February 16, 2016 at 8 39 am Informative and easy to follow  I ve recently started following several pages like this one and this is the best material ive seen yet .;material_ive;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;N colas Robles says  February 18, 2016 at 5 51 am Cool stuff  I just can t get the necessary libraries .;cool_stuff;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;N colas Robles says  February 18, 2016 at 5 51 am Cool stuff  I just can t get the necessary libraries .;colas_robles;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;wizzerd says  February 26, 2016 at 12 09 pm looks sgood article  Do I need any data to do the examples .;sgood_article;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Pansy says  March 8, 2016 at 3 04 pm I have to thank you for this informative summary  Really useful .;informative_summary;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;place_people;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;highly_recommend;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;drew_conway;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;cooking_recipes;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;danger_zone;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;poster_boy;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;making_programmers;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Robin White says  March 15, 2016 at 11 38 pm Great article  It really summarize some of the most important topics on machine learning  But as asked above I would like to present thedevmasters com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends .;great_professors;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Robin White says  March 15, 2016 at 11 38 pm Great article  It really summarize some of the most important topics on machine learning  But as asked above I would like to present thedevmasters com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends .;present_thedevmasters;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Robin White says  March 15, 2016 at 11 38 pm Great article  It really summarize some of the most important topics on machine learning  But as asked above I would like to present thedevmasters com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends .;continue_learning;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Robin White says  March 15, 2016 at 11 38 pm Great article  It really summarize some of the most important topics on machine learning  But as asked above I would like to present thedevmasters com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends .;important_topics;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Robin White says  March 15, 2016 at 11 38 pm Great article  It really summarize some of the most important topics on machine learning  But as asked above I would like to present thedevmasters com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends .;robin_white;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;salman ahmed says  March 19, 2016 at 8 29 am awesome , recommended this article to all my friends.;salman_ahmed;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .;succinct_description;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .;svm_section;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .;important_algorithms;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .;borun_chowdhury;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Isaac says  May 24, 2016 at 2 29 am Thank you for this wonderful article it s proven helpful .;proven_helpful;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Payal gour says  June 11, 2016 at 5 52 am Thank you very much, A Very useful and excellent compilation .;payal_gour;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;nd says  June 17, 2016 at 7 39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish  Hence you must always compare models, understand residuals profile and how prediction really predicts  In that sense, analysis of data is never ending  In R, use summary, plot and check for assumptions validity  .;assumptions_validity;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;nd says  June 17, 2016 at 7 39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish  Hence you must always compare models, understand residuals profile and how prediction really predicts  In that sense, analysis of data is never ending  In R, use summary, plot and check for assumptions validity  .;compare_models;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dung Dinh says  June 17, 2016 at 10 24 am The amazing article  I m new in data analysis  It s very useful and easy to understand  Thanks,.;dung_dinh;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;ankita srivastava says  July 5, 2016 at 8 07 am a very very helpful tutorial  Thanks a lot you guys .;ankita_srivastava;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;ankita srivastava says  July 5, 2016 at 8 07 am a very very helpful tutorial  Thanks a lot you guys .;helpful_tutorial;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Jacques Gouimenou says  August 14, 2016 at 1 57 pm very useful compilation  Thanks .;jacques_gouimenou;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Baseer says  August 18, 2016 at 5 11 am Very precise quick tutorial for those who want to gain insight of machine learning.;gain_insight;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Ali Kazim says  August 28, 2016 at 11 41 pm Very useful and informative  Thanks for sharing it .;ali_kazim;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Denis says  September 4, 2016 at 10 53 am great summary, Thank you.;great_summary;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .;sharing_knowledge;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .;nicely_complied;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .;crystal_clear;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Emerson Moizes says  October 5, 2016 at 10 58 am Perfect  It s exactly what I was looking for  Thanks for the explanation and thanks for sharing your knowledge with us .;emerson_moizes;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Malini Ramamurthy says  October 14, 2016 at 5 53 am Very informative article  For a person new to machine learning, this article gives a good starting point .;malini_ramamurthy;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Anand Rai says  October 21, 2016 at 7 23 pm Good article  thank you for explaining with python    Kareermatrix.;anand_rai;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;RAVI RATHORE says  November 8, 2016 at 1 44 pm hello I have to implement machine learning algorithms in python so could you help me in this  any body provide me the proper code for any algorithm .;ravi_rathore;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;RAVI RATHORE says  November 8, 2016 at 1 44 pm hello I have to implement machine learning algorithms in python so could you help me in this  any body provide me the proper code for any algorithm .;body_provide;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;RAVI RATHORE says  November 8, 2016 at 1 44 pm hello I have to implement machine learning algorithms in python so could you help me in this  any body provide me the proper code for any algorithm .;proper_code;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Raghav says  November 10, 2016 at 8 09 pm All programe has error named Error in model frame default   , data   x,   invalid type for variable  as list  What is that   Gaurav says  December 8, 2016 at 1 54 pm I think that  y_train  is data frame and it cannot be converted directly to list with  as list  command  Try this instead y_train  _ as list   See if this works for you .;converted_directly;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Raghav says  November 10, 2016 at 8 09 pm All programe has error named Error in model frame default   , data   x,   invalid type for variable  as list  What is that   Gaurav says  December 8, 2016 at 1 54 pm I think that  y_train  is data frame and it cannot be converted directly to list with  as list  command  Try this instead y_train  _ as list   See if this works for you .;invalid_type;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Suman says  December 9, 2016 at 6 43 am Very nice summary  Can you tell how to get machine learning problems for practice  NSS says  January 3, 2017 at 6 20 am Analytics Vidhya has some practice datasets  Check Analytics Vidhya hackathon  Also UCI machine learning repository is a phenomenal place  Google it and enjoy .;nice_summary;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;YB says  December 26, 2016 at 2 46 pm Do you have R codes based on caret  NSS says  January 3, 2017 at 6 21 am Yes .;codes_based;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html; Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.;datasets;0.6363636363636364;0.0;1.0;1.6363636363636362;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;dalila;0.6363636363636364;0.0;1.0;1.6363636363636362;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;linear_relationship;0.60606060606;0.0018867924528301887;1.0;1.60794739851283;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;equation_linear;0.60606060606;0.0018867924528301887;1.0;1.60794739851283;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;ramesh says  October 23, 2016 at 12 35 pm Hi Friends, i m new person to these machine learning algorithms  i have some questions   1  we have so many ML algorithms  but how can we choose the algorithms which one is suitable for my data set  2  How does these algorithms works   3  why only these particular algorithms   why not others  .;ml_algorithms;0.6;0.0018867924528301887;1.0;1.6018867924528302;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;parametric_model;0.5757575757581818;0.0018867924528301887;1.0;1.577644368211012;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;nice_summary___huzefa;0.5636363636363636;0.0018867924528301887;1.0;1.5655231560891938;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Shankar Pandala says  September 15, 2015 at 12 09 pm This is very easy and helpful than any other courses I have completed  simple  clear  To the point .;completed__simple__clear;0.5636363636363636;0.0018867924528301887;1.0;1.5655231560891938;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;introduces_bias;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;linear_model;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;simple_point;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;fact_wrong;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;irresponsible_article;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .;points_lying;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .;support_vectors;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Malini Ramamurthy says  October 14, 2016 at 5 53 am Very informative article  For a person new to machine learning, this article gives a good starting point .;informative_article;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;big_data___george;0.5272727272727272;0.0018867924528301887;1.0;1.5291595197255574;0.0;0;Wed Jun 14 11:23:01 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;underlying_assumption;0.5151515151509091;0.0018867924528301887;1.0;1.5170383076037393;0.0;0;Wed Jun 14 11:23:01 IST 2017
genetic algorithm for supervised learning;https://www.quora.com/Is-genetic-algorithm-considered-a-supervised-or-non-supervised-algorithm;Supervised and unsupervised are related to learning, which may use any search or optimization tool like genetic algorithm, gradient based algorithm etc.;gradient_based_algorithm;0.9393939393939394;0.125;0.3333333333333333;1.3977272727272727;0.0;0;Wed Jun 14 11:23:01 IST 2017
genetic algorithm for supervised learning;https://news.ycombinator.com/item%3Fid%3D11936908;Supervised learning algorithms are ones for which we have some known labels on our inputs , whereas in unsupervised learning we don t have any known outputs.;supervised_learning_algorithms;0.9393939393939394;0.08333333333333333;0.3333333333333333;1.356060606060606;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html; Rebentrost,  Quantum algorithms for supervised and unsupervised machine learning,  arXiv.;unsupervised_machine_learning;1.0;0.06666666666666667;0.2;1.2666666666666666;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;https://en.wikipedia.org/wiki/Quantum_machine_learning; Quantum machine learning algorithms can use the advantages of quantum computation in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a quantum computer.;developing_efficient_implementations;1.0;0.038461538461538464;0.2;1.2384615384615385;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;https://en.wikipedia.org/wiki/Quantum_machine_learning; On the other hand, one can apply classical methods of machine learning to analyse quantum systems.;analyse_quantum_systems;1.0;0.038461538461538464;0.2;1.2384615384615385;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html; What Quantum Computing Means to Data Mining explains the most relevant concepts of machine learning, quantum mechanics, and quantum information theory, and contrasts classical learning algorithms to their quantum counterparts.;quantum_computing_means;0.9393939393939394;0.06666666666666667;0.2;1.206060606060606;0.0;0;Wed Jun 14 11:23:01 IST 2017
spectral algorithms for supervised learning;http://www.cs.columbia.edu/~scohen/naacl13tutorial/; We will review the basic algorithms underlying CCA, give some formal results giving guarantees for latent_variable models and also describe how they have been applied recently to learning lexical representations from large quantities of unlabeled data.;learning_lexical_representations;1.0;0.06896551724137931;0.06666666666666667;1.1356321839080459;0.0;0;Wed Jun 14 11:23:01 IST 2017
spectral algorithms for supervised learning;http://www.cs.columbia.edu/~scohen/naacl13tutorial/; With CCA, two or more views of the data are created, and they are all projected into a lower dimensional space which maximizes the correlation between the views.;lower_dimensional_space;1.0;0.034482758620689655;0.06666666666666667;1.1011494252873564;0.0;0;Wed Jun 14 11:23:01 IST 2017
genetic algorithm for supervised learning;https://www.quora.com/Is-genetic-algorithm-considered-a-supervised-or-non-supervised-algorithm;Supervised and unsupervised are related to learning, which may use any search or optimization tool like genetic algorithm, gradient based algorithm etc.;optimization_tool;0.6363636363636364;0.125;0.3333333333333333;1.0946969696969697;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; The related fields to SA that attracted researchers recently are discussed.;attracted_researchers_recently;1.0;0.024390243902439025;0.06666666666666667;1.0910569105691057;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html; What Quantum Computing Means to Data Mining explains the most relevant concepts of machine learning, quantum mechanics, and quantum information theory, and contrasts classical learning algorithms to their quantum counterparts.;quantum_counterparts;0.7272727272727273;0.06666666666666667;0.2;0.9939393939393939;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html; What Quantum Computing Means to Data Mining explains the most relevant concepts of machine learning, quantum mechanics, and quantum information theory, and contrasts classical learning algorithms to their quantum counterparts.;relevant_concepts;0.6363636363636364;0.06666666666666667;0.2;0.9030303030303031;0.0;0;Wed Jun 14 11:23:01 IST 2017
quantum algorithms for supervised and unsupervised machine learning;https://en.wikipedia.org/wiki/Quantum_machine_learning; Most generally, one can consider situations wherein both the learning device and the system under study are fully quantum.;fully_quantum;0.6363636363636364;0.038461538461538464;0.2;0.8748251748251747;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550;Sentiment Analysis is an ongoing field of research in text mining field.;ongoing_field;0.7272727272727273;0.024390243902439025;0.06666666666666667;0.818329637841833;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; These articles are categorized according to their contributions in the various SA techniques.;sa_techniques;0.6363636363636364;0.04878048780487805;0.06666666666666667;0.7518107908351811;0.0;0;Wed Jun 14 11:23:01 IST 2017
spectral algorithms for supervised learning;http://www.cs.columbia.edu/~scohen/naacl13tutorial/; CCA is an early method from statistics for dimensionality reduction.;dimensionality_reduction;0.6363636363636364;0.034482758620689655;0.06666666666666667;0.7375130616509926;1.0;0;Wed Jun 14 11:23:01 IST 2017
spectral algorithms for supervised learning;http://www.cs.columbia.edu/~scohen/naacl13tutorial/; We will review the basic algorithms underlying CCA, give some formal results giving guarantees for latent_variable models and also describe how they have been applied recently to learning lexical representations from large quantities of unlabeled data.;large_quantities;0.6363636363636364;0.034482758620689655;0.06666666666666667;0.7375130616509926;0.0;0;Wed Jun 14 11:23:01 IST 2017
spectral algorithms for supervised learning;http://www.cs.columbia.edu/~scohen/naacl13tutorial/; This idea of learning lexical representations can be extended further, where unlabeled data is used to learn underlying representations which are subsequently used as additional information for supervised training.;supervised_training;0.6363636363636364;0.034482758620689655;0.06666666666666667;0.7375130616509926;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; SA is the computational treatment of opinions, sentiments and subjectivity of text.;computational_treatment;0.6363636363636364;0.024390243902439025;0.06666666666666667;0.727420546932742;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; Many recently proposed algorithms  enhancements and various SA applications are investigated and presented briefly in this survey.;sa_applications;0.6363636363636364;0.024390243902439025;0.06666666666666667;0.727420546932742;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; The main target of this survey is to give nearly full image of SA techniques and the related fields with brief details.;full_image;0.6363636363636364;0.024390243902439025;0.06666666666666667;0.727420546932742;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas.;sophisticated_categorizations;0.6363636363636364;0.024390243902439025;0.06666666666666667;0.727420546932742;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas.;main_contributions;0.6363636363636364;0.024390243902439025;0.06666666666666667;0.727420546932742;0.0;0;Wed Jun 14 11:23:01 IST 2017
supervised learning algorithms for sentiment analysis;http://www.sciencedirect.com/science/article/pii/S2090447914000550; The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas.;recent_trend;0.6363636363636364;0.024390243902439025;0.06666666666666667;0.727420546932742;0.0;0;Wed Jun 14 11:23:01 IST 2017
