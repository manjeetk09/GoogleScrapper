Google s self driving cars and robots get a lot of press  but the company s real future is in machine learning  the technology that enables computers to get smarter and more personal.
0.741: (Google; s; self driving cars and robots)
0.592: (real future; is learning; the technology that enables computers)
0.567: (real future; s in; the company)
0.564: (computers to get smarter and more personal; be enables by; the technology)
0.434: (the technology that enables computers; be learning in; machine)
0.407: (real future; is learning the technology that enables computers to get smarter and more personal in; machine)

   Eric Schmidt  Google Chairman .
No extractions found.

We are probably living in the most defining period of human history.
0.715: (We; are probably living in; the most defining period of human history)

 The period when computing moved from large mainframes to PCs to cloud.
0.908: (computing; moved from; large mainframes)
0.882: (computing; moved to; PCs)

 But what makes it defining is not what has happened  but what is coming our way in years to come.
0.771: (our way; is coming in; years)

What makes this period exciting for some one like me is the democratization of the tools and techniques  which followed the boost in computing.
No extractions found.

 Today  as a data scientist  I can build data crunching machines with complex algorithms for a few dollors per hour.
0.772: (I; can build; data crunching machines with complex algorithms for a few dollors per hour)
0.701: (data; crunching machines with complex algorithms for; a few dollors)
0.643: (machines; be crunching for; a few dollors)

 But  reaching here wasn t easy  I had my dark days and nights.
0.61: (I; had; my dark days and nights)
0.397: (t; had; my dark days and nights)

.
No extractions found.

The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world.
0.821: (The idea; is to simplify; the journey of aspiring data scientists)

 Through this guide  I will enable you to work on machine learning problems and gain from experience.
0.891: (you; to work on; machine learning problems and gain)
0.473: (I; will enable through; this guide)

 I am providing a high level understanding about various machine learning algorithms along with R   Python codes to run them.
0.6: (R Python codes; to run; them)
0.582: (I; am providing; a high level understanding)

 These should be sufficient to get your hands dirty.
No extractions found.

.
No extractions found.

I have deliberately skipped the statistics behind these techniques  as you don t need to understand them at the start.
0.724: (you; don; t need to understand them at the start)
0.707: (I; have deliberately skipped; the statistics)[enabler=as you don t need to understand them at the start]
0.601: (them; to be understand at; the start)

 So  if you are looking for statistical understanding of these algorithms  you should look elsewhere.
0.181: (you; are looking for; statistical understanding of these algorithms you should look elsewhere)

 But  if you are looking to equip yourself to start building machine learning project  you are in for a treat.
0.169: (you; to equip; yourself)
0.109: (you; are looking to equip; yourself)

 .
No extractions found.

 .
No extractions found.

How it works  This algorithm consist of a target   outcome variable  or dependent variable  which is to be predicted from a given set of predictors  independent variables .
0.83: (This algorithm; consist of; a target outcome)

 Using these set of variables  we generate a function that map inputs to desired outputs.
0.739: (we; generate; a function)

 The training process continues until the model achieves a desired level of accuracy on the training data.
0.774: (the model; achieves; a desired level of accuracy)

 Examples of Supervised Learning  Regression  Decision Tree  Random Forest  KNN  Logistic Regression etc.
No extractions found.

 .
No extractions found.

How it works  In this algorithm  we do not have any target or outcome variable to predict   estimate.
0.711: (we; do not have; any target or outcome variable)
0.679: (it; works in; this algorithm)

  It is used for clustering population in different groups  which is widely used for segmenting customers in different groups for specific intervention.
0.898: (It; is used in; different groups which is widely used for segmenting customers in different groups for specific intervention)
0.891: (It; is used for; clustering population)
0.707: (customers; be segmenting in; different groups)
0.262: (different; be groups for; specific intervention)

 Examples of Unsupervised Learning  Apriori algorithm  K means.
No extractions found.

 .
No extractions found.

How it works   Using this algorithm  the machine is trained to make specific decisions.
0.581: (it; works Using; this algorithm the machine is trained to make specific decisions)
0.564: (the machine; is trained in; this algorithm)

 It works this way  the machine is exposed to an environment where it trains itself continually using trial and error.
0.904: (the machine; is exposed to; an environment where it trains itself continually using trial and error)
0.692: (It; works; this way the machine is exposed to an environment)
0.643: (itself; be trains by; an environment)
0.564: (the machine; is exposed in; this way)
0.561: (it; trains; itself)
0.555: (it; trains itself continually using; trial and error)

 This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions.
0.853: (This machine; tries to capture; the best possible knowledge to make accurate business decisions)
0.823: (This machine; learns from; past experience)

 Example of Reinforcement Learning  Markov Decision Process.
No extractions found.

Here is the list of commonly used machine learning algorithms.
No extractions found.

 These algorithms can be applied to almost any data problem .
0.919: (These algorithms; can be applied to; any data problem)

It is used to estimate real values  cost of houses  number of calls  total sales etc.
0.734: (It; to estimate; real values cost of houses number of calls total sales etc)

  based on continuous variable s .
No extractions found.

 Here  we establish relationship between independent and dependent variables by fitting a best line.
0.637: (we; Here establish; relationship)

 This best fit line is known as regression line and represented by a linear equation Y  a  X   b.
0.9: (This best fit line; is known as; regression line)
0.871: (This best fit line; be represented by; a linear equation Y a X b)

The best way to understand linear regression is to relive this experience of childhood.
0.833: (The best way to understand linear regression; is to relive; this experience of childhood)

 Let us say  you ask a child in fifth grade to arrange people in his class by increasing order of weight  without asking them their weights  What do you think the child will do  He   she would likely look  visually analyze  at the height and build of people and arrange them using a combination of these visible parameters.
0.876: (you; ask a child in; fifth grade)[attrib=us say]
0.84: (you; ask a child to arrange people by; increasing order of weight)[attrib=us say]
0.79: (you; ask; a child)[attrib=us say]
0.778: (people; to be arrange by; increasing order of weight)
0.743: (you; ask a child to arrange people in; his class)[attrib=us say]
0.719: (you; ask a child to arrange; people)[attrib=us say]
0.658: (people; to be arrange in; his class)
0.643: (the child; will do; He)

 This is linear regression in real life  The child has actually figured out that height and build would be correlated to the weight by a relationship  which looks like the equation above.
0.887: (height; would be correlated by; a relationship which looks like the equation above)
0.886: (height; would be correlated to; the weight)
0.711: (a relationship; looks like; the equation)

In this equation .
No extractions found.

These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.
0.654: (a and b; are derived in; These coefficients)

Look at the below example.
No extractions found.

 Here we have identified the best fit line having linear equation y 0.
0.639: (we; Here have identified; the best fit line having linear equation y)

2811x 13.
No extractions found.

9.
No extractions found.

 Now using this equation  we can find the weight  knowing the height of a person.
0.772: (we; can find; the weight knowing the height of a person)

.
No extractions found.

Linear Regression is of mainly two types  Simple Linear Regression and Multiple Linear Regression.
0.886: (Linear Regression; is of; two types Simple Linear Regression and Multiple Linear Regression)

 Simple Linear Regression is characterized by one independent variable.
0.914: (Simple Linear Regression; is characterized by; one independent variable)

 And  Multiple Linear Regression as the name suggests  is characterized by multiple  more than 1  independent variables.
No extractions found.

 While finding best fit line  you can fit a polynomial or curvilinear regression.
0.825: (you; can fit; a polynomial or curvilinear regression)

 And these are known as polynomial or curvilinear regression.
No extractions found.

Python Code.
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

Don t get confused by its name  It is a classification not a regression algorithm.
0.807: (Don t; get confused by; its name It is a classification not a regression algorithm)
0.539: (It; is; a classification not)

 It is used to estimate discrete values   Binary values like 0 1  yes no  true false   based on given set of independent variable s .
0.698: (It; to estimate; discrete values Binary values)

 In simple words  it predicts the probability of occurrence of an event by fitting data to a logit function.
0.773: (it; predicts; the probability of occurrence of an event)
0.674: (data; be fitting to; a logit function)
0.223: (it; predicts the probability of occurrence of an event by fitting data to a logit function in; simple words)

 Hence  it is also known as logit regression.
0.838: (it; Hence is also known as; logit regression)

 Since  it predicts the probability  its output values lies between 0 and 1  as expected .
0.884: (its output values; lies; between 0 and 1)
0.584: (it; predicts; the probability its output values lies between 0 and 1 as expected)
0.548: (between 0 and 1; be lies by; the probability)

Again  let us try and understand this through a simple example.
0.757: (us; understand this through; a simple example)

Let s say your friend gives you a puzzle to solve.
0.893: (your friend; gives you; a puzzle to solve)[attrib=s say]

 There are only 2 outcome scenarios   either you solve it or you don t.
0.77: (you; don; t)
0.738: (you; solve; it)

 Now imagine  that you are being given wide range of puzzles   quizzes in an attempt to understand which subjects you are good at.
0.633: (you; are being given; wide range of puzzles)

 The outcome to this study would be something like this   if you are given a trignometry based tenth grade problem  you are 70  likely to solve it.
0.788: (The outcome; would be; something)
0.701: (you; to solve; it)
0.372: (something; be The outcome to; this study)
0.155: (you; are given; based tenth grade problem you are 70 likely to solve it)

 On the other hand  if it is grade fifth history question  the probability of getting an answer is only 30 .
0.751: (the probability getting an answer; is only; 30)
0.114: (it; is; grade fifth history question the probability of getting an answer is only 30)

 This is what Logistic Regression provides you.
0.641: (Logistic Regression; provides; you)

Coming to the math  the log odds of the outcome is modeled as a linear combination of the predictor variables.
0.949: (the log odds of the outcome; is modeled as; a linear combination of the predictor variables)
0.621: (the log odds of the outcome; is modeled in; the math)

Above  p is the probability of presence of the characteristic of interest.
0.609: (the probability of presence of the characteristic of interest; is above; p)

 It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors  like in ordinary regression .
0.807: (It; chooses; parameters that maximize the likelihood)

Now  you may ask  why take a log  For the sake of simplicity  let s just say that this is one of the best mathematical way to replicate a step function.
0.534: (the sake of simplicity; let; s)

 I can go in more details  but that will beat the purpose of this article.
0.724: (I; can go in; more details)

Python Code.
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

There are many different steps that could be tried in order to improve the model .
No extractions found.

 .
No extractions found.

This is one of my favorite algorithm and I use it quite frequently.
0.381: (I; use quite frequently; it)
0.379: (I; is; one of my favorite algorithm)

 It is a type of supervised learning algorithm that is mostly used for classification problems.
0.93: (It; is a type of; supervised learning algorithm that is mostly used for classification problems)
0.634: (It; is; a type of supervised learning algorithm)

 Surprisingly  it works for both categorical and continuous dependent variables.
0.729: (it; Surprisingly works for; categorical and continuous dependent variables)

 In this algorithm  we split the population into two or more homogeneous sets.
0.84: (we; split the population into; two or more homogeneous sets)
0.779: (the population; be split into; two or more homogeneous sets)
0.704: (we; split; the population)
0.66: (we; split the population in; this algorithm)
0.565: (the population; be split in; this algorithm)

 This is done based on most significant attributes  independent variables to make as distinct groups as possible.
0.262: (distinct; be groups as; possible)

 For more details  you can read  Decision Tree Simplified.
0.813: (you; can read; Decision Tree Simplified)

.
No extractions found.

source  statsexchange.
No extractions found.

In the image above  you can see that population is classified into four different groups based on multiple attributes to identify  if they will play or not .
0.937: (that population; is classified into; four different groups based on multiple attributes)[enabler=if they will play or not;attrib=you can see]
0.732: (four different groups; be based on; multiple attributes to identify)

 To split the population into different heterogeneous groups  it uses various techniques like Gini  Information Gain  Chi square  entropy.
0.785: (the population; To be split into; different heterogeneous groups it uses various techniques)
0.71: (it; uses; various techniques)
0.564: (various techniques; be uses by; different heterogeneous groups)
0.348: (heterogeneous; be groups like; Gini Information Gain Chi square entropy)
0.25: (different; be heterogeneous groups like; Gini Information Gain Chi square entropy)

The best way to understand how decision tree works  is to play Jezzball   a classic game from Microsoft  image below .
0.797: (The best way to understand how decision tree works; is to play Jezzball; a classic game)
0.751: (The best way to understand how decision tree works; is to play Jezzball a classic game from; Microsoft image)
0.711: (The best way to understand how decision tree works; to play Jezzball a classic game from; Microsoft image)
0.644: (a classic game; to be play Jezzball from; Microsoft image)

 Essentially  you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.
0.869: (maximum area; gets cleared off with; the balls)
0.826: (walls; gets cleared off with; the balls)
0.658: (you; Essentially have; a room)

.
No extractions found.

So  every time you split the room with a wall  you are trying to create 2 different populations with in the same room.
0.919: (you; So split the room with; a wall you are trying to create 2 different populations with in the same room)
0.841: (you; So split; the room)
0.83: (you; to create; 2 different populations)
0.814: (the room; be So split with; a wall you are trying to create 2 different populations with in the same room)
0.746: (you; are trying to create; 2 different populations)
0.657: (you; So split the room in; every time)

 Decision trees work in very similar fashion by dividing a population in as different groups as possible.
0.842: (Decision trees; work in; similar fashion)
0.262: (different; be groups as; possible)

More  Simplified Version of Decision Tree Algorithms.
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

It is a classification method.
0.611: (It; is; a classification method)

 In this algorithm  we plot each data item as a point in n dimensional space  where n is number of features you have  with the value of each feature being the value of a particular coordinate.
0.876: (you; have with; the value of each feature)
0.816: (n; is number of; features)
0.69: (we; be plot each data item as; a point)
0.509: (n; is; number of features you have with the value of each feature)
0.414: (we; be plot each data item in; this algorithm)

For example  if we only had two features like Height and Hair length of an individual  we d first plot these two variables in two dimensional space where each point has two co ordinates  these co ordinates are known as Support Vectors .
0.902: (these co ordinates; are known as; Support Vectors)
0.68: (each point; has; two co ordinates)
0.618: (we; d; first plot)
0.564: (two co ordinates; be has by; two dimensional space)
0.441: (two features we d first plot these two variables in two dimensional space; be only had for; example)
0.121: (we; only had; two features we d first plot these two variables in two dimensional space)
0.011: (we; only had two features like Height and Hair length of an individual we d first plot these two variables in two dimensional space where each point has two co ordinates these co ordinates are known as Support Vectors for; example)

.
No extractions found.

Now  we will find some line that splits the data between the two differently classified groups of data.
0.727: (we; Now will find; some line that splits the data between the two differently classified groups of data)
0.655: (the data; be splits by; some line)

 This will be the line such that the distances from the closest point in each of the two groups will be farthest away.
No extractions found.

.
No extractions found.

In the example shown above  the line which splits the data into two differently classified groups is the black line  since the two closest points are the farthest apart from the line.
0.691: (the line; splits the data into; two differently classified groups)
0.643: (the farthest; is the black line since; the two closest points)
0.564: (the data; be splits by; the line)

 This line is our classifier.
0.645: (This line; is; our classifier)

 Then  depending on where the testing data lands on either side of the line  that s what class we can classify the new data as.
0.711: (we; can classify; the new data)

More  Simplified Version of Support Vector Machine.
No extractions found.

Think of this algorithm as playing JezzBall in n dimensional space.
0.695: (JezzBall; be playing in; dimensional space)

 The tweaks in the game are .
No extractions found.

 .
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

It is a classification technique based on Bayes  theorem with an assumption of independence between predictors.
0.807: (a classification technique; be based on; Bayes theorem)
0.732: (It; is a classification technique on; Bayes theorem)
0.716: (It; is; a classification technique based on Bayes theorem with an assumption of independence between predictors)

 In simple terms  a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.
0.802: (the presence of a particular feature; is unrelated to; the presence of any other feature)
0.634: (the presence of a particular feature; is; unrelated)
0.494: (unrelated; be the presence of; a particular feature)

 For example  a fruit may be considered to be an apple if it is red  round  and about 3 inches in diameter.
0.778: (a fruit; may be considered; an apple)[enabler=if it is red round and about 3 inches in diameter]
0.46: (a fruit; may be considered for; example)[enabler=if it is red round and about 3 inches in diameter]
0.115: (it; is; red round and about 3 inches)
0.094: (it; be about 3 inches in; diameter)

 Even if these features depend on each other or upon the existence of the other features  a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.
0.527: (this fruit; is; an apple)
0.132: (these features; Even depend on; each other or upon)

Naive Bayesian model is easy to build and particularly useful for very large data sets.
0.746: (Naive Bayesian model; is; easy)

 Along with simplicity  Naive Bayes is known to outperform even highly sophisticated classification methods.
No extractions found.

Bayes theorem provides a way of calculating posterior probability P c x  from P c   P x  and P x c .
0.845: (Bayes theorem; provides; a way calculating posterior probability P c x from P c P x and P x c)

 Look at the equation below .
No extractions found.

Here .
No extractions found.

Example  Let s understand it using an example.
0.557: (s; understand; it)
0.555: (s; understand it using; an example)

 Below I have a training data set of weather and corresponding target variable  Play .
0.626: (I; have; a training data set of weather and corresponding target variable Play)

 Now  we need to classify whether players will play or not based on weather condition.
No extractions found.

 Let s follow the below steps to perform it.
No extractions found.

Step 1  Convert the data set to frequency table.
0.793: (the data; be set to; frequency table)

Step 2  Create Likelihood table by finding the probabilities like Overcast probability   0.
No extractions found.

29 and probability of playing is 0.
No extractions found.

64.
No extractions found.

.
No extractions found.

Step 3  Now  use Naive Bayesian equation to calculate the posterior probability for each class.
0.708: (the posterior probability; to be calculate for; each class)
0.609: (3; to calculate; the posterior probability)
0.549: (3; Now use Naive Bayesian equation to calculate; the posterior probability)
0.534: (3; Now use; Naive Bayesian equation)
0.432: (3; Now use Naive Bayesian equation to calculate the posterior probability for; each class)

 The class with the highest posterior probability is the outcome of prediction.
0.93: (The class; is the outcome of; prediction)
0.778: (The class; is; the outcome of prediction)
0.441: (the outcome of prediction; be The class with; the highest posterior probability)

Problem  Players will pay if weather is sunny  is this statement is correct .
0.704: (this statement; is; correct)
0.093: (weather; is; sunny)

We can solve it using above discussed method  so P Yes   Sunny    P  Sunny   Yes    P Yes    P  Sunny .
0.609: (We; can solve it using above; discussed method)
0.571: (We; can solve; it)

Here we have P  Sunny  Yes    3 9   0.
0.601: (we; Here have; P Sunny Yes 3 9)

33  P Sunny    5 14   0.
No extractions found.

36  P  Yes   9 14   0.
No extractions found.

64.
No extractions found.

Now  P  Yes   Sunny    0.
No extractions found.

33   0.
No extractions found.

64   0.
No extractions found.

36   0.
No extractions found.

60  which has higher probability.
0.639: (higher probability; be has by; 60)

Naive Bayes uses a similar method to predict the probability of different class based on various attributes.
0.812: (Naive Bayes; uses; a similar method to predict the probability of different class)
0.705: (different class; be based on; various attributes)

 This algorithm is mostly used in text classification and with problems having multiple classes.
0.948: (This algorithm; is mostly used in; text classification and with problems having multiple classes)
0.934: (This algorithm; is mostly used with; text classification and with problems having multiple classes)

R Code.
No extractions found.

 .
No extractions found.

It can be used for both classification and regression problems.
0.875: (It; can be used for; classification and regression problems)

 However  it is more widely used in classification problems in the industry.
0.858: (it; However is more widely used in; classification problems)

 K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors.
0.734: (K nearest neighbors; is; a simple algorithm)
0.586: (stores; be all available cases new cases by; a majority vote of its k neighbors)

 The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.
0.841: (The case; being assigned to; the class)
0.769: (its K nearest neighbors; be measured by; a distance function)

These distance functions can be Euclidean  Manhattan  Minkowski and Hamming distance.
0.762: (These distance functions; can be; Euclidean Manhattan Minkowski and Hamming distance)

 First three functions are used for continuous function and fourth one  Hamming  for categorical variables.
0.931: (three functions; First are used for; continuous function and fourth one Hamming for categorical variables)
0.627: (continuous function and fourth one; Hamming for; categorical variables)

 If K   1  then the case is simply assigned to the class of its nearest neighbor.
0.206: (K 1; is simply assigned to; the class of its nearest neighbor)

 At times  choosing K turns out to be a challenge while performing KNN modeling.
No extractions found.

More  Introduction to k nearest neighbors   Simplified.
No extractions found.

.
No extractions found.

KNN can easily be mapped to our real lives.
0.81: (KNN; can easily be mapped to; our real lives)

 If you want to learn about a person  of whom you have no information  you might like to find out about his close friends and the circles he moves in and gain access to his her information .
0.855: (you; might like to find out about; his close friends and the circles he moves in)
0.598: (you; might like to gain; access)
0.496: (you; to gain; access)
0.223: (you; want to learn about; a person of whom)

Things to consider before selecting KNN .
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

It is a type of unsupervised algorithm which  solves the clustering problem.
0.905: (It; is a type of; unsupervised algorithm which solves the clustering problem)
0.634: (It; is; a type of unsupervised algorithm)
0.564: (the clustering problem; be solves by; unsupervised algorithm)

 Its procedure follows a simple and easy  way to classify a given data set through a certain number of  clusters  assume k clusters .
0.809: (a given data; be set through; a certain number of clusters)
0.756: (a simple and easy way to classify a given data; assume; k clusters)[attrib=Its procedure follows]

 Data points inside a cluster are homogeneous and heterogeneous to peer groups.
0.837: (Data; points inside; a cluster)

Remember figuring out shapes from ink blots  k means is somewhat similar this activity.
0.675: (shapes; be figuring out from; ink blots)

 You look at the shape and spread to decipher how many different clusters   population are present .
0.843: (You; look at; the shape)

.
No extractions found.

How K means forms cluster .
0.625: (K; means; forms cluster)

How to determine value of K .
No extractions found.

In K means  we have clusters and each cluster has its own centroid.
0.658: (each cluster; has; its own centroid)
0.451: (we; have each cluster has its own centroid; clusters)

 Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster.
0.899: (Sum of square of difference; constitutes within; sum of square value)
0.811: (Sum of square of difference; constitutes for; that cluster)

 Also  when the sum of square values for all the clusters are added  it becomes total within sum of square value for the cluster solution.
0.824: (it; becomes total within; sum of square value)
0.692: (it; becomes total for; the cluster solution)
0.685: (it; becomes; total)

We know that as the number of cluster increases  this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k  and then much more slowly after that.
0.814: (this value; keeps on; decreasing)[attrib=We know]
0.688: (this value; keeps as; the number of cluster increases)[attrib=We know]
0.2: (you; plot; the result you may see that the sum of squared distance decreases sharply up to some value of k and then much more slowly after that)

 Here  we can find the optimum number of cluster.
0.697: (we; Here can find; the optimum number of cluster)

.
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

Random Forest is a trademark term for an ensemble of decision trees.
0.943: (Random Forest; is a trademark term for; an ensemble of decision trees)
0.764: (Random Forest; is; a trademark term)

 In Random Forest  we ve collection of decision trees  so known as  Forest  .
0.673: (decision trees; be so known as; Forest)
0.627: (we; ve; collection of decision trees)
0.342: (we; ve collection of decision trees so known as Forest in; Random Forest)

 To classify a new object based on attributes  each tree gives a classification and we say the tree  votes  for that class.
0.778: (a new object; be based on; attributes each tree gives a classification)
0.755: (the tree votes; To classify; a new object based on attributes)
0.729: (each tree; gives; a classification)
0.657: (we; To classify; a new object based on attributes)
0.564: (a classification; be gives by; attributes)

 The forest chooses the classification having the most votes  over all the trees in the forest .
0.853: (The forest; chooses; the classification having the most votes over all the trees in the forest)

Each tree is planted   grown as follows .
No extractions found.

For more details on this algorithm  comparing with decision tree and tuning model parameters  I would suggest you to read these articles .
0.78: (you; to read; these articles)
0.59: (I; would suggest for; more details on this algorithm comparing with decision tree and tuning model parameters)

Introduction to Random forest   Simplified.
No extractions found.

Comparing a CART model to Random Forest  Part 1 .
0.723: (a CART model; be Comparing to; Random Forest)

Comparing a Random Forest to a CART model  Part 2 .
0.782: (a Random Forest; be Comparing to; a CART model Part 2)

Tuning the parameters of your Random Forest model.
No extractions found.

Python.
No extractions found.

R Code.
No extractions found.

 .
No extractions found.

In the last 4 5 years  there has been an exponential increase in data capturing at every possible stages.
No extractions found.

 Corporates  Government Agencies  Research organisations are not only coming with new sources but also they are capturing data in great detail.
0.684: (Corporates Government Agencies Research organisations; are not only coming with; new sources)[enabler=also they are capturing data in great detail]
0.613: (they; also are capturing; data)

For example  E commerce companies are capturing more details about customer like their demographics  web crawling history  what they like or dislike  purchase history  feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.
0.665: (E commerce companies; are capturing; more details)
0.607: (them; personalized; attention)
0.167: (more details; are capturing for; example)
0.065: (E commerce companies; are capturing more details about customer like their demographics web crawling history what they like or dislike purchase history feedback and many others to give them personalized attention more than your nearest grocery shopkeeper for; example)

As a data scientist  the data we are offered also consist of many features  this sounds good for building good robust model but there is a challenge.
0.875: (the data; also consist of; many features this sounds good for building good robust model)
0.444: (a challenge; is as; a data scientist)

 How d you identify highly significant variable s  out 1000 or 2000  In such cases  dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree  Random Forest  PCA  Factor Analysis  Identify based on correlation matrix  missing value ratio and others.
0.789: (you; identify; significant variable)
0.687: (dimensionality reduction algorithm; helps; us)
0.655: (various other algorithms; be based on; correlation matrix)

To know more about this algorithms  you can read  Beginners Guide To Learn Dimension Reduction Techniques .
0.72: (Beginners Guide; To Learn; Dimension Reduction Techniques)

 .
No extractions found.

GBM   AdaBoost are boosting algorithms used when we deal with plenty of data to make a prediction with high prediction power.
0.822: (we; deal with; plenty of data)
0.723: (GBM AdaBoost; are boosting; algorithms used when we deal with plenty of data to make a prediction with high prediction power)
0.546: (we; deal to make; a prediction)

 Boosting is an ensemble learning algorithm which combines the prediction of several base estimators in order to improve robustness over a single estimator.
0.817: (Boosting; is; an ensemble learning algorithm which combines the prediction of several base estimators in order to improve robustness over a single estimator)
0.708: (robustness; to be improve over; a single estimator)
0.564: (the prediction of several base estimators; be combines by; an ensemble learning algorithm)[enabler=in order to improve robustness over a single estimator]

 It combines multiple weak or average predictors to a build strong predictor.
0.785: (It; combines multiple weak or average predictors to; a build strong predictor)
0.722: (It; combines; multiple weak or average predictors)

 These boosting algorithms always work well in data science competitions like Kaggle  AV Hackathon  CrowdAnalytix.
No extractions found.

More  Know about Gradient and AdaBoost in detail.
No extractions found.

GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the difference between these two algorithms.
0.742: (two different; are ask about; the difference)
0.741: (GradientBoostingClassifier and Random Forest; are ask about; the difference)

By now  I am sure  you would have an idea of commonly used machine learning algorithms.
0.858: (you; would have; an idea of commonly used machine)
0.665: (you; be an idea of; used machine learning algorithms)

 My sole intention behind writing this article and providing the codes in R and Python is to get you started right away.
0.607: (the codes; be providing in; R and Python)

 If you are keen to master machine learning  start right away.
0.208: (you; to master; machine learning start right away)

 Take up problems  develop a physical understanding of the process  apply these codes and see the fun .
0.826: (problems; develop; a physical understanding of the process)

Did you find this article useful   Share your views and opinions in the comments section below.
0.788: (you; Did find; this article useful Share)

Awesowe compilation   Thank you.
0.652: (Awesowe compilation; Thank; you)

Thank you very much  A Very useful and excellent compilation.
No extractions found.

 I have already bookmarked this page.
0.674: (I; have already bookmarked; this page)

Straight  Informative and effective   Thank you.
No extractions found.

Good Summary airticle.
No extractions found.

Super Compilation .
No extractions found.

Wonderful  Really helpful.
No extractions found.

Very nicely done  Thanks for this.
No extractions found.

Thank you  Well presented article.
No extractions found.

Thank you  Well presented.
No extractions found.

Hello .
No extractions found.

Superb information in just one blog.
No extractions found.

 Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated.
0.668: (me; to run; the codes what should be replaced with symbol in codes Help)

Hello .
No extractions found.

Superb information in just one blog.
No extractions found.

 Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated .
0.668: (me; to run; the codes what should be replaced with symbol in codes Help)

    is used to select the variables that you ll be using for a particular model.
0.721: (you; that ll be using for; a particular model)

  label .
No extractions found.

     Uses all your Attributes  label Att1   Att2     Uses only Att1 and Att2 to create the model.
No extractions found.

Enjoyed the simplicity.
No extractions found.

 Thanks for the effort.
No extractions found.

Great Article  Helps a lot  as naive in Machine Learning.
0.883: (Great Article; Helps a lot as; naive)
0.785: (Great Article; Helps; a lot)

Hi All .
No extractions found.

Thanks for the comment  .
No extractions found.

Very good summary.
No extractions found.

Thank  One simple point.
No extractions found.

 The reason for taking the log p  1 p   in Logistic Regression is to make the equation linear  I.
0.644: (The reason; is to make; the equation linear I.)
0.586: (the log p; be taking in; Logistic Regression)

e.
No extractions found.

  easy to solve.
No extractions found.

Thanks Dalila   .
No extractions found.

That s not the reason for taking the log.
No extractions found.

 The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes.
0.915: (the probability; is governed by; a step function whose argument is linear in the attributes)
0.814: (whose argument; is linear in; the attributes)
0.722: (whose argument; is; linear)

 First of all the assumption of linearity or otherwise introduces bias.
No extractions found.

 However  logistic regression being a parametric model some bias is inevitable.
0.704: (some bias; is; inevitable)

 The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason.
0.825: (higher bias and one; would not like to do so without; good reason)

Now coming to the choice of log  it is just a convention.
0.652: (it; is just; a convention)

 Basically  once we have decided to go with a linear model  in the case of one attribute we model the probability by.
0.707: (we; Basically once have decided to go with; a linear model we model the probability by)

p x    f  ax b .
No extractions found.

such that p  infinity  0 and p infinity  0.
No extractions found.

 It so happens that this is satisfied by.
No extractions found.

p x    exp ax b    1   exp ax b  .
No extractions found.

which can be re written as.
No extractions found.

log p x   1 p x     a x  b.
No extractions found.

While I am at it  it may be useful to talk about another point.
0.685: (it; may be; useful)
0.515: (I; am at; it)

 One should ask is why we don t use least square method.
No extractions found.

 The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process.
0.604: (we; estimate; the probability)[enabler=that a yes no choice is a Bernoulli random variable and thus]
0.445: (a yes no choice; is thus; a Bernoulli random)

 For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method.
No extractions found.

 So deep down linear regression and logistic regression both use maximum likelihood estimates.
No extractions found.

 Its just that they are max likelihoods according to different distributions.
No extractions found.

Nice summary   Huzefa  you shouldn t replace the     in the R code  it basically means  as a function of .
0.896: (you; shouldn t replace the in; the R code it basically means as a function of)
0.827: (it; basically means as; a function of)

 You can also keep the  .
No extractions found.

  right after  it stands for  all other variables in the dataset provided .
0.714: (it; stands for; all other variables)
0.569: (it; stands in; the dataset)

 If you want to be explicit  you can write y   x1   x2     where x1  x2 .
0.825: (you; can write; y x1 x2 where x1 x2)

.
No extractions found.

 are the names of the columns of your data.
No extractions found.

frame or data.
No extractions found.

table.
No extractions found.

 Further note on formula specification  by default R adds an intercept  so that y   x is equivalent to y   1   x  you can remove it via y   0   x.
0.79: (you; to can remove it via; y 0 x)
0.712: (Further note; adds; an intercept)
0.689: (you; to can remove; it)
0.601: (it; to can be remove via; y 0 x)
0.564: (that y x; is; equivalent)

 Interactions are specified with either    which also adds the two variables  or    which only adds the interaction term .
No extractions found.

 y   x1   x2 is equivalent to y   x1   x2   x1   x2.
0.793: (x1 x2; is equivalent to; y)
0.735: (x1 x2; is; equivalent)
0.714: (x1 x2; is equivalent x1; x2 x1 x2)

 Hope this helps .
No extractions found.

You did a Wonderful job  This is really helpful.
0.829: (You; did; a Wonderful job This is really helpful)

 Thanks .
No extractions found.

I took the Stanford Coursera ML class  but have not used it  and I found this to be an incredibly useful summary.
0.685: (I; took; the Stanford Coursera ML class)
0.659: (I; have found; this to be an incredibly useful summary)
0.362: (I; have not used; it)

 I appreciate the real world analogues  such as your mention of Jezzball.
0.618: (I; appreciate; the real world analogues)

 And showing the brief code snips is terrific.
No extractions found.

This is very easy and helpful than any other courses I have completed.
No extractions found.

 simple.
No extractions found.

 clear.
No extractions found.

 To the point.
No extractions found.

You Sir are a gentleman and a scholar .
No extractions found.

Hi Sunil .
No extractions found.

This is really superb tutorial along with good examples and codes which is surely much helpful.
No extractions found.

 Just  can you add Neural Network here in simple terms with example and code.
0.864: (you; Just can add Neural Network here in; simple terms)
0.795: (Neural Network; Just can be add here in; simple terms)
0.746: (you; Just can add here; Neural Network)

Errata   fit    kmeans X  3    5 cluster solution It s a 3 cluster solution.
0.728: (Errata fit; kmeans; X 3 5 cluster solution)
0.624: (It; s; a 3 cluster solution)
0.548: (a 3 cluster solution; be s by; X 3 5 cluster solution)

Well done  Thank you .
No extractions found.

This is a great resource overall and surely the product of a lot of work.
No extractions found.

Just a note as I go through this  your comment on Logistic Regression not actually being regression is in fact wrong.
0.614: (I; go through; this your comment)
0.502: (I; go on; Logistic Regression)

 It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability.
0.736: (a continuous variable; be bound between; 0 and 1)
0.64: (we; regard as; probability)

 it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression.
0.752: (a threshold; is not the main aim of; Logistic Regression)
0.564: (the choice of a threshold; be requires by; an extra step)

 As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example.
0.848: (it; falls under; the umbrella of Generalized Libear Models)
0.84: (a matter of fact; falls under; the umbrella of Generalized Libear Models)
0.68: (a matter of fact; falls as; the glm R package)
0.679: (it; falls as; the glm R package)

I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1.
No extractions found.

 Thanks for the great article overall.
No extractions found.

Very Nice.
No extractions found.

  .
No extractions found.

Thank you.
No extractions found.

.
No extractions found.

 reallu helpful article.
No extractions found.

I wanted to know if I can use rattle instead of writing the R code explicitly.
0.083: (I; can use; rattle)

Thank you.
No extractions found.

 Very nice and useful article.
No extractions found.

This is such a wonderful article.
No extractions found.

Informative and easy to follow.
No extractions found.

 I ve recently started following several pages like this one and this is the best material ive seen yet.
0.858: (material; ve recently started following; several pages)

One of the best content ever read regarding algorithms.
No extractions found.

Thank you so much for this article.
No extractions found.

Cool stuff  I just can t get the necessary libraries .
0.678: (I; just can t get; the necessary libraries)

looks sgood article.
No extractions found.

 Do I need any data to do the examples .
0.754: (I; Do need; any data to do the examples)

Good Article.
No extractions found.

I have to thank you for this informative summary.
0.772: (you; to be thank for; this informative summary)
0.752: (I; have to thank you for; this informative summary)
0.646: (I; have to thank; you)

 Really useful .
No extractions found.

Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it.
0.593: (it; does not mention; any measure of performance)

 Cooking recipes like these are the ones that place people in Drew Conway s danger zone  https   www.
No extractions found.

quora.
No extractions found.

com In the data science venn diagram why is the common region of Hacking Skills and Substantive Expertise considered as danger zone   thus making programmers the worst data analysts  let alone scientists  that requires another mindset completely .
0.744: (the worst data analysts; let; alone scientists that requires another mindset completely)
0.695: (the common region of Hacking Skills; considered in; the data science venn diagram)[enabler=as danger zone thus making programmers the worst data analysts let alone scientists that requires another mindset completely]
0.564: (another mindset; be requires by; alone scientists)
0.542: (com; considered in; the data science venn diagram)[enabler=as danger zone thus making programmers the worst data analysts let alone scientists that requires another mindset completely]
0.51: (danger zone; thus making; programmers the worst data analysts let alone scientists)
0.341: (common; be region of; Hacking Skills and Substantive Expertise)

 I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background.
0.767: (I; highly recommend; anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background)

 Otherwise you could end up like Google  Target  Telefonica  or Google  again  and become a poster boy for  The Big Flops of Big Data .
0.843: (you; could become a poster boy for; The Big Flops of Big Data)
0.813: (you; Otherwise could end up again like; Google Target Telefonica or Google)
0.569: (you; could become; a poster boy)

Do you have a better article Please share .
0.772: (you; Do have; a better article)

Great article.
No extractions found.

 It really summarize some of the most important topics on machine learning.
No extractions found.

 But as asked above I would like to present thedevmasters.
0.569: (I; would like to present; thedevmasters)
0.466: (I; to present; thedevmasters)

com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends.
0.688: (a company; to learn; more depth about machine learning with great professors and a sense of community)

awesome   recommended this article to all my friends.
0.873: (awesome; recommended this article to; all my friends)
0.837: (awesome; recommended; this article)
0.73: (this article; be recommended to; all my friends)

Very succinct description of some important algorithms.
No extractions found.

 Thanks.
No extractions found.

 I d like to point out a mistake in the SVM section.
0.632: (I; d to point out; a mistake)

 You say  where each point has two co ordinates  these co ordinates are known as Support Vectors  .
0.902: (these co ordinates; are known as; Support Vectors)
0.68: (each point; has; two co ordinates)

 This is not correct  the coordinates are just features.
No extractions found.

 Its the points lying on the margin that are called the  support vectors .
No extractions found.

 These are the points that  support  the margin i.
No extractions found.

e.
No extractions found.

 define it  as opposed to a weighted average of all points for instance.
No extractions found.

 .
No extractions found.

Thank you for this wonderful article it s proven helpful.
0.874: (you; be Thank for; this wonderful article it s proven helpful)
0.504: (it; s proven in; this wonderful article)

Thank you very much  A Very useful and excellent compilation.
No extractions found.

Very good information interms of initial knowledge Note one warning  many methods can be fitted into a particular problem  but result might not be what you wish.
0.863: (many methods; can be fitted into; a particular problem)

 Hence you must always compare models  understand residuals profile and how prediction really predicts.
No extractions found.

 In that sense  analysis of data is never ending.
No extractions found.

 In R  use summary  plot and check for assumptions validity .
No extractions found.

The amazing article.
No extractions found.

 I m new in data analysis.
No extractions found.

 It s very useful and easy to understand.
No extractions found.

Thanks .
No extractions found.

This is really good article  also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know   what and where to apply in machine learning .
0.232: (you; would have explain about; Anomaly dection algorithm)

a very very helpful tutorial.
No extractions found.

 Thanks a lot you guys.
No extractions found.

The amazing article.
No extractions found.

Analytics Vidhya   I am loving it.
0.399: (I; Analytics Vidhya am loving; it)
0.378: (I; am loving it; Analytics Vidhya)

Good article.
No extractions found.

 thank you for explaining with python.
No extractions found.

very useful compilation.
No extractions found.

 Thanks .
No extractions found.

Very precise quick tutorial for those who want to gain insight of machine learning.
No extractions found.

great.
No extractions found.

Very useful and informative.
No extractions found.

 Thanks for sharing it.
No extractions found.

Superb .
No extractions found.

great summary  Thank you.
0.672: (great summary; Thank; you)

Great article.
No extractions found.

 It would have become even better if you had some test data with each code snippet.
0.75: (It; would have become even better had some test data with; each code snippet)
0.709: (some test data; would have become even better had with; each code snippet)
0.632: (It; would have become even better had; some test data)
0.256: (you; would have become even better had some test data with; each code snippet)
0.165: (you; would have become even better had; some test data)

 Add metrics and hyper parameter tunning for each of these models.
No extractions found.

Thanks for the  jezzball  example.
No extractions found.

 You made my day .
0.799: (You; made in; my day)

Nicely complied.
No extractions found.

 Every explanation is crystal clear and very easy to digest.
0.746: (Every explanation; is; clear and very easy)

 Thanks for sharing knowledge.
No extractions found.

Perfect  It s exactly what I was looking for  Thanks for the explanation and thanks for sharing your knowledge with us .
0.684: (your knowledge; be sharing with; us)
0.395: (I; exactly what was looking for; Thanks)

Very useful summary.
No extractions found.

 Thank you.
No extractions found.

Very informative article.
No extractions found.

 For a person new to machine learning  this article gives a good starting point.
0.772: (this article; gives; a good starting point)
0.679: (a person; gives; a good starting point)

Good article.
No extractions found.

 thank you for explaining with python.
No extractions found.

Good article.
No extractions found.

 thank you for explaining with python.
No extractions found.

  Kareermatrix.
No extractions found.

Hi Friends  i m new person to these machine learning algorithms.
No extractions found.

 i have some questions.
0.671: (i; have; some questions)

  1  we have so many ML algorithms.
No extractions found.

 but how can we choose the algorithms which one is suitable for my data set  2  How does these algorithms works.
0.674: (one; is; suitable)
0.609: (one; which is suitable for; my data set 2 How does these algorithms works)
0.607: (we; choose; the algorithms which one is suitable for my data)

  3  why only these particular algorithms.
No extractions found.

  why not others.
No extractions found.

 .
No extractions found.

Nice Article.
No extractions found.

.
No extractions found.

 Thanks for your effort.
No extractions found.

hello.
No extractions found.

I have to implement machine learning algorithms in python so could you help me in this.
0.689: (you; could help; me)
0.661: (I; have to implement; machine learning algorithms in python so)
0.462: (machine; learning algorithms so in; python)
0.459: (algorithms; be learning so in; python)

 any body provide me the proper code for any algorithm.
0.808: (any body; provide me; the proper code)
0.589: (any body; be the proper code for; any algorithm)

All programe has error named Error in model.
0.793: (error; be named in; model)

frame.
No extractions found.

default formula   as.
No extractions found.

list y train    .
No extractions found.

  data   x    invalid type  list  for variable  as.
No extractions found.

list y train  .
No extractions found.

What is that  .
No extractions found.

I think that  y train  is data frame and it cannot be converted directly to list with  as.
0.883: (it; cannot be converted directly to; list)[attrib=I think]
0.445: (y train; is cannot; data frame)

list  command.
No extractions found.

 Try this instead.
No extractions found.

y train    as.
No extractions found.

list as.
No extractions found.

data.
No extractions found.

frame t y train   .
No extractions found.

See if this works for you.
No extractions found.

Very nice summary .
No extractions found.

Can you tell how to get machine learning problems for practice .
No extractions found.

Analytics Vidhya has some practice datasets.
0.747: (Vidhya; has; some practice datasets)
0.653: (some practice datasets; be has by; Analytics)

 Check Analytics Vidhya hackathon.
No extractions found.

 Also UCI machine learning repository is a phenomenal place.
0.689: (UCI machine learning repository; Also is; a phenomenal place)

 Google it and enjoy.
No extractions found.

Do you have R codes based on caret .
0.873: (you; Do have; R codes based on caret)
0.829: (R codes; be based on; caret)

Yes.
No extractions found.

.
No extractions found.

Receive awesome tips  guides  infographics and become expert at .
No extractions found.

Interact with thousands of data science professionals across the globe .
No extractions found.

.
No extractions found.

 P.
No extractions found.

S.
No extractions found.

 We only publish awesome content.
0.691: (We; only publish; awesome content)

 We will never share your information with anyone.
0.772: (your information; will be never share with; anyone)
0.768: (We; will never share your information with; anyone)
0.666: (We; will never share; your information)

