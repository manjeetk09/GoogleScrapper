examples of message passing in c++;https://www.hscripts.com/tutorials/cpp/cpp-message-passing.php;How is Message Passing used in OOPs concept of C   .;oops_concept;0.6568627450980393;1.0;1.0;2.6568627450980395;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;msgop c  Sample Program to Illustrate msgsndand msgrcv.;illustrate_msgsndand_msgrcv;1.0;0.0784313725490196;1.0;2.0784313725490193;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Controlling message queues.;controlling_message_queues;1.0;0.0392156862745098;1.0;2.0392156862745097;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; For single_message transactions, multiple server processes can work in parallel on transactions sent to a shared message queue.;shared_message_queue;1.0;0.0392156862745098;1.0;2.0392156862745097;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; For single_message transactions, multiple server processes can work in parallel on transactions sent to a shared message queue.;multiple_server_processes;1.0;0.0392156862745098;1.0;2.0392156862745097;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Message passing differs from conventional programming where a process, subroutine, or function is directly invoked by name.;message_passing_differs;1.0;0.02631578947368421;1.0;2.026315789473684;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Just as the function caller stops until the called function completes, the sending process stops until the receiving process completes.;receiving_process_completes;1.0;0.02631578947368421;1.0;2.026315789473684;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Just as the function caller stops until the called function completes, the sending process stops until the receiving process completes.;function_caller_stops;1.0;0.02631578947368421;1.0;2.026315789473684;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Just as the function caller stops until the called function completes, the sending process stops until the receiving process completes.;called_function_completes;1.0;0.02631578947368421;1.0;2.026315789473684;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Just as the function caller stops until the called function completes, the sending process stops until the receiving process completes.;sending_process_stops;1.0;0.02631578947368421;1.0;2.026315789473684;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; For example, if synchronous message passing would be used exclusively, large, distributed systems generally would not perform well enough to be usable.;distributed_systems_generally;1.0;0.02631578947368421;1.0;2.026315789473684;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing;Examples of systems that support distributed objects are.;support_distributed_objects;1.0;0.02631578947368421;1.0;2.026315789473684;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing;NET Remoting, CTOS, QNX Neutrino RTOS, OpenBinder, and D_Bus.;qnx_neutrino_rtos;1.0;0.02631578947368421;1.0;2.026315789473684;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Create a data type representing a row of an array and distribute a different row to all processes.;data_type_representing;1.0;0.0035955056179775282;1.0;2.0035955056179775;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; As such, MPI is the first standardized, vendor independent, message passing library.;message_passing_library;1.0;0.002696629213483146;1.0;2.0026966292134833;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Task 0 pings task 1 and awaits return ping.;awaits_return_ping;1.0;0.002696629213483146;1.0;2.0026966292134833;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;System buffer space is  Opaque to the programmer and managed entirely by the MPI library A finite resource that can be easy to exhaust Often mysterious and not well documented Able to exist on the sending side, the receiving side, or both Something that may improve program performance because it allows send _ receive operations to be asynchronous .;system_buffer_space;1.0;0.002696629213483146;1.0;2.0026966292134833;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The Message Passing Interface Standard is a message passing library standard based on the consensus of the MPI Forum, which has over 40 participating organizations, including vendors, researchers, software library developers, and users.;software_library_developers;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The advantages of developing message passing software using MPI closely match the design goals of portability, efficiency, and flexibility.;mpi_closely_match;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Nearest neighbor exchange in a ring topology.;nearest_neighbor_exchange;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Collective communication routines must involve all processes within the scope of a communicator.;collective_communication_routines;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Create a datatype by extracting variable portions of an array and distribute to all tasks.;extracting_variable_portions;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;argonne_national_laboratory;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;lc_usage_details;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;mpich_mpi_library;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;selected_dotkit_package;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;default___requires;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Note that increasing the number of points generated improves the approximation .;points_generated_improves;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;System buffer space is  Opaque to the programmer and managed entirely by the MPI library A finite resource that can be easy to exhaust Often mysterious and not well documented Able to exist on the sending side, the receiving side, or both Something that may improve program performance because it allows send _ receive operations to be asynchronous .;improve_program_performance;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer .;blocking_send_routine;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this .;requested_non_blocking_operation;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Login to the LC workshop cluster, if you are not already logged in.;lc_workshop_cluster;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Group and Communicator Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43       include  mpi h       include  stdio h       define NPROCS 8       main       int        rank, new_rank, sendbuf, recvbuf, numtasks,                ranks1 4   0,1,2,3 , ranks2 4   4,5,6,7       MPI_Group  orig_group, new_group       required variables     MPI_Comm   new_comm       required variable       MPI_Init      MPI_Comm_rank      MPI_Comm_size        if         printf        MPI_Finalize        exit                sendbuf   rank           extract the original group handle     MPI_Comm_group            divide tasks into two distinct groups based upon rank     if         MPI_Group_incl              else         MPI_Group_incl                   create new new communicator and then perform collective communications     MPI_Comm_create      MPI_Allreduce           get rank in new group     MPI_Group_rank       printf        MPI_Finalize       .;distinct_groups_based;1.0;0.0017977528089887641;1.0;2.0017977528089888;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Livermore Computing specific information  Linux Clusters Overview tutorial computing llnl gov tutorials linux_clusters Using the Sequoia Vulcan BG Q Systems tutorial computing llnl gov tutorials bgq.;sequoia_vulcan_bg;1.0;0.0017977528089887641;1.0;2.0017977528089888;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The goal of this tutorial is to teach those unfamiliar with MPI how to develop and run parallel programs according to the MPI standard.;run_parallel_programs;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The tutorial materials also include more advanced topics such as Derived Data Types, Group and Communicator Management Routines, and Virtual Topologies.;derived_data_types;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The tutorial materials also include more advanced topics such as Derived Data Types, Group and Communicator Management Routines, and Virtual Topologies.;communicator_management_routines;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI is a specification for the developers and users of message passing libraries.;message_passing_libraries;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI primarily addresses the message_passing parallel programming model.;mpi_primarily_addresses;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Interface specifications have been defined for C and Fortran90 language bindings.;fortran90_language_bindings;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Originally, MPI was designed for distributed memory architectures, which were becoming increasingly popular at that time .;distributed_memory_architectures;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; As architecture trends changed, shared memory SMPs were combined over networks creating hybrid distributed memory   shared memory systems.;architecture_trends_changed;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; As architecture trends changed, shared memory SMPs were combined over networks creating hybrid distributed memory   shared memory systems.;shared_memory_smps;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI implementors adapted their libraries to handle both types of underlying memory architectures seamlessly.;mpi_implementors_adapted;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; They also adapted developed ways of handling different interconnects and protocols.;adapted_developed_ways;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The programming model clearly remains a distributed memory model however, regardless of the underlying physical architecture of the machine.;underlying_physical_architecture;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; the programmer is responsible for correctly identifying parallelism and implementing parallel algorithms using MPI constructs.;correctly_identifying_parallelism;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; the programmer is responsible for correctly identifying parallelism and implementing parallel algorithms using MPI constructs.;implementing_parallel_algorithms;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Standardization _ MPI is the only message passing library that can be considered a standard.;standardization___mpi;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Any implementation is free to develop optimized algorithms.;develop_optimized_algorithms;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Distributed memory, parallel computing develops, as do a number of incompatible software tools for writing such programs _ usually with tradeoffs between portability, performance, functionality and price.;incompatible_software_tools;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Distributed memory, parallel computing develops, as do a number of incompatible software tools for writing such programs _ usually with tradeoffs between portability, performance, functionality and price.;parallel_computing_develops;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Workshop on Standards for Message Passing in a Distributed Memory Environment, sponsored by the Center for Research on Parallel Computing, Williamsburg, Virginia.;distributed_memory_environment;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The basic features essential to a standard message passing interface were discussed, and a working group established to continue the standardization process.;working_group_established;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The basic features essential to a standard message passing interface were discussed, and a working group established to continue the standardization process.;basic_features_essential;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Working group meets in Minneapolis.;working_group_meets;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI draft proposal from ORNL presented.;mpi_draft_proposal;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Group adopts procedures and organization to form the MPI Forum.;group_adopts_procedures;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Although the MPI programming interface has been standardized, actual library implementations will differ in which version and features of the standard they support.;actual_library_implementations;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Although the MPI programming interface has been standardized, actual library implementations will differ in which version and features of the standard they support.;mpi_programming_interface;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MVAPICH _ Linux clusters Open MPI _ Linux clusters Intel MPI _ Linux clusters IBM BG Q MPI _ BG Q clusters IBM Spectrum MPI _ Coral Early Access clusters.;mpi___bg;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A summary of each is provided below, along with links to additional detailed information.;additional_detailed_information;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI executables are launched using the SLURM srun command with the appropriate options.;slurm_srun_command;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The srun command is discussed in detail in the Running Jobs section of the Linux Clusters Overview tutorial.;running_jobs_section;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MVAPICH home page.;mvapich_home_page;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MVAPICH2 User Guides.;mvapich2_user_guides;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPICH home page.;mpich_home_page;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;  usr local docs on LC s TOSS 2 clusters.;usr_local_docs;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Launching an Open MPI job can be done using the following commands.;open_mpi_job;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; CALL MPI_XXXXX parameter,.;call_mpi_xxxxx_parameter;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; CALL MPI_BSENDError code.;call_mpi_bsenderror_code;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI uses objects called communicators and groups to define which collection of processes may communicate with each other.;objects_called_communicators;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Most MPI routines require you to specify a communicator as an argument.;mpi_routines_require;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Within a communicator, every process has its own unique, integer identifier assigned by the system when the process initializes.;integer_identifier_assigned;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Often used conditionally by the application to control program execution .;control_program_execution;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Most MPI routines include a return error code parameter, as described in the  Format of MPI Calls  section above.;mpi_routines_include;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; This means you will probably not be able to capture a return error code other than MPI_SUCCESS .;return_error_code;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The standard does provide a means to override this default error handler.;default_error_handler;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; You can also consult the error handling section of the relevant MPI Standard documentation located at http.;error_handling_section;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Order rules do not apply if there are multiple threads participating in the communication operations.;multiple_threads_participating;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI does not guarantee fairness _ it s up to the programmer to prevent  operation starvation .;guarantee_fairness__;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Check the MPI header file.;mpi_header_file;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Collective Computation _ one member of the group collects data from the other members and performs an operation  min, max, add, multiply, etc.;collective_computation__;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Collective Computation _ one member of the group collects data from the other members and performs an operation  min, max, add, multiply, etc.;group_collects_data;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Unexpected behavior, including program failure, can occur if even one task in the communicator doesn t participate.;including_program_failure;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Collective communication routines do not take message tag arguments.;message_tag_arguments;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;mpi_reduction_operation;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Note from the MPI_Reduce man page.;mpi_reduce_man_page;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; This may change the result of the reduction for operations that are not strictly associative and commutative, such as floating point addition.;floating_point_addition;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Each process in a group is associated with a unique integer rank.;unique_integer_rank;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Rank values start at zero and go to N_1, where N is the number of processes in the group.;rank_values_start;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group Form new group as a subset of global group using MPI_Group_incl Create new communicator for new group using MPI_Comm_create Determine new rank in new communicator using MPI_Comm_rank Conduct communications using any MPI message passing routine When finished, free up new communicator and group using MPI_Comm_free and MPI_Group_free.;mpi_comm_rank_conduct_communications;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; In terms of MPI, a virtual topology describes a mapping ordering of MPI processes into a geometric  shape .;virtual_topology_describes;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Convenience Virtual topologies may be useful for applications with specific communication patterns _ patterns that match an MPI topology structure.;mpi_topology_structure;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Convenience Virtual topologies may be useful for applications with specific communication patterns _ patterns that match an MPI topology structure.;convenience_virtual_topologies;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For example, a Cartesian topology might prove convenient for an application that requires 4_way nearest neighbor communications for grid based data.;grid_based_data;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Dynamic Processes _ extensions that remove the static process model of MPI.;static_process_model;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Include shared memory operations and remote accumulate operations.;remote_accumulate_operations;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Additional Language Bindings _ describes C   bindings and discusses Fortran_90 issues.;discusses_fortran_90_issues;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The MPI_3 standard was adopted in 2012, and contains significant extensions to MPI_1 and MPI_2 functionality including.;mpi_2_functionality_including;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Neighborhood Collectives _ extends the distributed graph and Cartesian process topologies with additional communication power.;additional_communication_power;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Neighborhood Collectives _ extends the distributed graph and Cartesian process topologies with additional communication power.;cartesian_process_topologies;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Standard documents.;mpi_standard_documents;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH MPI is developed and supported by the Network_Based Computing Lab at Ohio State University .;ohio_state_university;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH MPI is developed and supported by the Network_Based Computing Lab at Ohio State University .;network_based_computing_lab;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;mvapich2_multiple_versions;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Available on all LC Linux clusters  However, you ll need to load the desired dotkit or module first  For example  dotkit   use _l openmpi                 use openmpi_gnu_1 8 4        module   module avail                   module load openmpi 2 0 0    This ensures that LC s MPI wrapper scripts point to the desired version of Open MPI .;lc_linux_clusters;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Compiling and running IBM BG Q MPI programs  see the BG Q Tutorial  computing llnl gov tutorials bgq .;running_ibm_bg;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;NVIDIA GPU support.;nvidia_gpu_support;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;LC developed MPI compiler wrapper scripts are used to compile MPI programs.;compile_mpi_programs;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded.;__gnu_mpig;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded.;dotkit_package_loaded;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded.;man_page_issue;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded.;_help_option_view;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;With MPI_3 Fortran, the USE mpi_f08 module is preferred over using the include file shown above .;include_file_shown;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Serial pseudo code for this procedure  npoints   10000  circle_count   0    do j   1,npoints    generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do    PI   4 0 circle_count npoints.;serial_pseudo_code;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Pseudo code solution  red highlights changes for parallelism  npoints   10000  circle_count   0   p   number of tasks num   npoints p find out if I am MASTER or WORKER     do j   1,num     generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do   if I am MASTER receive from WORKERS their circle_counts compute PI else if I am WORKER send to MASTER circle_count endif Example MPI Program in C    mpi_pi_reduce c Example MPI Program in Fortran    mpi_pi_reduce f.;circle_counts_compute_pi;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Combined send receive.;combined_send_receive;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The MPI implementation decides what happens to data in these types of cases  Typically, a system buffer area is reserved to hold data in transit  For example .;system_buffer_area;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The MPI implementation decides what happens to data in these types of cases  Typically, a system buffer area is reserved to hold data in transit  For example .;mpi_implementation_decides;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Multiple sends arrive at the same receiving task which can only accept one send at a time _ what happens to the messages that are  backing up  .;multiple_sends_arrive;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Most of the MPI point_to_point routines can be used in either blocking or non_blocking mode .;mpi_point_to_point_routines;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program scatter     include  mpif h        integer SIZE     parameter     integer numtasks, rank, sendcount, recvcount, source, ierr     real 4 sendbuf, recvbuf         Fortran stores this array in column major order, so the        scatter will actually scatter columns, not rows      data sendbuf  1 0, 2 0, 3 0, 4 0,                     5 0, 6 0, 7 0, 8 0,                     9 0, 10 0, 11 0, 12 0,                     13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then          define source task and elements to send receive, then perform collective scatter        source   1        sendcount   SIZE        recvcount   SIZE        call MPI_SCATTER sendbuf, sendcount, MPI_REAL, recvbuf, recvcount, MPI_REAL,                           source, MPI_COMM_WORLD, ierr           print  ,  rank   ,rank,  Results   ,recvbuf        else        print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       end.;column_major_order;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI also provides facilities for you to define your own data structures based upon sequences of the MPI primitive data types  Such user defined structures are called derived data types .;data_structures_based;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI also provides facilities for you to define your own data structures based upon sequences of the MPI primitive data types  Such user defined structures are called derived data types .;user_defined_structures;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Group and Communicator Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43       include  mpi h       include  stdio h       define NPROCS 8       main       int        rank, new_rank, sendbuf, recvbuf, numtasks,                ranks1 4   0,1,2,3 , ranks2 4   4,5,6,7       MPI_Group  orig_group, new_group       required variables     MPI_Comm   new_comm       required variable       MPI_Init      MPI_Comm_rank      MPI_Comm_size        if         printf        MPI_Finalize        exit                sendbuf   rank           extract the original group handle     MPI_Comm_group            divide tasks into two distinct groups based upon rank     if         MPI_Group_incl              else         MPI_Group_incl                   create new new communicator and then perform collective communications     MPI_Comm_create      MPI_Allreduce           get rank in new group     MPI_Group_rank       printf        MPI_Finalize       .;language___group;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Group and Communicator Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42      program group     include  mpif h        integer NPROCS     parameter     integer rank, new_rank, sendbuf, recvbuf, numtasks     integer ranks1, ranks2, ierr     integer orig_group, new_group, new_comm     required variables     data ranks1  0, 1, 2, 3 , ranks2  4, 5, 6, 7        call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then       print  ,  Must specify NPROCS   ,NPROCS,  Terminating         call MPI_FINALIZE       stop     endif       sendbuf   rank         extract the original group handle     call MPI_COMM_GROUP         divide tasks into two distinct groups based upon rank     if then        call MPI_GROUP_INCL     else         call MPI_GROUP_INCL     endif         create new new communicator and then perform collective communications     call MPI_COMM_CREATE     call MPI_ALLREDUCE         get rank in new group     call MPI_GROUP_RANK     print  ,  rank   ,rank,  newrank   ,new_rank,  recvbuf   , recvbuf       call MPI_FINALIZE     end.;fortran___group;1.0;8.988764044943821E-4;1.0;2.000898876404494;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Several codes provide serial examples for a comparison with the parallel MPI versions .;parallel_mpi_versions;1.0;8.988764044943821E-4;1.0;2.000898876404494;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;msgctl cSample Program to Illustrate msgctl.;msgctl_csample_program;0.94281045751634;0.0392156862745098;1.0;1.9820261437908497;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Because of this, a server process can direct message traffic between clients on its queue by using the client process PID as the message type.;client_process_pid;0.94281045751634;0.0392156862745098;1.0;1.9820261437908497;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Because of this, a server process can direct message traffic between clients on its queue by using the client process PID as the message type.;direct_message_traffic;0.94281045751634;0.0392156862745098;1.0;1.9820261437908497;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;multi_threaded_mpi_program;0.9618736383446078;0.0017977528089887641;1.0;1.9636713911535966;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;mpi_mpi_1_implementation;0.9618736383446078;8.988764044943821E-4;1.0;1.9627725147491022;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;mpi_3_implementations_based;0.94281045751634;0.0017977528089887641;1.0;1.9446082103253288;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The programming model clearly remains a distributed memory model however, regardless of the underlying physical architecture of the machine.;distributed_memory_model;0.94281045751634;8.988764044943821E-4;1.0;1.9437093339208342;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 picked up where the first MPI specification left off, and addressed topics which went far beyond the MPI_1 specification.;mpi_specification_left;0.94281045751634;8.988764044943821E-4;1.0;1.9437093339208342;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The mapping of processes into an MPI virtual topology is dependent upon the MPI implementation, and may be totally ignored.;mpi_virtual_topology;0.94281045751634;8.988764044943821E-4;1.0;1.9437093339208342;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; One_Sided Communications _ provides routines for one directional communications.;one_sided_communications__;0.94281045751634;8.988764044943821E-4;1.0;1.9437093339208342;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI point_to_point operations typically involve message passing between two, and only two, different MPI tasks  One task is performing a send operation and the other task is performing a matching receive operation .;matching_receive_operation;0.94281045751634;8.988764044943821E-4;1.0;1.9437093339208342;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;mpi_byte_mpi_lor_logical;0.931372549019608;8.988764044943821E-4;1.0;1.9322714254241022;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;mpi_byte_mpi_maxloc_max;0.931372549019608;8.988764044943821E-4;1.0;1.9322714254241022;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions .;mpi_libraries_vary;0.931372549019608;8.988764044943821E-4;1.0;1.9322714254241022;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing;In computer science, message passing sends a message to a process and relies on the process and the supporting infrastructure to select and invoke the actual code to run.;message_passing_sends;0.8856209150326797;0.02631578947368421;1.0;1.9119367045063638;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Distributed object systems have been called  shared nothing  systems because the message passing abstraction hides underlying state changes that may be used in the implementation of sending messages.;distributed_object_systems;0.8856209150326797;0.02631578947368421;1.0;1.9119367045063638;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34       include  mpi h       include  stdio h        main       int numtasks, rank, next, prev, buf 2 , tag1 1, tag2 2      MPI_Request reqs 4        required variable for non_blocking calls     MPI_Status stats 4        required variable for Waitall routine       MPI_Init      MPI_Comm_size      MPI_Comm_rank              determine left and right neighbors     prev   rank_1      next   rank 1      if  prev   numtasks _ 1      if    next   0           post non_blocking receives and sends for neighbors     MPI_Irecv      MPI_Irecv        MPI_Isend      MPI_Isend                do some work while sends receives progress in background          wait for all non_blocking operations to complete     MPI_Waitall                continue _ do more work       MPI_Finalize       .;sends_receives_progress;0.8856209150326797;0.0017977528089887641;1.0;1.8874186678416685;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33       include  mpi h       include  stdio h       define SIZE 4       main       int numtasks, rank, sendcount, recvcount, source      float sendbuf SIZE  SIZE             1 0, 2 0, 3 0, 4 0 ,        5 0, 6 0, 7 0, 8 0 ,        9 0, 10 0, 11 0, 12 0 ,        13 0, 14 0, 15 0, 16 0          float recvbuf SIZE         MPI_Init      MPI_Comm_rank      MPI_Comm_size        if            define source task and elements to send receive, then perform collective scatter       source   1        sendcount   SIZE        recvcount   SIZE        MPI_Scatter sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,                   MPI_FLOAT,source,MPI_COMM_WORLD           printf  rank   d  Results   f  f  f  f n ,rank,recvbuf 0 ,              recvbuf 1 ,recvbuf 2 ,recvbuf 3                else       printf        MPI_Finalize       .;define_source_task;0.8856209150326797;0.0017977528089887641;1.0;1.8874186678416685;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; This is followed by a detailed look at the MPI routines that are most useful for new MPI programmers, including MPI Environment Management, Point_to_Point Communications, and Collective Communications routines.;collective_communications_routines;0.8856209150326797;8.988764044943821E-4;1.0;1.886519791437174;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Pseudo code solution  red highlights changes for parallelism  npoints   10000  circle_count   0   p   number of tasks num   npoints p find out if I am MASTER or WORKER     do j   1,num     generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do   if I am MASTER receive from WORKERS their circle_counts compute PI else if I am WORKER send to MASTER circle_count endif Example MPI Program in C    mpi_pi_reduce c Example MPI Program in Fortran    mpi_pi_reduce f.;master_circle_count_endif;0.8856209150326797;8.988764044943821E-4;1.0;1.886519791437174;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;This group of routines is used for interrogating and setting the MPI execution environment, and covers an assortment of purposes, such as initializing and terminating the MPI environment, querying a rank s identity, querying the MPI library s version, etc.;mpi_execution_environment;0.8665577342044117;8.988764044943821E-4;1.0;1.867456610608906;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;complex_mpi_land_logical;0.8474945533772876;8.988764044943821E-4;1.0;1.848393429781782;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized .;make_mpi_calls;0.8284313725490196;0.0017977528089887641;1.0;1.8302291253580083;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40      program ringtopo     include  mpif h        integer numtasks, rank, next, prev, buf, tag1, tag2, ierr     integer reqs    required variable for non_blocking calls      integer stats    required variable for WAITALL routine      tag1   1     tag2   2       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         determine left and right neighbors      prev   rank _ 1     next   rank   1     if then        prev   numtasks _ 1     endif     if then        next   0     endif         post non_blocking receives and sends for neighbors      call MPI_IRECV, 1, MPI_INTEGER, prev, tag1, MPI_COMM_WORLD, reqs, ierr      call MPI_IRECV, 1, MPI_INTEGER, next, tag2, MPI_COMM_WORLD, reqs, ierr        call MPI_ISEND, ierr      call MPI_ISEND, ierr             do some work while sends receives progress in background         wait for all non_blocking operations to complete      call MPI_WAITALL             continue _ do more work       call MPI_FINALIZE       end.;ierr;0.8284313725490196;0.0;1.0;1.8284313725490196;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Message passing can still be used, of course, in a shared memory platform.;shared_memory_platform;1.0;0.03125;0.7799999999999999;1.8112499999999998;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Since the sender and receiver are at specific known points in their code at a known specific instant of time, synchronous message passing is also called a simple rendezvous with a one_way flow of information from the sender to the receiver.;synchronous_message_passing;1.0;0.03125;0.7799999999999999;1.8112499999999998;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; An example is a chess game agent.;chess_game_agent;1.0;0.03125;0.7799999999999999;1.8112499999999998;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; The agents can process messages synchronously, since they ll be handshaking throughout the entire game.;process_messages_synchronously;1.0;0.03125;0.7799999999999999;1.8112499999999998;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; In asynchronous message passing, the sender does not block.;asynchronous_message_passing;1.0;0.03125;0.7799999999999999;1.8112499999999998;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;This group of routines is used for interrogating and setting the MPI execution environment, and covers an assortment of purposes, such as initializing and terminating the MPI environment, querying a rank s identity, querying the MPI library s version, etc.;mpi_environment;0.7998366013066176;8.988764044943821E-4;1.0;1.800735477711112;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;http://condor.cc.ku.edu/~grobe/docs/intro-MPI-C.shtml;This is a short introduction to the Message Passing Interface designed to convey the fundamental operation and use of the interface.;short_introduction;0.6568627450980393;0.13333333333333333;1.0;1.7901960784313726;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;http://condor.cc.ku.edu/~grobe/docs/intro-MPI-C.shtml;This is a short introduction to the Message Passing Interface designed to convey the fundamental operation and use of the interface.;fundamental_operation;0.6568627450980393;0.13333333333333333;1.0;1.7901960784313726;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;http://condor.cc.ku.edu/~grobe/docs/intro-MPI-C.shtml; This introduction is designed for readers with some background programming C, and should deliver enough information to allow readers to write and run their own parallel C programs using MPI.;background_programming;0.6568627450980393;0.13333333333333333;1.0;1.7901960784313726;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why .;non_blocking_communications;0.7855392156862745;0.0017977528089887641;1.0;1.7873369684952634;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Some further example message queue programs msgget c  Simple Program to illustrate msgetmsgctl cSample Program to Illustrate msgctlmsgop c  Sample Program to Illustrate msgsndand msgrcv.;illustrate_msgctlmsgop;0.7426470588235294;0.0392156862745098;1.0;1.781862745098039;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;msgctl cSample Program to Illustrate msgctl.;illustrate_msgctl;0.7426470588235294;0.0392156862745098;1.0;1.781862745098039;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Because of this, a server process can direct message traffic between clients on its queue by using the client process PID as the message type.;message_type;0.7426470588235294;0.0392156862745098;1.0;1.781862745098039;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Because of this, a server process can direct message traffic between clients on its queue by using the client process PID as the message type.;server_process;0.7426470588235294;0.0392156862745098;1.0;1.781862745098039;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;mpi_calls;0.7712418300659314;0.006292134831460674;1.0;1.777533964897392;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Struct Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66       include  mpi h       include  stdio h       define NELEM 25       main       int numtasks, rank, source 0, dest, tag 1, i        typedef struct         float x, y, z        float velocity        int  n, type                   Particle      Particle     p NELEM , particles NELEM       MPI_Datatype particletype, oldtypes 2        required variables     int          blockcounts 2            MPI_Aint type used to be consistent with syntax of        MPI_Type_extent routine     MPI_Aint    offsets 2 , extent        MPI_Status stat        MPI_Init      MPI_Comm_rank      MPI_Comm_size            setup description of the 4 MPI_FLOAT fields x, y, z, velocity     offsets 0    0      oldtypes 0    MPI_FLOAT      blockcounts 0    4           setup description of the 2 MPI_INT fields n, type        need to first figure offset by getting size of MPI_FLOAT     MPI_Type_extent      offsets 1    4   extent      oldtypes 1    MPI_INT      blockcounts 1    2           define structured type and commit it     MPI_Type_struct      MPI_Type_commit           task 0 initializes the particle array and then sends it to each task     if         for            particles i  x   i   1 0           particles i  y   i   _1 0           particles i  z   i   1 0            particles i  velocity   0 25           particles i  n   i           particles i  type   i   2                    for           MPI_Send                    all tasks receive particletype data     MPI_Recv        printf  rank   d    3 2f  3 2f  3 2f  3 2f  d  d n , rank,p 3  x,          p 3  y,p 3  z,p 3  velocity,p 3  n,p 3  type            free datatype when done using it     MPI_Type_free      MPI_Finalize       .;particle_array;0.7712418300659314;0.0017977528089887641;1.0;1.7730395828749201;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer .;safe;0.7712418300642158;0.0017977528089887641;1.0;1.7730395828732046;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why .;application_buffer;0.7426470588235294;0.0035955056179775282;1.0;1.746242564441507;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The mapping of processes into an MPI virtual topology is dependent upon the MPI implementation, and may be totally ignored.;mpi_implementation;0.7426470588235294;0.002696629213483146;1.0;1.7453436880370126;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; This is followed by a detailed look at the MPI routines that are most useful for new MPI programmers, including MPI Environment Management, Point_to_Point Communications, and Collective Communications routines.;mpi_routines;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The programming model clearly remains a distributed memory model however, regardless of the underlying physical architecture of the machine.;programming_model;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 picked up where the first MPI specification left off, and addressed topics which went far beyond the MPI_1 specification.;mpi_1_specification;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;implement_mpi_3;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Write a simple  Hello World  MPI program using several MPI Environment Management routines.;world__mpi_program;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI point_to_point operations typically involve message passing between two, and only two, different MPI tasks  One task is performing a send operation and the other task is performing a matching receive operation .;send_operation;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;receive_task;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;safe_send;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35       include  mpi h       include  stdio h        main       int numtasks, rank, dest, source, rc, count, tag 1        char inmsg, outmsg  x       MPI_Status Stat       required variable for receive routines       MPI_Init      MPI_Comm_size      MPI_Comm_rank           task 0 sends to task 1 and waits to receive a return message     if         dest   1        source   1        MPI_Send        MPI_Recv                    task 1 waits for task 0 message then returns a message     else if         dest   0        source   0        MPI_Recv        MPI_Send                   query recieve Stat variable and print message details     MPI_Get_count      printffrom task  d with tag  d  n ,            rank, count, Stat MPI_SOURCE, Stat MPI_TAG         MPI_Finalize       .;stat_mpi_source;0.7426470588235294;0.0017977528089887641;1.0;1.7444448116325182;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; This is followed by a detailed look at the MPI routines that are most useful for new MPI programmers, including MPI Environment Management, Point_to_Point Communications, and Collective Communications routines.;point_to_point_communications;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Most MPI routines include a return error code parameter, as described in the  Format of MPI Calls  section above.;mpi_calls__section;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI does not guarantee fairness _ it s up to the programmer to prevent  operation starvation .;prevent__operation_starvation;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The  canonical  evaluation order of a reduction is determined by the ranks of the processes in the group.;canonical__evaluation_order;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Communication Efficiency Some hardware architectures may impose penalties for communications between successively distant  nodes .;successively_distant__nodes;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; One_Sided Communications _ provides routines for one directional communications.;directional_communications;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler .;mpi_libraries;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation .;approximating_pi__inscribe;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;User managed address space is called the application buffer  MPI also provides for a user managed send buffer .;application_buffer__mpi;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Primitive data types are contiguous  Derived data types allow you to specify non_contiguous data in a convenient manner and to treat it as though it was contiguous .;non_contiguous_data;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Author  Blaise Barney, Livermore Computing .;author__blaise_barney;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Using MPI , Gropp, Lusk and Skjellum  MIT Press, 1994 .;skjellum__mit_press;0.7426470588235294;8.988764044943821E-4;1.0;1.7435459352280238;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Synchronous message passing is analogous to a function call in which the message sender is the function caller and the message receiver is the called function.;message_receiver;0.7140522875811274;0.02631578947368421;1.0;1.7403680770548116;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Synchronous message passing is analogous to a function call in which the message sender is the function caller and the message receiver is the called function.;message_sender;0.7140522875811274;0.02631578947368421;1.0;1.7403680770548116;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Some further example message queue programs msgget c  Simple Program to illustrate msgetmsgctl cSample Program to Illustrate msgctlmsgop c  Sample Program to Illustrate msgsndand msgrcv.;sample_program;0.6568627450980393;0.0784313725490196;1.0;1.7352941176470589;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Some further example message queue programs msgget c  Simple Program to illustrate msgetmsgctl cSample Program to Illustrate msgctlmsgop c  Sample Program to Illustrate msgsndand msgrcv.;simple_program;0.6568627450980393;0.0784313725490196;1.0;1.7352941176470589;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;long_double_real;0.7331154684099673;8.988764044943821E-4;1.0;1.7340143448144616;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions .;call_mpi;0.7254901960784315;0.002696629213483146;1.0;1.7281868252919146;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;This group of routines is used for interrogating and setting the MPI execution environment, and covers an assortment of purposes, such as initializing and terminating the MPI environment, querying a rank s identity, querying the MPI library s version, etc.;mpi_library;0.7140522875811274;0.005393258426966292;1.0;1.7194455460080937;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;blocking_send;0.6997549019607843;0.002696629213483146;1.0;1.7024515311742674;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why .;non_blocking__non_blocking_send;0.6997549019607843;0.002696629213483146;1.0;1.7024515311742674;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;IPC Functions, Key Arguments, and Creation Flags   sys ipc h .;ipc_functions;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;IPC Functions, Key Arguments, and Creation Flags   sys ipc h .;key_arguments;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Sending and Receiving Messages.;receiving_messages;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Example  Sending messages between two processes message_send c __ creating and sending to a simple message queue message_rec c __ receiving the above message.;processes_message_send;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;msgget c  Simple Program to illustrate msget.;illustrate_msget;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;1 Basic Message Passing IPC messaging lets processes send and receive messages, and queue messages for processing in an arbitrary order.;queue_messages;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;1 Basic Message Passing IPC messaging lets processes send and receive messages, and queue messages for processing in an arbitrary order.;arbitrary_order;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Unlike the file byte_stream data flow of pipes, each IPC message has an explicit length.;ipc_message;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Unlike the file byte_stream data flow of pipes, each IPC message has an explicit length.;explicit_length;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; Messages can be assigned a specific type.;specific_type;0.6568627450980393;0.0392156862745098;1.0;1.6960784313725492;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;mpi_byte_integer;0.6873638344230393;8.988764044943821E-4;1.0;1.6882627108275337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer .;receive_task__safe;0.6854575163395834;0.0017977528089887641;1.0;1.6872552691485723;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer .;reuse__safe_means;0.6854575163395834;0.0017977528089887641;1.0;1.6872552691485723;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing;In computer science, message passing sends a message to a process and relies on the process and the supporting infrastructure to select and invoke the actual code to run.;supporting_infrastructure;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing;In computer science, message passing sends a message to a process and relies on the process and the supporting infrastructure to select and invoke the actual code to run.;actual_code;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Message passing differs from conventional programming where a process, subroutine, or function is directly invoked by name.;conventional_programming;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Message passing differs from conventional programming where a process, subroutine, or function is directly invoked by name.;directly_invoked;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Synchronous message passing is analogous to a function call in which the message sender is the function caller and the message receiver is the called function.;function_call;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Synchronous message passing is analogous to a function call in which the message sender is the function caller and the message receiver is the called function.;function_caller;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Synchronous message passing is analogous to a function call in which the message sender is the function caller and the message receiver is the called function.;called_function;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Function calling is easy and familiar.;function_calling;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Such large, distributed systems may need to continue to operate while some of their subsystems are down  subsystems may need to go offline for some kind of maintenance, or have times when subsystems are not open to receiving input from other systems.;receiving_input;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; ONC RPC, CORBA, Java RMI, DCOM, SOAP, .;java_rmi;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; ONC RPC, CORBA, Java RMI, DCOM, SOAP, .;onc_rpc;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in java;https://en.wikipedia.org/wiki/Message_passing; Distributed object systems have been called  shared nothing  systems because the message passing abstraction hides underlying state changes that may be used in the implementation of sending messages.;sending_messages;0.6568627450980393;0.02631578947368421;1.0;1.6831785345717236;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Environment Management Routines  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29      program simple         required MPI include file     include  mpif h        integer numtasks, rank, len, ierr       characterhostname         initialize MPI     call MPI_INIT         get number of tasks     call MPI_COMM_SIZE         get my rank     call MPI_COMM_RANK         this one is obvious     call MPI_GET_PROCESSOR_NAME     print  ,  Number of tasks  ,numtasks,  My rank  ,rank,  Running on  ,hostname                do some work with message passing            done with MPI     call MPI_FINALIZE       end.;tasks;0.6568627450980393;0.012584269662921348;1.0;1.6694470147609608;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; C   bindings from MPI_1 are removed in MPI_3 MPI_3 also provides support for Fortran 2003 and 2008 features.;mpi_3_mpi_3;0.6568627450980393;0.0071910112359550565;1.0;1.6640537563339943;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The Message Passing Interface Standard is a message passing library standard based on the consensus of the MPI Forum, which has over 40 participating organizations, including vendors, researchers, software library developers, and users.;mpi_forum;0.6568627450980393;0.005393258426966292;1.0;1.6622560035250056;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The MPI standard has gone through a number of revisions, with the most recent version being MPI_3.;mpi_standard;0.6568627450980393;0.005393258426966292;1.0;1.6622560035250056;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Types of Collective Operations.;collective_operations;0.6568627450980393;0.005393258426966292;1.0;1.6622560035250056;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Send MPI_SEND .;mpi_send_mpi_send;0.6568627450980393;0.0044943820224719105;1.0;1.6613571271205112;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34       include  mpi h       include  stdio h        main       int numtasks, rank, next, prev, buf 2 , tag1 1, tag2 2      MPI_Request reqs 4        required variable for non_blocking calls     MPI_Status stats 4        required variable for Waitall routine       MPI_Init      MPI_Comm_size      MPI_Comm_rank              determine left and right neighbors     prev   rank_1      next   rank 1      if  prev   numtasks _ 1      if    next   0           post non_blocking receives and sends for neighbors     MPI_Irecv      MPI_Irecv        MPI_Isend      MPI_Isend                do some work while sends receives progress in background          wait for all non_blocking operations to complete     MPI_Waitall                continue _ do more work       MPI_Finalize       .;sends;0.6568627450980393;0.0044943820224719105;1.0;1.6613571271205112;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;This ensures that LC s MPI wrapper scripts point to the desired version of Open MPI.;open_mpi;0.6568627450980393;0.0035955056179775282;1.0;1.6604582507160168;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;means;0.6568627450980393;0.0035955056179775282;1.0;1.6604582507160168;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program ping     include  mpif h        integer numtasks, rank, dest, source, count, tag, ierr     integer stat    required variable for receive routines     character inmsg, outmsg     outmsg    x      tag   1       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         task 0 sends to task 1 and waits to receive a return message     if then        dest   1        source   1        call MPI_SEND        call MPI_RECV         task 1 waits for task 0 message then returns a message     else if then        dest   0        source   0        call MPI_RECV        call MPI_SEND     endif         query recieve Stat variable and print message details     call MPI_GET_COUNT     print  ,  Task  ,rank,   Received , count,  charfrom task ,                stat,  with tag ,stat       call MPI_FINALIZE       end.;integer_numtasks;0.6568627450980393;0.0035955056179775282;1.0;1.6604582507160168;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI is not an IEEE or ISO standard, but has in fact, become the  industry standard  for writing message passing programs on HPC platforms.;hpc_platforms;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The tutorial materials also include more advanced topics such as Derived Data Types, Group and Communicator Management Routines, and Virtual Topologies.;virtual_topologies;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;  Most MPI programs can be written using a dozen or less routines.;mpi_programs;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;org docs .;org_docs;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; However, according to the MPI standard, the default behavior of an MPI call is to abort if there is an error.;mpi_call;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Example MPI Program in C.;mpi_program;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Reduce MPI_REDUCE .;mpi_reduce_mpi_reduce;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;system_buffer;0.6568627450980393;0.002696629213483146;1.0;1.6595593743115225;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The Message Passing Interface Standard is a message passing library standard based on the consensus of the MPI Forum, which has over 40 participating organizations, including vendors, researchers, software library developers, and users.;including_vendors;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The goal of the Message Passing Interface is to establish a portable, efficient, and flexible standard for message passing that will be widely used for writing message passing programs.;flexible_standard;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; As such, MPI is the first standardized, vendor independent, message passing library.;vendor_independent;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The advantages of developing message passing software using MPI closely match the design goals of portability, efficiency, and flexibility.;design_goals;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI is not an IEEE or ISO standard, but has in fact, become the  industry standard  for writing message passing programs on HPC platforms.;industry_standard;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI is not an IEEE or ISO standard, but has in fact, become the  industry standard  for writing message passing programs on HPC platforms.;iso_standard;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The primary topics that are presented focus on those which are the most useful for new MPI programmers.;mpi_programmers;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;basics mpi.;basics_mpi;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;This ensures that LC s MPI wrapper scripts point to the desired version of Open MPI.;desired_version;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For now, simply use MPI_COMM_WORLD whenever a communicator is required _ it is the predefined communicator that includes all of your MPI processes.;mpi_processes;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Init MPI_INIT .;mpi_init_mpi_init;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Comm_size MPI_COMM_SIZE .;mpi_comm_size_mpi_comm_size;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Comm_rank MPI_COMM_RANK .;mpi_comm_rank_mpi_comm_rank;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Abort MPI_ABORT .;mpi_abort_mpi_abort;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Get_processor_name MPI_GET_PROCESSOR_NAME .;mpi_get_processor_name_mpi_get_processor_name;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Get_version MPI_GET_VERSION .;mpi_get_version_mpi_get_version;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Initialized MPI_INITIALIZED .;mpi_initialized_mpi_initialized;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Wtime MPI_WTIME .;mpi_wtime_mpi_wtime;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Wtick MPI_WTICK .;mpi_wtick_mpi_wtick;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Finalize MPI_FINALIZE .;mpi_finalize_mpi_finalize;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Recv MPI_RECV .;mpi_recv_mpi_recv;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Ssend MPI_SSEND .;mpi_ssend_mpi_ssend;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Isend MPI_ISEND .;mpi_isend_mpi_isend;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Irecv MPI_IRECV .;mpi_irecv_mpi_irecv;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Issend MPI_ISSEND .;mpi_issend_mpi_issend;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Nearest neighbor exchange in a ring topology.;ring_topology;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Programming Considerations and Restrictions.;programming_considerations;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Barrier MPI_BARRIER .;mpi_barrier_mpi_barrier;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Bcast MPI_BCAST .;mpi_bcast_mpi_bcast;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Users can also define their own reduction functions by using the MPI_Op_create routine.;mpi_op_create_routine;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.9999999999999998;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Users can also define their own reduction functions by using the MPI_Op_create routine.;reduction_functions;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.9999999999999998;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Allreduce MPI_ALLREDUCE .;mpi_allreduce_mpi_allreduce;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Scan MPI_SCAN .;mpi_scan_mpi_scan;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Perform a scatter operation on the rows of an array.;scatter_operation;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_contiguous MPI_TYPE_CONTIGUOUS .;mpi_type_contiguous_mpi_type_contiguous;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_struct MPI_TYPE_STRUCT ,offsets,old_types,newtype,ierr .;mpi_type_struct_mpi_type_struct;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_extent MPI_TYPE_EXTENT .;mpi_type_extent_mpi_type_extent;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_commit MPI_TYPE_COMMIT .;mpi_type_commit_mpi_type_commit;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_free MPI_TYPE_FREE .;mpi_type_free_mpi_type_free;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; In MPI, a group is represented within system memory as an object.;system_memory;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Processes may be in more than one group communicator.;group_communicator;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group Form new group as a subset of global group using MPI_Group_incl Create new communicator for new group using MPI_Comm_create Determine new rank in new communicator using MPI_Comm_rank Conduct communications using any MPI message passing routine When finished, free up new communicator and group using MPI_Comm_free and MPI_Group_free.;mpi_group_incl_create;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Create two different process groups for separate collective communications exchange.;process_groups;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI topologies are virtual _ there may be no relation between the physical structure of the parallel machine and the process topology.;parallel_machine;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A simplified mapping of processes into a Cartesian virtual topology appears below.;simplified_mapping;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Create a 4 x 4 Cartesian topology from 16 processors and have each process exchange its rank with four neighbors.;process_exchange;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Available on all of LC s Linux clusters .;linux_clusters;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;master_thread;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details .;includes_support;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe.;_l_mvapich;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Default version of MPI.;default_version;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The IBM BG Q MPI library is the only supported implementation on these clusters .;supported_implementation;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions .;made_concurrently;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions .;multiple_threads;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions .;distinct_threads;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions .;calls;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread .;main_thread;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Successfully compile your program.;successfully_compile;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Successfully run your program _ several different ways.;successfully_run;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation .;fall_inside;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation .;side_length;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation .;randomly_generate;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation .;points_inside;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation .;points;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Serial pseudo code for this procedure  npoints   10000  circle_count   0    do j   1,npoints    generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do    PI   4 0 circle_count npoints.;inside_circle;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Break the loop iterations into chunks that can be executed by different tasks simultaneously .;tasks_simultaneously;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Break the loop iterations into chunks that can be executed by different tasks simultaneously .;loop_iterations;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Each task executes its portion of the loop a number of times .;task_executes;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Consider the following two cases  A send operation occurs 5 seconds before the receive is ready _ where is the message while the receive is pending  Multiple sends arrive at the same receiving task which can only accept one send at a time _ what happens to the messages that are  backing up  .;receiving_task;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;System buffer space is  Opaque to the programmer and managed entirely by the MPI library A finite resource that can be easy to exhaust Often mysterious and not well documented Able to exist on the sending side, the receiving side, or both Something that may improve program performance because it allows send _ receive operations to be asynchronous .;sending_side;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;System buffer space is  Opaque to the programmer and managed entirely by the MPI library A finite resource that can be easy to exhaust Often mysterious and not well documented Able to exist on the sending side, the receiving side, or both Something that may improve program performance because it allows send _ receive operations to be asynchronous .;receiving_side;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;System buffer space is  Opaque to the programmer and managed entirely by the MPI library A finite resource that can be easy to exhaust Often mysterious and not well documented Able to exist on the sending side, the receiving side, or both Something that may improve program performance because it allows send _ receive operations to be asynchronous .;finite_resource;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;eventual_delivery;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;blocking_receive;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;handshaking_occurring;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why .;overlap_computation;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why .;actual_arrival;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why .;message_copying;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message .;non_blocking_send;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message .;communication_events;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message .;user_memory;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34       include  mpi h       include  stdio h        main       int numtasks, rank, next, prev, buf 2 , tag1 1, tag2 2      MPI_Request reqs 4        required variable for non_blocking calls     MPI_Status stats 4        required variable for Waitall routine       MPI_Init      MPI_Comm_size      MPI_Comm_rank              determine left and right neighbors     prev   rank_1      next   rank 1      if  prev   numtasks _ 1      if    next   0           post non_blocking receives and sends for neighbors     MPI_Irecv      MPI_Irecv        MPI_Isend      MPI_Isend                do some work while sends receives progress in background          wait for all non_blocking operations to complete     MPI_Waitall                continue _ do more work       MPI_Finalize       .;non_blocking_operations;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33       include  mpi h       include  stdio h       define SIZE 4       main       int numtasks, rank, sendcount, recvcount, source      float sendbuf SIZE  SIZE             1 0, 2 0, 3 0, 4 0 ,        5 0, 6 0, 7 0, 8 0 ,        9 0, 10 0, 11 0, 12 0 ,        13 0, 14 0, 15 0, 16 0          float recvbuf SIZE         MPI_Init      MPI_Comm_rank      MPI_Comm_size        if            define source task and elements to send receive, then perform collective scatter       source   1        sendcount   SIZE        recvcount   SIZE        MPI_Scatter sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,                   MPI_FLOAT,source,MPI_COMM_WORLD           printf  rank   d  Results   f  f  f  f n ,rank,recvbuf 0 ,              recvbuf 1 ,recvbuf 2 ,recvbuf 3                else       printf        MPI_Finalize       .;send_receive;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Indexed Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47      program indexed     include  mpif h        integer NELEMENTS     parameter     integer numtasks, rank, source, dest, tag, i,  ierr     integer blocklengths, displacements     real 4 a, b     integer stat     integer indextype     required variable     tag   1       data a   1 0, 2 0, 3 0, 4 0, 5 0, 6 0, 7 0, 8 0,                9 0, 10 0, 11 0, 12 0, 13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       blocklengths  4     blocklengths  2     displacements  5     displacements  12         create indexed derived data type     call MPI_TYPE_INDEXED 2, blocklengths, displacements, MPI_REAL,                             indextype, ierr      call MPI_TYPE_COMMIT         if then          task 0 sends one element of indextype to all tasks        do i 0, numtasks_1        call MPI_SEND        end do     endif         all tasks receive indextype data from task 0     source   0     call MPI_RECV b, NELEMENTS, MPI_REAL, source, tag, MPI_COMM_WORLD,                     stat, ierr      print  ,  rank   ,rank,  b   ,b         free datatype when done using it     call MPI_TYPE_FREE     call MPI_FINALIZE       end.;free_datatype;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Struct Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66       include  mpi h       include  stdio h       define NELEM 25       main       int numtasks, rank, source 0, dest, tag 1, i        typedef struct         float x, y, z        float velocity        int  n, type                   Particle      Particle     p NELEM , particles NELEM       MPI_Datatype particletype, oldtypes 2        required variables     int          blockcounts 2            MPI_Aint type used to be consistent with syntax of        MPI_Type_extent routine     MPI_Aint    offsets 2 , extent        MPI_Status stat        MPI_Init      MPI_Comm_rank      MPI_Comm_size            setup description of the 4 MPI_FLOAT fields x, y, z, velocity     offsets 0    0      oldtypes 0    MPI_FLOAT      blockcounts 0    4           setup description of the 2 MPI_INT fields n, type        need to first figure offset by getting size of MPI_FLOAT     MPI_Type_extent      offsets 1    4   extent      oldtypes 1    MPI_INT      blockcounts 1    2           define structured type and commit it     MPI_Type_struct      MPI_Type_commit           task 0 initializes the particle array and then sends it to each task     if         for            particles i  x   i   1 0           particles i  y   i   _1 0           particles i  z   i   1 0            particles i  velocity   0 25           particles i  n   i           particles i  type   i   2                    for           MPI_Send                    all tasks receive particletype data     MPI_Recv        printf  rank   d    3 2f  3 2f  3 2f  3 2f  d  d n , rank,p 3  x,          p 3  y,p 3  z,p 3  velocity,p 3  n,p 3  type            free datatype when done using it     MPI_Type_free      MPI_Finalize       .;figure_offset;0.6568627450980393;0.0017977528089887641;1.0;1.658660497907028;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The primary topics that are presented focus on those which are the most useful for new MPI programmers.;primary_topics;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The primary topics that are presented focus on those which are the most useful for new MPI programmers.;presented_focus;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The tutorial begins with an introduction, background, and basic information for getting started with MPI.;tutorial_begins;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The tutorial begins with an introduction, background, and basic information for getting started with MPI.;basic_information;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Numerous examples in both C and Fortran are provided, as well as a lab exercise.;numerous_examples;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Numerous examples in both C and Fortran are provided, as well as a lab exercise.;lab_exercise;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The tutorial materials also include more advanced topics such as Derived Data Types, Group and Communicator Management Routines, and Virtual Topologies.;tutorial_materials;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The tutorial materials also include more advanced topics such as Derived Data Types, Group and Communicator Management Routines, and Virtual Topologies.;advanced_topics;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Level Prerequisites.;level_prerequisites;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A basic understanding of parallel programming in C or Fortran is required.;basic_understanding;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For those who are unfamiliar with Parallel Programming in general, the material covered in EC3500.;material_covered;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;An Interface Specification.;interface_specification;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; data is moved from the address space of one process to that of another process through cooperative operations on each process.;cooperative_operations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; data is moved from the address space of one process to that of another process through cooperative operations on each process.;address_space;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Simply stated, the goal of the Message Passing Interface is to provide a widely used standard for writing message passing programs.;simply_stated;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The interface attempts to be.;interface_attempts;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The MPI standard has gone through a number of revisions, with the most recent version being MPI_3.;recent_version;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Interface specifications have been defined for C and Fortran90 language bindings.;interface_specifications;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Developers users will need to be aware of this.;developers_users;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Originally, MPI was designed for distributed memory architectures, which were becoming increasingly popular at that time .;increasingly_popular;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Today, MPI runs on virtually any hardware platform.;hardware_platform;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Today, MPI runs on virtually any hardware platform.;mpi_runs;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; the programmer is responsible for correctly identifying parallelism and implementing parallel algorithms using MPI constructs.;mpi_constructs;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Performance Opportunities _ Vendor implementations should be able to exploit native hardware features to optimize performance.;optimize_performance;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI has resulted from the efforts of numerous individuals and groups that began in 1992.;numerous_individuals;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Distributed memory, parallel computing develops, as do a number of incompatible software tools for writing such programs _ usually with tradeoffs between portability, performance, functionality and price.;distributed_memory;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Recognition of the need for a standard arose.;standard_arose;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The basic features essential to a standard message passing interface were discussed, and a working group established to continue the standardization process.;standardization_process;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI draft proposal from ORNL presented.;ornl_presented;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; It eventually comprised of about 175 individuals from 40 organizations including parallel computer vendors, software writers, academia and application scientists.;application_scientists;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; It eventually comprised of about 175 individuals from 40 organizations including parallel computer vendors, software writers, academia and application scientists.;software_writers;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; It eventually comprised of about 175 individuals from 40 organizations including parallel computer vendors, software writers, academia and application scientists.;eventually_comprised;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Final version of MPI_1.;final_version;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 picked up where the first MPI specification left off, and addressed topics which went far beyond the MPI_1 specification.;mpi_2_picked;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 picked up where the first MPI specification left off, and addressed topics which went far beyond the MPI_1 specification.;addressed_topics;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Currently, LC supports these MPI implementations.;lc_supports;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Currently, LC supports these MPI implementations.;mpi_implementations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI executables are launched using the SLURM srun command with the appropriate options.;mpi_executables;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For example, to launch an 8_process MPI job split across two different nodes in the pdebug pool.;pdebug_pool;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; If you are running a batch job, you will need to load the dotkit module in your batch script.;batch_job;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; If you are running a batch job, you will need to load the dotkit module in your batch script.;dotkit_module;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; If you are running a batch job, you will need to load the dotkit module in your batch script.;batch_script;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Programs must not declare variables or functions with names beginning with the prefix MPI_ or PMPI_ .;declare_variables;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Programs must not declare variables or functions with names beginning with the prefix MPI_ or PMPI_ .;names_beginning;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Programs must not declare variables or functions with names beginning with the prefix MPI_ or PMPI_ .;prefix_mpi_;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; C Binding Format.;binding_format;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A rank is sometimes also called a  task ID .;task_id;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Error Handling.;error_handling;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; However, according to the MPI standard, the default behavior of an MPI call is to abort if there is an error.;default_behavior;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The types of errors displayed to the user are implementation dependent.;errors_displayed;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The types of errors displayed to the user are implementation dependent.;implementation_dependent;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI guarantees that messages will not overtake each other.;mpi_guarantees;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; If a sender sends two messages in succession to the same destination, and both match the same receive, the receive operation will receive Message 1 before Message 2.;sender_sends;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; If a receiver posts two receives , in succession, and both are looking for the same message, Receive 1 will receive the message before Receive 2.;receiver_posts;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Order rules do not apply if there are multiple threads participating in the communication operations.;communication_operations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Order rules do not apply if there are multiple threads participating in the communication operations.;order_rules;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; However, task 1 sends a competing message that matches task 2 s receive.;competing_message;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_BYTE and MPI_PACKED do not correspond to standard C or Fortran types.;fortran_types;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Types shown in GRAY FONT are recommended if possible.;gray_font;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Types shown in GRAY FONT are recommended if possible.;types_shown;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_Probe status.;mpi_probe_status;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Probe MPI_PROBE .;mpi_probe_mpi_probe;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_Get_count status.;mpi_get_count_status;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Get_count MPI_GET_COUNT .;mpi_get_count_mpi_get_count;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_Iprobe status.;mpi_iprobe_status;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Iprobe MPI_IPROBE .;mpi_iprobe_mpi_iprobe;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Synchronization _ processes wait until all members of the group have reached the synchronization point.;synchronization_point;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Data Movement _ broadcast, scatter gather, all to all.;scatter_gather;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; All processes are by default, members in the communicator MPI_COMM_WORLD.;communicator_mpi_comm_world;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Additional communicators can be defined by the programmer.;additional_communicators;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Unexpected behavior, including program failure, can occur if even one task in the communicator doesn t participate.;communicator_doesn;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Unexpected behavior, including program failure, can occur if even one task in the communicator doesn t participate.;unexpected_behavior;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; It is the programmer s responsibility to ensure that all processes within a communicator participate in any collective operations.;communicator_participate;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 extended most collective operations to allow data movement between intercommunicators .;mpi_2_extended;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 extended most collective operations to allow data movement between intercommunicators .;data_movement;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Only blocking operations are covered in this tutorial.;blocking_operations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;location_float;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; All predefined operations are also assumed to be commutative.;predefined_operations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Users may define operations that are assumed to be associative, but not commutative.;define_operations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; This may change the result of the reduction for operations that are not strictly associative and commutative, such as floating point addition.;strictly_associative;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;  Advice to implementors  It is strongly recommended that MPI_REDUCE be implemented so that the same result be obtained whenever the function is applied on the same arguments, appearing in the same order.;strongly_recommended;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Note that this may prevent optimizations that take advantage of the physical location of processors.;physical_location;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Note that this may prevent optimizations that take advantage of the physical location of processors.;prevent_optimizations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_Type_vector MPI_Type_hvector .;mpi_type_vector_mpi_type_hvector;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_vector MPI_TYPE_VECTOR .;mpi_type_vector_mpi_type_vector;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_Type_indexed MPI_Type_hindexed .;mpi_type_indexed_mpi_type_hindexed;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_Type_indexed MPI_TYPE_INDEXED ,offsets,old_type,newtype,ierr .;mpi_type_indexed_mpi_type_indexed;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A group is an ordered set of processes.;ordered_set;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A group is always associated with a communicator object.;communicator_object;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A communicator encompasses a group of processes that may communicate with each other.;communicator_encompasses;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; All MPI messages must specify a communicator.;mpi_messages;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; In the simplest sense, the communicator is an extra  tag  that must be included with MPI calls.;simplest_sense;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The group routines are primarily used to specify which processes should be used to construct a communicator.;group_routines;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Primary Purposes of Group and Communicator Objects.;primary_purposes;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Primary Purposes of Group and Communicator Objects.;communicator_objects;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Groups communicators are dynamic _ they can be created and destroyed during program execution.;groups_communicators;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Groups communicators are dynamic _ they can be created and destroyed during program execution.;program_execution;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; They will have a unique rank within each group communicator.;unique_rank;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Typical usage.;typical_usage;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group Form new group as a subset of global group using MPI_Group_incl Create new communicator for new group using MPI_Comm_create Determine new rank in new communicator using MPI_Comm_rank Conduct communications using any MPI message passing routine When finished, free up new communicator and group using MPI_Comm_free and MPI_Group_free.;mpi_comm_group_form;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group Form new group as a subset of global group using MPI_Group_incl Create new communicator for new group using MPI_Comm_create Determine new rank in new communicator using MPI_Comm_rank Conduct communications using any MPI message passing routine When finished, free up new communicator and group using MPI_Comm_free and MPI_Group_free.;extract_handle;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group Form new group as a subset of global group using MPI_Group_incl Create new communicator for new group using MPI_Comm_create Determine new rank in new communicator using MPI_Comm_rank Conduct communications using any MPI message passing routine When finished, free up new communicator and group using MPI_Comm_free and MPI_Group_free.;mpi_comm_create_determine;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Requires creating new communicators also.;requires_creating;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; In terms of MPI, a virtual topology describes a mapping ordering of MPI processes into a geometric  shape .;mapping_ordering;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The two main types of topologies supported by MPI are Cartesian and Graph.;topologies_supported;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The two main types of topologies supported by MPI are Cartesian and Graph.;main_types;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI topologies are virtual _ there may be no relation between the physical structure of the parallel machine and the process topology.;mpi_topologies;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI topologies are virtual _ there may be no relation between the physical structure of the parallel machine and the process topology.;process_topology;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI topologies are virtual _ there may be no relation between the physical structure of the parallel machine and the process topology.;physical_structure;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Virtual topologies are built upon MPI communicators and groups.;mpi_communicators;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Must be  programmed  by the application developer.;application_developer;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For example, a Cartesian topology might prove convenient for an application that requires 4_way nearest neighbor communications for grid based data.;prove_convenient;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For example, a Cartesian topology might prove convenient for an application that requires 4_way nearest neighbor communications for grid based data.;cartesian_topology;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Communication Efficiency Some hardware architectures may impose penalties for communications between successively distant  nodes .;impose_penalties;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Communication Efficiency Some hardware architectures may impose penalties for communications between successively distant  nodes .;hardware_architectures;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Communication Efficiency Some hardware architectures may impose penalties for communications between successively distant  nodes .;communication_efficiency;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A particular implementation may optimize process mapping based upon the physical characteristics of a given parallel machine.;physical_characteristics;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For reasons of expediency, these issues were deferred to a second specification, called MPI_2 in 1998.;called_mpi_2;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 was a major revision to MPI_1 adding new functionality and corrections.;mpi_1_adding;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI_2 was a major revision to MPI_1 adding new functionality and corrections.;major_revision;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Key areas of new functionality in MPI_2.;key_areas;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Provides routines to create new processes after job startup.;job_startup;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The MPI_3 standard was adopted in 2012, and contains significant extensions to MPI_1 and MPI_2 functionality including.;significant_extensions;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; The MPI_3 standard was adopted in 2012, and contains significant extensions to MPI_1 and MPI_2 functionality including.;mpi_3_standard;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Nonblocking Collective Operations _ permits tasks in a collective to perform operations without blocking, possibly offering performance improvements.;perform_operations;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; New One_sided Communication Operations _ to better handle different memory models.;memory_models;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Neighborhood Collectives _ extends the distributed graph and Cartesian process topologies with additional communication power.;distributed_graph;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Fortran 2008 Bindings _ expanded from Fortran90 bindings MPIT Tool Interface _ allows the MPI implementation to expose certain internal variables, counters, and other states to the user .;internal_variables;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Matched Probe _ fixes an old bug in MPI_2 where one could not probe for messages in a multi_threaded environment.;multi_threaded_environment;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MVAPICH MPI is developed and supported by the Network_Based Computing Lab at Ohio State University .;mvapich_mpi;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI_1 implementation that also includes support for MPI_I O.;mpi_1_implementation;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Multiple versions available.;multiple_versions;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Open MPI is a thread_safe, open source MPI implementation developed and supported by a consortium of academic, research, and industry partners .;industry_partners;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Available on all LC Linux clusters  However, you ll need to load the desired dotkit or module first  For example  dotkit   use _l openmpi                 use openmpi_gnu_1 8 4        module   module avail                   module load openmpi 2 0 0    This ensures that LC s MPI wrapper scripts point to the desired version of Open MPI .;_l_openmpi;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Compiling and running Intel MPI programs  see the LC documentation at  https   lc llnl gov confluence pages viewpage action pageId 137725526.;lc_documentation;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The IBM BG Q MPI library is the only supported implementation on these clusters .;ibm_bg;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler .;automatically_perform;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler .;underlying_compiler;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler .;pass_options;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler .;error_checks;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded.;additional_information;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;See the man page .;man_page;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Issue the script name with the _help option.;_help_option;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Consult the MPI_Init_threadman page for details .;mpi_init_threadman_page;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;With MPI_3 Fortran, the USE mpi_f08 module is preferred over using the include file shown above .;mpi_3_fortran;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;With MPI_3 Fortran, the USE mpi_f08 module is preferred over using the include file shown above .;mpi_f08_module;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Login to an LC cluster using your workshop username and OTP token.;workshop_username;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Login to an LC cluster using your workshop username and OTP token.;otp_token;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Login to an LC cluster using your workshop username and OTP token.;lc_cluster;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Copy the exercise files to your home directory.;exercise_files;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Copy the exercise files to your home directory.;home_directory;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Familiarize yourself with LC s MPI compilers.;mpi_compilers;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Pseudo code solution  red highlights changes for parallelism  npoints   10000  circle_count   0   p   number of tasks num   npoints p find out if I am MASTER or WORKER     do j   1,num     generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do   if I am MASTER receive from WORKERS their circle_counts compute PI else if I am WORKER send to MASTER circle_count endif Example MPI Program in C    mpi_pi_reduce c Example MPI Program in Fortran    mpi_pi_reduce f.;master_receive;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Key Concept  Divide work between available tasks which communicate data via point_to_point message passing calls .;communicate_data;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;MPI point_to_point operations typically involve message passing between two, and only two, different MPI tasks  One task is performing a send operation and the other task is performing a matching receive operation .;mpi_tasks;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;There are different types of send and receive routines used for different purposes  For example  Synchronous send Blocking send   blocking receive Non_blocking send   non_blocking receive Buffered send Combined send receive  Ready  send.;receive_routines;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Any type of send routine can be paired with any type of receive routine .;receive_routine;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Any type of send routine can be paired with any type of receive routine .;send_routine;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Synchronous send.;synchronous_send;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Buffered send.;buffered_send;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;In a perfect world, every send operation would be perfectly synchronized with its matching receive  This is rarely the case  Somehow or other, the MPI implementation must be able to deal with storing data when the two tasks are out of sync .;matching_receive;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;In a perfect world, every send operation would be perfectly synchronized with its matching receive  This is rarely the case  Somehow or other, the MPI implementation must be able to deal with storing data when the two tasks are out of sync .;perfect_world;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;In a perfect world, every send operation would be perfectly synchronized with its matching receive  This is rarely the case  Somehow or other, the MPI implementation must be able to deal with storing data when the two tasks are out of sync .;perfectly_synchronized;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;In a perfect world, every send operation would be perfectly synchronized with its matching receive  This is rarely the case  Somehow or other, the MPI implementation must be able to deal with storing data when the two tasks are out of sync .;storing_data;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Most of the MPI point_to_point routines can be used in either blocking or non_blocking mode .;non_blocking_mode;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;As previously mentioned, MPI predefines its primitive data types  C Data Types Fortran Data Types   MPI_CHAR  MPI_WCHAR  MPI_SHORT  MPI_INT  MPI_LONG  MPI_LONG_LONG_INT   MPI_LONG_LONG      MPI_SIGNED_CHAR  MPI_UNSIGNED_CHAR  MPI_UNSIGNED_SHORT  MPI_UNSIGNED_LONG  MPI_UNSIGNED  MPI_FLOAT  MPI_DOUBLE  MPI_LONG_DOUBLE     MPI_C_COMPLEX  MPI_C_FLOAT_COMPLEX  MPI_C_DOUBLE_COMPLEX  MPI_C_LONG_DOUBLE_COMPLEX      MPI_C_BOOL  MPI_LOGICAL  MPI_C_LONG_DOUBLE_COMPLEX     MPI_INT8_T   MPI_INT16_T  MPI_INT32_T   MPI_INT64_T      MPI_UINT8_T   MPI_UINT16_T   MPI_UINT32_T   MPI_UINT64_T  MPI_BYTE  MPI_PACKED     MPI_CHARACTER  MPI_INTEGER  MPI_INTEGER1   MPI_INTEGER2  MPI_INTEGER4  MPI_REAL  MPI_REAL2   MPI_REAL4  MPI_REAL8  MPI_DOUBLE_PRECISION  MPI_COMPLEX  MPI_DOUBLE_COMPLEX  MPI_LOGICAL  MPI_BYTE  MPI_PACKED.;previously_mentioned;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;As previously mentioned, MPI predefines its primitive data types  C Data Types Fortran Data Types   MPI_CHAR  MPI_WCHAR  MPI_SHORT  MPI_INT  MPI_LONG  MPI_LONG_LONG_INT   MPI_LONG_LONG      MPI_SIGNED_CHAR  MPI_UNSIGNED_CHAR  MPI_UNSIGNED_SHORT  MPI_UNSIGNED_LONG  MPI_UNSIGNED  MPI_FLOAT  MPI_DOUBLE  MPI_LONG_DOUBLE     MPI_C_COMPLEX  MPI_C_FLOAT_COMPLEX  MPI_C_DOUBLE_COMPLEX  MPI_C_LONG_DOUBLE_COMPLEX      MPI_C_BOOL  MPI_LOGICAL  MPI_C_LONG_DOUBLE_COMPLEX     MPI_INT8_T   MPI_INT16_T  MPI_INT32_T   MPI_INT64_T      MPI_UINT8_T   MPI_UINT16_T   MPI_UINT32_T   MPI_UINT64_T  MPI_BYTE  MPI_PACKED     MPI_CHARACTER  MPI_INTEGER  MPI_INTEGER1   MPI_INTEGER2  MPI_INTEGER4  MPI_REAL  MPI_REAL2   MPI_REAL4  MPI_REAL8  MPI_DOUBLE_PRECISION  MPI_COMPLEX  MPI_DOUBLE_COMPLEX  MPI_LOGICAL  MPI_BYTE  MPI_PACKED.;mpi_predefines;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Primitive data types are contiguous  Derived data types allow you to specify non_contiguous data in a convenient manner and to treat it as though it was contiguous .;convenient_manner;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Group and Communicator Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42      program group     include  mpif h        integer NPROCS     parameter     integer rank, new_rank, sendbuf, recvbuf, numtasks     integer ranks1, ranks2, ierr     integer orig_group, new_group, new_comm     required variables     data ranks1  0, 1, 2, 3 , ranks2  4, 5, 6, 7        call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then       print  ,  Must specify NPROCS   ,NPROCS,  Terminating         call MPI_FINALIZE       stop     endif       sendbuf   rank         extract the original group handle     call MPI_COMM_GROUP         divide tasks into two distinct groups based upon rank     if then        call MPI_GROUP_INCL     else         call MPI_GROUP_INCL     endif         create new new communicator and then perform collective communications     call MPI_COMM_CREATE     call MPI_ALLREDUCE         get rank in new group     call MPI_GROUP_RANK     print  ,  rank   ,rank,  newrank   ,new_rank,  recvbuf   , recvbuf       call MPI_FINALIZE     end.;call_mpi_group_incl;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Cartesian Virtual Topology Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58      program cartesian     include  mpif h        integer SIZE, UP, DOWN, LEFT, RIGHT     parameter     parameter     parameter     parameter     parameter     integer numtasks, rank, source, dest, outbuf, i, tag, ierr,               inbuf, nbrs, dims, coords, periods, reorder     integer stats, reqs     integer cartcomm     required variable     data inbuf  MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL ,            dims  4,4 , tag  1 , periods  0,0 , reorder  0         call MPI_INIT     call MPI_COMM_SIZE         if then          create cartesian virtual topology, get rank, coordinates, neighbor ranks        call MPI_CART_CREATE MPI_COMM_WORLD, 2, dims, periods, reorder,                               cartcomm, ierr         call MPI_COMM_RANK        call MPI_CART_COORDS        call MPI_CART_SHIFT, nbrs, ierr         call MPI_CART_SHIFT, nbrs, ierr           writerank,coords,coords,nbrs,nbrs,                      nbrs,nbrs            exchange data with 4 neighbors        outbuf   rank        do i 1,4           dest   nbrs           source   nbrs           call MPI_ISEND outbuf, 1, MPI_INTEGER, dest, tag,                           MPI_COMM_WORLD, reqs, ierr            call MPI_IRECV, 1, MPI_INTEGER, source, tag,                           MPI_COMM_WORLD, reqs, ierr         enddo          call MPI_WAITALL          writerank,inbuf       else       print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       20 format  rank   ,I3,  coords   ,I2,I2,                   neighbors   ,I3,I3,I3,I3       30 format  rank   ,I3,                   ,                   inbuf   ,I3,I3,I3,I3         end.;integer_size;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Cartesian Virtual Topology Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58      program cartesian     include  mpif h        integer SIZE, UP, DOWN, LEFT, RIGHT     parameter     parameter     parameter     parameter     parameter     integer numtasks, rank, source, dest, outbuf, i, tag, ierr,               inbuf, nbrs, dims, coords, periods, reorder     integer stats, reqs     integer cartcomm     required variable     data inbuf  MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL ,            dims  4,4 , tag  1 , periods  0,0 , reorder  0         call MPI_INIT     call MPI_COMM_SIZE         if then          create cartesian virtual topology, get rank, coordinates, neighbor ranks        call MPI_CART_CREATE MPI_COMM_WORLD, 2, dims, periods, reorder,                               cartcomm, ierr         call MPI_COMM_RANK        call MPI_CART_COORDS        call MPI_CART_SHIFT, nbrs, ierr         call MPI_CART_SHIFT, nbrs, ierr           writerank,coords,coords,nbrs,nbrs,                      nbrs,nbrs            exchange data with 4 neighbors        outbuf   rank        do i 1,4           dest   nbrs           source   nbrs           call MPI_ISEND outbuf, 1, MPI_INTEGER, dest, tag,                           MPI_COMM_WORLD, reqs, ierr            call MPI_IRECV, 1, MPI_INTEGER, source, tag,                           MPI_COMM_WORLD, reqs, ierr         enddo          call MPI_WAITALL          writerank,inbuf       else       print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       20 format  rank   ,I3,  coords   ,I2,I2,                   neighbors   ,I3,I3,I3,I3       30 format  rank   ,I3,                   ,                   inbuf   ,I3,I3,I3,I3         end.;reorder;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Author  Blaise Barney, Livermore Computing .;livermore_computing;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; A User s Guide to MPI , Peter S  Pacheco  Department of Mathematics, University of San Francisco .;san_francisco;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;These man pages were derived from the MVAPICH 0 9 implementation of MPI and may differ from the man pages of other implementations .;man_pages;0.6568627450980393;8.988764044943821E-4;1.0;1.6577616215025337;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Example  Sending messages between two processes message_send c __ creating and sending to a simple message queue message_rec c __ receiving the above message.;___creating;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;Example  Sending messages between two processes message_send c __ creating and sending to a simple message queue message_rec c __ receiving the above message.;___receiving;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Portability _ There is little or no need to modify your source code when you port your application to a different platform that supports the MPI standard.;portability__;0.6568627450980393;0.0;1.0;1.6568627450980393;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Availability _ A variety of implementations are available, both vendor and public domain.;availability__;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For now, simply use MPI_COMM_WORLD whenever a communicator is required _ it is the predefined communicator that includes all of your MPI processes.;required__;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI topologies are virtual _ there may be no relation between the physical structure of the parallel machine and the process topology.;virtual__;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded.;mpicxx;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Consider the following two cases  A send operation occurs 5 seconds before the receive is ready _ where is the message while the receive is pending  Multiple sends arrive at the same receiving task which can only accept one send at a time _ what happens to the messages that are  backing up  .;ready__;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program .;received__;0.6568627450980393;0.0;1.0;1.6568627450980393;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34       include  mpi h       include  stdio h        main       int numtasks, rank, next, prev, buf 2 , tag1 1, tag2 2      MPI_Request reqs 4        required variable for non_blocking calls     MPI_Status stats 4        required variable for Waitall routine       MPI_Init      MPI_Comm_size      MPI_Comm_rank              determine left and right neighbors     prev   rank_1      next   rank 1      if  prev   numtasks _ 1      if    next   0           post non_blocking receives and sends for neighbors     MPI_Irecv      MPI_Irecv        MPI_Isend      MPI_Isend                do some work while sends receives progress in background          wait for all non_blocking operations to complete     MPI_Waitall                continue _ do more work       MPI_Finalize       .;prev;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Indexed Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47      program indexed     include  mpif h        integer NELEMENTS     parameter     integer numtasks, rank, source, dest, tag, i,  ierr     integer blocklengths, displacements     real 4 a, b     integer stat     integer indextype     required variable     tag   1       data a   1 0, 2 0, 3 0, 4 0, 5 0, 6 0, 7 0, 8 0,                9 0, 10 0, 11 0, 12 0, 13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       blocklengths  4     blocklengths  2     displacements  5     displacements  12         create indexed derived data type     call MPI_TYPE_INDEXED 2, blocklengths, displacements, MPI_REAL,                             indextype, ierr      call MPI_TYPE_COMMIT         if then          task 0 sends one element of indextype to all tasks        do i 0, numtasks_1        call MPI_SEND        end do     endif         all tasks receive indextype data from task 0     source   0     call MPI_RECV b, NELEMENTS, MPI_REAL, source, tag, MPI_COMM_WORLD,                     stat, ierr      print  ,  rank   ,rank,  b   ,b         free datatype when done using it     call MPI_TYPE_FREE     call MPI_FINALIZE       end.;blocklengths;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The intention is review the codes and see what s happening _ not just compile and run .;happening__;0.6568627450980393;0.0;1.0;1.6568627450980393;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html;POSIX Messages   mqueue h .;posix_messages___mqueue;0.5882352941176471;0.0392156862745098;1.0;1.6274509803921569;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; MPI Reduction Operation C Data Types Fortran Data Type MPI_MAX maximum integer, float integer, real, complex MPI_MIN minimum integer, float integer, real, complex MPI_SUM sum integer, float integer, real, complex MPI_PROD product integer, float integer, real, complex MPI_LAND logical AND integer logical MPI_BAND bit_wise AND integer, MPI_BYTE integer, MPI_BYTE MPI_LOR logical OR integer logical MPI_BOR bit_wise OR integer, MPI_BYTE integer, MPI_BYTE MPI_LXOR logical XOR integer logical MPI_BXOR bit_wise XOR integer, MPI_BYTE integer, MPI_BYTE MPI_MAXLOC max value and location float, double and long double real, complex,double precision MPI_MINLOC min value and location float, double and long double real, complex, double precision.;float_integer;0.6187363834426471;8.988764044943821E-4;1.0;1.6196352598471415;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler .;mpi__include_files;0.6139705882352942;8.988764044943821E-4;1.0;1.6148694646397885;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://users.cs.cf.ac.uk/Dave.Marshall/C/node25.html; For single_message transactions, multiple server processes can work in parallel on transactions sent to a shared message queue.;single_message_transactions;0.5710784313725491;0.0392156862745098;1.0;1.6102941176470589;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35       include  mpi h       include  stdio h        main       int numtasks, rank, dest, source, rc, count, tag 1        char inmsg, outmsg  x       MPI_Status Stat       required variable for receive routines       MPI_Init      MPI_Comm_size      MPI_Comm_rank           task 0 sends to task 1 and waits to receive a return message     if         dest   1        source   1        MPI_Send        MPI_Recv                    task 1 waits for task 0 message then returns a message     else if         dest   0        source   0        MPI_Recv        MPI_Send                   query recieve Stat variable and print message details     MPI_Get_count      printffrom task  d with tag  d  n ,            rank, count, Stat MPI_SOURCE, Stat MPI_TAG         MPI_Finalize       .;return_message;0.599673202614951;0.0035955056179775282;1.0;1.6032687082329287;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; If a sender sends two messages in succession to the same destination, and both match the same receive, the receive operation will receive Message 1 before Message 2.;receive_operation;0.599673202614951;8.988764044943821E-4;1.0;1.6005720790194453;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; rc   MPI_Xxxxx parameter, .;rc___mpi_xxxxx_parameter;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; rc   MPI_BsendError code.;rc___mpi_bsenderror_code;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; comm, status  MPI_SENDRECV  sendbuf,sendcount,sendtype,dest,sendtag, .;status__mpi_sendrecv__sendbuf;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; ierr  MPI_WAITSOME  incount,array_of_requests,outcount, .;ierr__mpi_waitsome__incount;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; recvcnt,recvtype,root,comm  MPI_SCATTER  sendbuf,sendcnt,sendtype,recvbuf, .;comm__mpi_scatter__sendbuf;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; recvcount,recvtype,root,comm  MPI_GATHER  sendbuf,sendcnt,sendtype,recvbuf, .;comm__mpi_gather__sendbuf;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; recvcount,recvtype,comm  MPI_ALLGATHER  sendbuf,sendcount,sendtype,recvbuf, .;comm__mpi_allgather__sendbuf;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; op,comm  MPI_REDUCE_SCATTER  sendbuf,recvbuf,recvcount,datatype, .;comm__mpi_reduce_scatter__sendbuf;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; recvcnt,recvtype,comm  MPI_ALLTOALL  sendbuf,sendcount,sendtype,recvbuf, .;comm__mpi_alltoall__sendbuf;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Pseudo code solution  red highlights changes for parallelism  npoints   10000  circle_count   0   p   number of tasks num   npoints p find out if I am MASTER or WORKER     do j   1,num     generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do   if I am MASTER receive from WORKERS their circle_counts compute PI else if I am WORKER send to MASTER circle_count endif Example MPI Program in C    mpi_pi_reduce c Example MPI Program in Fortran    mpi_pi_reduce f.;tasks_num___npoints;0.5882352941176471;8.988764044943821E-4;1.0;1.5891341705221413;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer .;data_intended;0.5710784313725491;0.0017977528089887641;1.0;1.572876184181538;1.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program ping     include  mpif h        integer numtasks, rank, dest, source, count, tag, ierr     integer stat    required variable for receive routines     character inmsg, outmsg     outmsg    x      tag   1       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         task 0 sends to task 1 and waits to receive a return message     if then        dest   1        source   1        call MPI_SEND        call MPI_RECV         task 1 waits for task 0 message then returns a message     else if then        dest   0        source   0        call MPI_RECV        call MPI_SEND     endif         query recieve Stat variable and print message details     call MPI_GET_COUNT     print  ,  Task  ,rank,   Received , count,  charfrom task ,                stat,  with tag ,stat       call MPI_FINALIZE       end.;charfrom_task;0.5710784313725491;0.0017977528089887641;1.0;1.572876184181538;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; For now, simply use MPI_COMM_WORLD whenever a communicator is required _ it is the predefined communicator that includes all of your MPI processes.;predefined_communicator;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Within a communicator, every process has its own unique, integer identifier assigned by the system when the process initializes.;process_initializes;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Available on all LC Linux clusters  However, you ll need to load the desired dotkit or module first  For example  dotkit   use _l openmpi                 use openmpi_gnu_1 8 4        module   module avail                   module load openmpi 2 0 0    This ensures that LC s MPI wrapper scripts point to the desired version of Open MPI .;desired_dotkit;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;Pseudo code solution  red highlights changes for parallelism  npoints   10000  circle_count   0   p   number of tasks num   npoints p find out if I am MASTER or WORKER     do j   1,num     generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do   if I am MASTER receive from WORKERS their circle_counts compute PI else if I am WORKER send to MASTER circle_count endif Example MPI Program in C    mpi_pi_reduce c Example MPI Program in Fortran    mpi_pi_reduce f.;worker_send;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;The MPI implementation decides what happens to data in these types of cases  Typically, a system buffer area is reserved to hold data in transit  For example .;hold_data;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program scatter     include  mpif h        integer SIZE     parameter     integer numtasks, rank, sendcount, recvcount, source, ierr     real 4 sendbuf, recvbuf         Fortran stores this array in column major order, so the        scatter will actually scatter columns, not rows      data sendbuf  1 0, 2 0, 3 0, 4 0,                     5 0, 6 0, 7 0, 8 0,                     9 0, 10 0, 11 0, 12 0,                     13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then          define source task and elements to send receive, then perform collective scatter        source   1        sendcount   SIZE        recvcount   SIZE        call MPI_SCATTER sendbuf, sendcount, MPI_REAL, recvbuf, recvcount, MPI_REAL,                           source, MPI_COMM_WORLD, ierr           print  ,  rank   ,rank,  Results   ,recvbuf        else        print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       end.;scatter_columns;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Cartesian Virtual Topology Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54       include  mpi h       include  stdio h       define SIZE 16      define UP    0      define DOWN  1      define LEFT  2      define RIGHT 3       main       int numtasks, rank, source, dest, outbuf, i, tag 1,         inbuf 4   MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL, ,         nbrs 4 , dims 2   4,4 ,         periods 2   0,0 , reorder 0, coords 2         MPI_Request reqs 8       MPI_Status stats 8       MPI_Comm cartcomm       required variable       MPI_Init      MPI_Comm_size        if             create cartesian virtual topology, get rank, coordinates, neighbor ranks        MPI_Cart_create         MPI_Comm_rank         MPI_Cart_coords         MPI_Cart_shift         MPI_Cart_shift           printf   d  d  d  d n ,               rank,coords 0 ,coords 1 ,nbrs UP ,nbrs DOWN ,nbrs LEFT ,               nbrs RIGHT             outbuf   rank              exchange data with 4 neighbors        for             dest   nbrs i             source   nbrs i             MPI_Isend  outbuf, 1, MPI_INT, dest, tag,                      MPI_COMM_WORLD,  reqs i              MPI_Irecv  inbuf i , 1, MPI_INT, source, tag,                      MPI_COMM_WORLD,  reqs i 4                         MPI_Waitall              printf   d  d  d  d n ,               rank,inbuf UP ,inbuf DOWN ,inbuf LEFT ,inbuf RIGHT           else        printf           MPI_Finalize       .;inbuf_left;0.5710784313725491;8.988764044943821E-4;1.0;1.5719773077770434;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     C Language _ Cartesian Virtual Topology Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54       include  mpi h       include  stdio h       define SIZE 16      define UP    0      define DOWN  1      define LEFT  2      define RIGHT 3       main       int numtasks, rank, source, dest, outbuf, i, tag 1,         inbuf 4   MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL, ,         nbrs 4 , dims 2   4,4 ,         periods 2   0,0 , reorder 0, coords 2         MPI_Request reqs 8       MPI_Status stats 8       MPI_Comm cartcomm       required variable       MPI_Init      MPI_Comm_size        if             create cartesian virtual topology, get rank, coordinates, neighbor ranks        MPI_Cart_create         MPI_Comm_rank         MPI_Cart_coords         MPI_Cart_shift         MPI_Cart_shift           printf   d  d  d  d n ,               rank,coords 0 ,coords 1 ,nbrs UP ,nbrs DOWN ,nbrs LEFT ,               nbrs RIGHT             outbuf   rank              exchange data with 4 neighbors        for             dest   nbrs i             source   nbrs i             MPI_Isend  outbuf, 1, MPI_INT, dest, tag,                      MPI_COMM_WORLD,  reqs i              MPI_Irecv  inbuf i , 1, MPI_INT, source, tag,                      MPI_COMM_WORLD,  reqs i 4                         MPI_Waitall              printf   d  d  d  d n ,               rank,inbuf UP ,inbuf DOWN ,inbuf LEFT ,inbuf RIGHT           else        printf           MPI_Finalize       .;nbrs_left;0.5588235294115197;8.988764044943821E-4;1.0;1.559722405816014;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/; Extract handle of global group from MPI_COMM_WORLD using MPI_Comm_group Form new group as a subset of global group using MPI_Group_incl Create new communicator for new group using MPI_Comm_create Determine new rank in new communicator using MPI_Comm_rank Conduct communications using any MPI message passing routine When finished, free up new communicator and group using MPI_Comm_free and MPI_Group_free.;global_group;0.553921568627451;8.988764044943821E-4;1.0;1.5548204450319454;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing in c++;https://computing.llnl.gov/tutorials/mpi/;     Fortran _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program scatter     include  mpif h        integer SIZE     parameter     integer numtasks, rank, sendcount, recvcount, source, ierr     real 4 sendbuf, recvbuf         Fortran stores this array in column major order, so the        scatter will actually scatter columns, not rows      data sendbuf  1 0, 2 0, 3 0, 4 0,                     5 0, 6 0, 7 0, 8 0,                     9 0, 10 0, 11 0, 12 0,                     13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then          define source task and elements to send receive, then perform collective scatter        source   1        sendcount   SIZE        recvcount   SIZE        call MPI_SCATTER sendbuf, sendcount, MPI_REAL, recvbuf, recvcount, MPI_REAL,                           source, MPI_COMM_WORLD, ierr           print  ,  rank   ,rank,  Results   ,recvbuf        else        print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       end.;recvbuf;0.5424836601318628;0.0;1.0;1.5424836601318628;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;https://www.defit.org/message-passing/; In this model, processes or objects can send and receive messages to other processes or objects.;receive_messages;0.6568627450980393;0.07407407407407407;0.7799999999999999;1.5109368191721133;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;https://www.defit.org/message-passing/; In asynchronous communication the sender and receiver do not wait for each other and can carry on their own computations while transfer of messages is being done.;asynchronous_communication;0.6568627450980393;0.07407407407407407;0.7799999999999999;1.5109368191721133;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Used for thread communication and synchronization in environments where the threads do not have shared memory Hence the threads cannot share semaphores or monitors and cannot use shared variables to communicate.;thread_communication;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Used for thread communication and synchronization in environments where the threads do not have shared memory Hence the threads cannot share semaphores or monitors and cannot use shared variables to communicate.;share_semaphores;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Used for thread communication and synchronization in environments where the threads do not have shared memory Hence the threads cannot share semaphores or monitors and cannot use shared variables to communicate.;shared_variables;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Messages are sent through a channel with an operation like sendand received from a channel with an operation like receive.;sendand_received;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Messages can be passed synchronously, meaning the sender blocks until the received does a receive and the receiver blocks until the sender does a send.;receiver_blocks;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Messages can be passed synchronously, meaning the sender blocks until the received does a receive and the receiver blocks until the sender does a send.;passed_synchronously;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Since the sender and receiver are at specific known points in their code at a known specific instant of time, synchronous message passing is also called a simple rendezvous with a one_way flow of information from the sender to the receiver.;one_way_flow;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Since the sender and receiver are at specific known points in their code at a known specific instant of time, synchronous message passing is also called a simple rendezvous with a one_way flow of information from the sender to the receiver.;simple_rendezvous;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; The agents can process messages synchronously, since they ll be handshaking throughout the entire game.;entire_game;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; If there is not a receiver waiting to receive the message, the message is queued or buffered.;receiver_waiting;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; The receiver still blocks if there is no queued or buffered message when a receive is executed.;buffered_message;0.6568627450980393;0.03125;0.7799999999999999;1.4681127450980394;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing;Message Passing In Java When a thread sends a message to another thread.;thread_sends;0.5710784313725491;0.0625;0.7799999999999999;1.413578431372549;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Messages can be passed synchronously, meaning the sender blocks until the received does a receive and the receiver blocks until the sender does a send.;sender_blocks;0.5710784313725491;0.03125;0.7799999999999999;1.382328431372549;0.0;0;Tue Jun 27 23:04:11 IST 2017
examples of message passing;http://stackoverflow.com/questions/3222375/what-is-message-passing; Since the sender and receiver are at specific known points in their code at a known specific instant of time, synchronous message passing is also called a simple rendezvous with a one_way flow of information from the sender to the receiver.;specific_instant;0.5710784313725491;0.03125;0.7799999999999999;1.382328431372549;0.7071067811865475;0;Tue Jun 27 23:04:11 IST 2017
