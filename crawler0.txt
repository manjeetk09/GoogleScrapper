  .
Specially, for CNE algorithm, we define a class CNE, where details are inside mlpack src mlpack methods ne cne.
hpp .
 For each algorithm, including CNE, NEAT and HyperNEAT, there are same key functions , so that different algorithms can be called in similar style.
 Here we list some key functions .
  .
There are several options available for the algorithm used for each Lloyd iteration, specified with the algorithm  option.
 The standard O approach can be used .
 Other options include the PellegMoore treebased algorithm , Elkan s triangleinequality based algorithm , and Hamerly s modification to Elkan s algorithm .
algorithm  .
These are a list of ideas compiled by mlpack developers  they range from simpler code maintenance tasks to difficult machine learning algorithm implementation, which means that there are suitable ideas for a wide range of student abilities and interests.
 The  necessary knowledge  sections can often be replaced with  willing to learn  for the easier projects, and for some of the more difficult problems, a full understanding of the description statement and some coding knowledge is sufficient.
.
.
.
.
.
.
This last goal is somewhat in contrast to the scikitlearn project, which generally only implements stable, wellknown algorithms.
 mlpack can fill a niche by providing highquality implementations of algorithms that just appeared in conferences or journals.
 In those cases where mlpack is implementing wellknown algorithms  i.
e.
 SVMs or other standard techniques , we should strive to ensure that our implementation is faster than other implementations.
 To ensure that, we may use the automatic benchmarking system  see https   www.
github.
com zoq benchmarks .
.
.
Currently mlpack supports the following algorithms .
Collaborative Filtering.
Density Estimation Trees.
Euclidean Minimum Spanning Trees.
Fast Exact Max-Kernel Search (FastMKS).
Gaussian Mixture Models (GMMs).
Hidden Markov Models (HMMs).
Kernel Principal Component Analysis (KPCA).
K-Means Clustering.
Least-Angle Regression (LARS/LASSO).
Local Coordinate Coding.
Locality-Sensitive Hashing (LSH).
Logistic regression.
Naive Bayes Classifier.
Neighbourhood Components Analysis (NCA).
Non-negative Matrix Factorization (NMF).
Principal Components Analysis (PCA).
Independent component analysis (ICA).
Rank-Approximate Nearest Neighbor (RANN).
Simple Least-Squares Linear Regression (and Ridge Regression).
Sparse Coding.
Tree-based Neighbor Search (all-k-nearest-neighbors, all-k-furthest-neighbors), using either kd-trees or cover trees.
Tree-based Range Search.
.
.
