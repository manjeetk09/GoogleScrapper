1;a_function;be_inferring_from;labeled_training_data;0.711
1;Supervised_learning;is;the_machine;0.704
2;The_training_data;consist_of;a_set_of_training_examples;0.892
3;a_vector_and_a_desired_output_value;also_called;the_supervisory_signal;0.806
3;a_pair;consisting_of;an_input_object_typically_a_vector_and_a_desired_output_value_also_called_the_supervisory_signal;0.792
3;each_example;is;a_pair_consisting_of_an_input_object;0.752
3;each_example;is_a_pair_in;supervised_learning;0.746
4;A_supervised_learning_algorithm_analyzes;produces;an_inferred_function_which_can_be_used_for_mapping_new_examples;0.812
5;An_optimal_scenario;will_allow_for;the_algorithm;0.861
5;An_optimal_scenario;to_correctly_determine;the_class_labels;0.788
5;An_optimal_scenario;will_allow_to_correctly_determine;the_class_labels;0.693
6;the_learning_algorithm;to_generalize_from;the_training_data;0.854
6;the_learning_algorithm;to_generalize_to;unseen_situations;0.82
6;the_learning_algorithm;to_generalize_in;a_reasonable_way;0.756
7;The_parallel_task;is_often_referred_as;concept_learning;0.898
11;available;be_A_wide_range_of;supervised_learning_algorithms;0.492
12;no_single_learning_algorithm_that_works_best_on_all_supervised_learning_problems;see;the_No_free_lunch_theorem;0.756
12;no_single_learning_algorithm;works_on;all_supervised_learning_problems;0.736
13;four_major_issues;to_consider_in;supervised_learning;0.834
14;A_first_issue;is_the_tradeoff_between;bias_and_variance;0.916
14;A_first_issue;is;the_tradeoff;0.746
15;we;have;available_several_different_but_equally_good_training_data_sets;0.603
16;A_learning_algorithm;is_biased_for;a_particular_input_x_displaystyle_x;0.929
16;it;is_systematically;incorrect;0.597
17;A_learning_algorithm;has_high_variance_for;a_particular_input_x_displaystyle_x;0.872
17;A_learning_algorithm;has;high_variance;0.768
17;it;predicts;different_output_values;0.083
18;The_prediction_error_of_a_learned_classifier;is_related_to;the_sum_of_the_bias;0.926
20;A_learning_algorithm;must_be;flexible;0.736
20;flexible;be_A_learning_algorithm_with;low_bias;0.476
20;it;can_fit_well;the_data;0.36
21;it;will_fit;each_training_data_set_differently_and_hence_have_high_variance;0.709
21;the_learning_algorithm;is_too;flexible;0.115
22;they;to_adjust;this_tradeoff;0.695
23;The_second_issue;is;the_amount_training_data_available_relative_to_the_complexity_of_the_true_function_classifier_or_regression_function;0.802
24;an_inflexible_learning_algorithm;will_be;able;0.778
24;an_inflexible_learning_algorithm;to_learn;it;0.712
24;it;to_be_learn_from;a_small_amount_of_data;0.677
24;able;be_an_inflexible_learning_algorithm_with;high_bias_and_low_variance;0.272
24;the_true_function;is_then;simple;0.065
25;the_true_function;is;complex_e;0.115
27;the_function;will_only_be_learnable_from;a_very_large_amount_of_training_data;0.837
27;the_function;will_only_be;learnable;0.778
27;it;involves_complex_interactions_among;many_different_input_features;0.63
27;it;involves;complex_interactions;0.605
27;it;behaves_differently_then_in;different_parts_of_the_input_space;0.56
27;the_function;will_using_a_flexible_learning_algorithm_with;low_bias_and_high_variance;0.425
27;a_flexible_learning_algorithm;will_be_using_with;low_bias_and_high_variance;0.419
27;the_function;will_using;a_flexible_learning_algorithm;0.364
27;different;be_parts_of;the_input_space;0.256
28;A_third_issue;is_the_dimensionality_of;the_input_space;0.93
28;A_third_issue;is;the_dimensionality_of_the_input_space;0.778
29;difficult;even_only_depends_on;a_small_number_of_those_features;0.756
29;the_learning_problem;can_be;difficult;0.705
29;the_true_function;even_only_depends_on;a_small_number_of_those_features;0.144
29;the_input_feature_vectors;have;high_dimension;0.091
30;the_many_extra_dimensions;can_confuse;the_learning_algorithm;0.627
30;it;to_have;high_variance;0.612
31;high_input_dimensionality;Hence_typically_requires_tuning;the_classifier;0.647
32;irrelevant_features;can_be_manually_remove_from;the_input_data;0.658
32;the_engineer;can_manually_remove_irrelevant_features_from;the_input_data;0.178
32;the_engineer;can_manually_remove;irrelevant_features;0.177
35;A_fourth_issue;is_the_degree_of;noise;0.939
35;A_fourth_issue;is;the_degree_of_noise;0.804
36;the_learning_algorithm;should_not_attempt_to_find;a_function_that_exactly_matches_the_training_examples;0.845
36;the_learning_algorithm;to_find;a_function_that_exactly_matches_the_training_examples;0.828
36;the_training_examples;be_exactly_matches_by;a_function;0.655
36;the_desired_output_values;are_often_incorrect_because_of;human_error_or_sensor_errors_then_the_learning_algorithm_should_not_attempt_to_find_a_function;0.367
38;the_function_you_are_trying_to_learn;is_too_complex_for;your_learning_model;0.168
38;the_function_you_are_trying_to_learn;is_too;complex;0.166
39;this_phenomenon;has_been_called;deterministic_noise;0.759
40;either_type_of_noise;is;present;0.653
40;present;be_either_type_of;noise;0.399
41;several_approaches_to_alleviate_noise_in_the_output_values_such_as_early_stopping_to_prevent_overfitting_as_well_as_detecting_and_removing_the_noisy_training_examples_prior_to_training_the_supervised_learning_algorithm;there_are_in;practice;0.569
44;which_one;works_on;the_problem;0.779
44;the_engineer;can_compare;multiple_learning_algorithms;0.729
44;which_one;works_at;hand;0.701
44;which_one;works_see;cross_validation;0.561
46;it;is_to_spend;extra_time;0.569
47;naive_Bayes_linear_discriminant_analysis_decision_trees;k;nearest_neighbor_algorithm_and_Neural_Networks_Multilayer_perceptron;0.71
54;such_that_x_i_displaystyle_x_i_is_the_feature_vector_of_the;is;its_label_i;0.631
56;a_learning_algorithm;seeks;a_function_g_X_Y_displaystyle;0.729
56;X_displaystyle_X;is;the_input_space_and_Y_displaystyle;0.639
56;a_function_g_X_Y_displaystyle;g_X_to;Y_where_X_displaystyle_X_is_the_input_space_and_Y_displaystyle_Y_is_the_output_space;0.639
56;X;be_g_to;Y_where_X_displaystyle_X_is_the_input_space_and_Y_displaystyle_Y_is_the_output_space;0.474
56;a_learning_algorithm;seeks_a_function_g_X_Y_displaystyle_g_X_to_Y_where_X_displaystyle_X_is_the_input_space_and_Y_displaystyle_Y_is_the_output_space_in;class;0.228
57;The_function_g_displaystyle_g;is_an_element_of;some_space_of_possible_functions;0.936
57;G_displaystyle_G;usually_called;the_hypothesis_space;0.796
57;The_function_g_displaystyle_g;is;an_element_of_some_space_of_possible_functions;0.736
58;It;is_sometimes;convenient;0.698
58;the_highest_score_g_x_arg_max_y_f_x_y_displaystyle_g_x_arg_max_y_f_x_y;be_gives_by;the_y_displaystyle_y_value;0.564
59;F;displaystyle;F;0.656
60;G_displaystyle_G_and_F_displaystyle_F;can_be_any_space_of;functions;0.854
60;g_displaystyle_g;takes;the_form_of_a_conditional_probability_model;0.725
60;G_displaystyle_G_and_F_displaystyle_F;can_be;any_space_of_functions;0.661
60;the_form_of_a_conditional_probability_model;be_takes_by;probabilistic_models;0.491
61;joint_probability_models;is;a_conditional_probability_model;0.704
61;a_conditional_probability_model;be_joint_probability_models_whereas;logistic_regression;0.272
63;Empirical_risk_minimization;seeks;the_function;0.807
63;best;fits;the_training_data;0.427
64;the_bias_variance_tradeoff;be_controls_by;a_penalty_function;0.564
65;the_training_set;consists_of;a_sample_of_independent_and_identically_distributed_pairs_x;0.888
65;it;is_assumed_in;both_cases;0.655
66;a_function;fits;the_training_data_a_loss_function_L_Y_Y_R_0_displaystyle_L_Y_times_Y_to_mathbb_R_geq_0_is_defined;0.752
66;a_loss_function_L_Y_Y_R_0_displaystyle_L_Y_times;is_defined_in;the_training_data;0.366
67;displaystyle_hat_y;is_L_y_i;y_displaystyle_L_y_i_hat_y;0.876
67;displaystyle_hat_y;is;L_y;0.718
68;g_displaystyle_R_g_of_function_g_displaystyle_g;is_defined_as;the_expected_loss_g_displaystyle_g;0.92
68;g_displaystyle_R_g_of_function_g_displaystyle_g;is_defined_in;The_risk_R;0.64
70;the_supervised_learning_algorithm;seeks;the_function_g_displaystyle_g_that_minimizes_R_g_displaystyle_R_g;0.796
70;the_supervised_learning_algorithm;seeks_the_function_g_displaystyle_g_that_minimizes_R_g_displaystyle_R_g_in;empirical_risk_minimization;0.242
72;empirical_risk_minimization;is_equivalent_to;maximum_likelihood_estimation;0.823
72;empirical_risk_minimization;is;equivalent;0.735
72;a_conditional_probability_distribution_P_y_x_displaystyle_P_y_x_and_the_loss_function;is;the_negative_log_likelihood_L_y_y;0.653
73;large_empirical_risk_minimization;is_not_sufficiently_large_empirical_risk_minimization_leads_to;high_variance_and_poor_generalization;0.792
73;the_training_set;is_not_sufficiently_large_empirical_risk_minimization_leads_to;high_variance_and_poor_generalization;0.792
73;G_displaystyle_G;contains;many_candidate_functions;0.716
74;The_learning_algorithm;to_memorize;the_training_examples;0.788
74;The_learning_algorithm;is;able;0.746
76;Structural_risk_minimization;seeks_to_prevent;overfitting;0.807
76;a_regularization_penalty;be_incorporating_into;the_optimization;0.674
77;razor;prefers_simpler_functions_over;complex_ones;0.772
77;simpler_functions;be_prefers_by;razor;0.476
78;A_wide_variety_of_penalties;have_been_employed_that;correspond;0.852
79;the_function_g_displaystyle_g;is_a_linear_function_of;the_form;0.926
79;the_function_g_displaystyle_g;is;a_linear_function_of_the_form;0.768
80;2_displaystyle_sum_j_beta_j_2;is_the_squared_Euclidean_norm_of;the_weights_also_known_as_the_L_2_displaystyle_L_2_norm;0.9
80;the_weights;be_also_known_as;the_L_2_displaystyle;0.655
81;Other_norms;include;the_L_1_displaystyle;0.723
82;The_penalty;will_be_denoted_by;C_g_displaystyle_C_g;0.925
83;The_supervised_learning_optimization_problem;is_to_find;the_function;0.74
84;The_parameter_displaystyle_lambda;controls;the_bias_variance_tradeoff;0.782
85;0_displaystyle_lambda_0;gives_empirical_risk_minimization_with;low_bias_and_high_variance;0.819
85;empirical_risk_minimization;be_gives_by;0_displaystyle_lambda_0;0.564
86;the_learning_algorithm;will_have;high_bias_and_low_variance;0.773
86;displaystyle_lambda;is;large;0.653
87;The_value_of_displaystyle_lambda;can_be_chosen_empirically_via;cross_validation;0.919
88;The_complexity_penalty;has_a_Bayesian_interpretation_as;the_negative_log_prior_probability;0.872
88;The_complexity_penalty;has_a_Bayesian_interpretation;the_posterior_probabability_g_displaystyle_g;0.677
88;The_complexity_penalty;has_is_the_posterior_probabability_of_g_displaystyle_g_as;the_negative_log_prior_probability;0.522
88;The_complexity_penalty;has_is_the_posterior_probabability_of_g_displaystyle_g;a_Bayesian_interpretation;0.401
89;they;seek_to_find;a_function_g_displaystyle_g;0.712
89;the_different_output_values;see;discriminative_model;0.659
90;f_displaystyle_f;can_be_regarded_as;a_generative_model_that_explains_how_the_data_were_generated;0.878
90;f_x_y_P_x_y_displaystyle_f_x_y_P_x_y;is;a_joint_probability_distribution_and_the_loss_function;0.585
90;the_negative_log_likelihood;log;P_x;0.497
91;Generative_training_algorithms;be_computationally_efficient_than;discriminative_training_algorithms;0.597
92;the_solution;can_be_computed_in;closed_form;0.925
92;the_solution;can_be_computed_in;some_cases;0.797
93;the_standard_supervised_learning_problem;can_be_generalized_in;several_ways;0.655
94;a_function;be_inferring_from;labeled_training_data;0.711
94;Supervised_learning;is;the_machine;0.704
95;The_training_data;consist_of;a_set_of_training_examples;0.892
96;a_vector_and_a_desired_output_value;also_called;the_supervisory_signal;0.806
96;a_pair;consisting_of;an_input_object_typically_a_vector_and_a_desired_output_value_also_called_the_supervisory_signal;0.792
96;each_example;is;a_pair_consisting_of_an_input_object;0.752
96;each_example;is_a_pair_in;supervised_learning;0.746
97;A_supervised_learning_algorithm_analyzes;produces;an_inferred_function_which_can_be_used_for_mapping_new_examples;0.812
98;An_optimal_scenario;will_allow_for;the_algorithm;0.861
98;An_optimal_scenario;to_correctly_determine;the_class_labels;0.788
98;An_optimal_scenario;will_allow_to_correctly_determine;the_class_labels;0.693
99;the_learning_algorithm;to_generalize_from;the_training_data;0.854
99;the_learning_algorithm;to_generalize_to;unseen_situations;0.82
99;the_learning_algorithm;to_generalize_in;a_reasonable_way;0.756
101;Supervised_learning;is_a_type_of;machine_learning_algorithm_that_uses_a_known_dataset;0.935
101;a_type_of_machine_learning_algorithm;called;the_training_dataset_to_make_predictions;0.894
101;Supervised_learning;is;a_type_of_machine_learning_algorithm;0.768
101;a_known_dataset;be_uses_by;machine_learning_algorithm;0.62
102;The_training_dataset;includes;input_data_and_response_values;0.782
103;the_supervised_learning_algorithm;seeks_to_build;a_model_that_can_make_predictions_of_the_response_values_for_a_new_dataset;0.803
103;predictions_of_the_response_values;can_be_make_for;a_new_dataset;0.708
104;A_test_dataset;to_validate;the_model;0.761
105;larger_training_datasets;often_yield;models;0.762
