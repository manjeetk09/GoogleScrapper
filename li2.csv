Initialising the Message Queue
IPC Functions, Key Arguments, and Creation Flags   sys ipc h 
Controlling message queues
Sending and Receiving Messages
POSIX Messages   mqueue h 
Example  Sending messages between two processes message_send c __ creating and sending to a simple message queue message_rec c __ receiving the above message
Some further example message queue programs msgget c  Simple Program to illustrate msgetmsgctl cSample Program to Illustrate msgctlmsgop c  Sample Program to Illustrate msgsndand msgrcv
Exercises
msgget c  Simple Program to illustrate msget
msgctl cSample Program to Illustrate msgctl
msgop c  Sample Program to Illustrate msgsndand msgrcv




MVAPICH MPI is developed and supported by the Network_Based Computing Lab at Ohio State University 
Available on all of LC s Linux clusters 
MVAPICH 1 2 Default version of MPI MPI_1 implementation that also includes support for MPI_I O Based on MPICH_1 2 7 MPI library from Argonne National Laboratory Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program  See  usr local docs mpi mvapich basics for LC usage details 
MVAPICH2 Multiple versions available MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation  TOSS 3  Default MPI implementation TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     Thread_safe
Default version of MPI
MPI_1 implementation that also includes support for MPI_I O
Based on MPICH_1 2 7 MPI library from Argonne National Laboratory
Not thread_safe  All MPI calls should be made by the master thread in a multi_threaded MPI program 
See  usr local docs mpi mvapich basics for LC usage details 
Multiple versions available
MPI_2 and MPI_3 implementations based on MPICH MPI library from Argonne National Laboratory  Versions 1 9 and later implement MPI_3 according to the developer s documentation 
TOSS 3  Default MPI implementation
TOSS 2  Not the default _ requires the  use  command to load the selected dotkit package  For example  use _l mvapich               use mvapich2_intel_2 1     
Thread_safe
Open MPI is a thread_safe, open source MPI implementation developed and supported by a consortium of academic, research, and industry partners 
Available on all LC Linux clusters  However, you ll need to load the desired dotkit or module first  For example  dotkit   use _l openmpi                 use openmpi_gnu_1 8 4        module   module avail                   module load openmpi 2 0 0    This ensures that LC s MPI wrapper scripts point to the desired version of Open MPI 

Available on LC s Linux clusters 
Based on MPICH3  Supports MPI_3 functionality 
Thread_safe
Compiling and running Intel MPI programs  see the LC documentation at  https   lc llnl gov confluence pages viewpage action pageId 137725526

The IBM BG Q MPI library is the only supported implementation on these clusters 
Default version is based on MPICH2, which includes MPI_2 functionality minus dynamic processes 
A version supporting MPI_3 functionality is available 
Thread_safe
Compiling and running IBM BG Q MPI programs  see the BG Q Tutorial  computing llnl gov tutorials bgq 

The IBM Spectrum MPI library is the only supported implementation on these clusters 
Based on Open MPI  Includes MPI_3 functionality 
Thread_safe
NVIDIA GPU support
Compiling and running IBM Spectrum MPI programs  see the CORAL EA MPI documentation at  https   lc llnl gov confluence display CORALEA MPI
LC developed MPI compiler wrapper scripts are used to compile MPI programs
Automatically perform some error checks, include the appropriate MPI  include files, link to the necessary MPI libraries, and pass options to the underlying compiler 

For additional information  See the man page Issue the script name with the _help option View the script yourself directly MPI Build Scripts Implementation Language Script Name Underlying Compiler MVAPCH 1 2 C mpicc gcc _ GNU mpigcc gcc _ GNU mpiicc icc _ Intel mpipgcc pgcc _ PGI C   mpiCC g   _ GNU mpig   g   _ GNU mpiicpc icpc _ Intel mpipgCC pgCC _ PGI Fortran mpif77 g77 _ GNU mpigfortran gfortran _ GNU mpiifort ifort _ Intel mpipgf77 pgf77 _ PGI mpipgf90 pgf90 _ PGI MVAPCH2 C mpicc C compiler of dotkit package loaded C   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded Open MPI C mpicc C compiler of dotkit package loaded C   mpiCC mpic   mpicxx C   compiler of dotkit package loaded Fortran mpif77 Fortran77 compiler of dotkit package loaded mpif90 Fortran90 compiler of dotkit package loaded
See the man page 
Issue the script name with the _help option
View the script yourself directly
MPI libraries vary in their level of thread support  MPI_THREAD_SINGLE _ Level 0  Only one thread will execute  MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread  MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized  MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions 
Consult the MPI_Init_threadman page for details 
A simple C language example for determining thread level support is shown below     include  mpi h    include  stdio h      int main         int provided, claimed           Select one of the following      MPI_Init_thread       MPI_Init_thread       MPI_Init_thread       MPI_Init_thread                MPI_Init_thread       MPI_Query_thread           printf          MPI_Finalize         Sample output    Query thread level  3  Init_thread level  3

MPI_THREAD_SINGLE _ Level 0  Only one thread will execute 
MPI_THREAD_FUNNELED _ Level 1  The process may be multi_threaded, but only the main thread will make MPI calls _ all MPI calls are funneled to the main thread 
MPI_THREAD_SERIALIZED _ Level 2  The process may be multi_threaded, and multiple threads may make MPI calls, but only one at a time  That is, calls are not made concurrently from two distinct threads as all MPI calls are serialized 
MPI_THREAD_MULTIPLE _ Level 3  Multiple threads may call MPI with no restrictions 

Required for all programs that make MPI library calls       C include file           Fortran include file       include  mpi h  include  mpif h 
With MPI_3 Fortran, the USE mpi_f08 module is preferred over using the include file shown above 

     C Language _ Environment Management Routines  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28           required MPI include file        include  mpi h       include  stdio h        int main      int  numtasks, rank, len, rc       char hostname MPI_MAX_PROCESSOR_NAME            initialize MPI       MPI_Init           get number of tasks      MPI_Comm_size           get my rank       MPI_Comm_rank           this one is obvious       MPI_Get_processor_name      printf                   do some work with message passing             done with MPI       MPI_Finalize       



     Fortran _ Environment Management Routines  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29      program simple         required MPI include file     include  mpif h        integer numtasks, rank, len, ierr       characterhostname         initialize MPI     call MPI_INIT         get number of tasks     call MPI_COMM_SIZE         get my rank     call MPI_COMM_RANK         this one is obvious     call MPI_GET_PROCESSOR_NAME     print  ,  Number of tasks  ,numtasks,  My rank  ,rank,  Running on  ,hostname                do some work with message passing            done with MPI     call MPI_FINALIZE       end
Login to an LC cluster using your workshop username and OTP token
Copy the exercise files to your home directory
Familiarize yourself with LC s MPI compilers
Write a simple  Hello World  MPI program using several MPI Environment Management routines
Successfully compile your program
Successfully run your program _ several different ways

The value of PI can be calculated in various ways  Consider the Monte Carlo method of approximating PI  Inscribe a circle with radius r in a square with side length of 2r The area of the circle is  r2 and the area of the square is 4r2 The ratio of the area of the circle to the area of the square is   r2   4r2       4 If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle    is then approximated as  N       4   M     4   M   N     4   M   N Note that increasing the number of points generated improves the approximation 
Serial pseudo code for this procedure  npoints   10000  circle_count   0    do j   1,npoints    generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do    PI   4 0 circle_count npoints
Leads to an  embarassingly parallel  solution  Break the loop iterations into chunks that can be executed by different tasks simultaneously  Each task executes its portion of the loop a number of times  Each task can do its work without requiring any information from the other tasks   Master task recieves results from other tasks using send receive point_to_point operations 
Pseudo code solution  red highlights changes for parallelism  npoints   10000  circle_count   0   p   number of tasks num   npoints p find out if I am MASTER or WORKER     do j   1,num     generate 2 random numbers between 0 and 1    xcoordinate   random1    ycoordinate   random2    if inside circle    then circle_count   circle_count   1  end do   if I am MASTER receive from WORKERS their circle_counts compute PI else if I am WORKER send to MASTER circle_count endif Example MPI Program in C    mpi_pi_reduce c Example MPI Program in Fortran    mpi_pi_reduce f
Key Concept  Divide work between available tasks which communicate data via point_to_point message passing calls 
Inscribe a circle with radius r in a square with side length of 2r
The area of the circle is  r2 and the area of the square is 4r2
The ratio of the area of the circle to the area of the square is   r2   4r2       4
If you randomly generate N points inside the square, approximately N       4 of those points should fall inside the circle 
  is then approximated as  N       4   M     4   M   N     4   M   N
Note that increasing the number of points generated improves the approximation 
Break the loop iterations into chunks that can be executed by different tasks simultaneously 
Each task executes its portion of the loop a number of times 
Each task can do its work without requiring any information from the other tasks  
Master task recieves results from other tasks using send receive point_to_point operations 
MPI point_to_point operations typically involve message passing between two, and only two, different MPI tasks  One task is performing a send operation and the other task is performing a matching receive operation 
There are different types of send and receive routines used for different purposes  For example  Synchronous send Blocking send   blocking receive Non_blocking send   non_blocking receive Buffered send Combined send receive  Ready  send
Any type of send routine can be paired with any type of receive routine 
MPI also provides several routines associated with send _ receive operations, such as those used to wait for a message s arrival or probe to find out if a message has arrived 
Synchronous send
Blocking send   blocking receive
Non_blocking send   non_blocking receive
Buffered send
Combined send receive
 Ready  send
In a perfect world, every send operation would be perfectly synchronized with its matching receive  This is rarely the case  Somehow or other, the MPI implementation must be able to deal with storing data when the two tasks are out of sync 
Consider the following two cases  A send operation occurs 5 seconds before the receive is ready _ where is the message while the receive is pending  Multiple sends arrive at the same receiving task which can only accept one send at a time _ what happens to the messages that are  backing up  
The MPI implementation decides what happens to data in these types of cases  Typically, a system buffer area is reserved to hold data in transit  For example 
System buffer space is  Opaque to the programmer and managed entirely by the MPI library A finite resource that can be easy to exhaust Often mysterious and not well documented Able to exist on the sending side, the receiving side, or both Something that may improve program performance because it allows send _ receive operations to be asynchronous 
User managed address space is called the application buffer  MPI also provides for a user managed send buffer 
A send operation occurs 5 seconds before the receive is ready _ where is the message while the receive is pending 
Multiple sends arrive at the same receiving task which can only accept one send at a time _ what happens to the messages that are  backing up  
Opaque to the programmer and managed entirely by the MPI library
A finite resource that can be easy to exhaust
Often mysterious and not well documented
Able to exist on the sending side, the receiving side, or both
Something that may improve program performance because it allows send _ receive operations to be asynchronous 
Most of the MPI point_to_point routines can be used in either blocking or non_blocking mode 
Blocking  A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer  A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send  A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive  A blocking receive only  returns  after the data has arrived and is ready for use by the program 
Non_blocking  Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message  Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen  It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this  Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why 
A blocking send routine will only  return  after it is safe to modify the application buffer for reuse  Safe means that modifications will not affect the data intended for the receive task  Safe does not imply that the data was actually received _ it may very well be sitting in a system buffer 
A blocking send can be synchronous which means there is handshaking occurring with the receive task to confirm a safe send 
A blocking send can be asynchronous if a system buffer is used to hold the data for eventual delivery to the receive 
A blocking receive only  returns  after the data has arrived and is ready for use by the program 
Non_blocking send and receive routines behave similarly _ they will return almost immediately  They do not wait for any communication events to complete, such as message copying from user memory to system buffer space or the actual arrival of message 
Non_blocking operations simply  request  the MPI library to perform the operation when it is able  The user can not predict when that will happen 
It is unsafe to modify the application buffer until you know for a fact the requested non_blocking operation was actually performed by the library  There are  wait  routines used to do this 
Non_blocking communications are primarily used to overlap computation with communication and exploit possible performance gains  Blocking Send Non_blocking Send   myvar   0     for       task   i      MPI_Send       myvar   myvar   2          do some work                  myvar   0     for       task   i      MPI_Isend       myvar   myvar   2           do some work          MPI_Wait             Safe  Why  Unsafe  Why 
Task 0 pings task 1 and awaits return ping

     C Language _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35       include  mpi h       include  stdio h        main       int numtasks, rank, dest, source, rc, count, tag 1        char inmsg, outmsg  x       MPI_Status Stat       required variable for receive routines       MPI_Init      MPI_Comm_size      MPI_Comm_rank           task 0 sends to task 1 and waits to receive a return message     if         dest   1        source   1        MPI_Send        MPI_Recv                    task 1 waits for task 0 message then returns a message     else if         dest   0        source   0        MPI_Recv        MPI_Send                   query recieve Stat variable and print message details     MPI_Get_count      printffrom task  d with tag  d  n ,            rank, count, Stat MPI_SOURCE, Stat MPI_TAG         MPI_Finalize       



     Fortran _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program ping     include  mpif h        integer numtasks, rank, dest, source, count, tag, ierr     integer stat    required variable for receive routines     character inmsg, outmsg     outmsg    x      tag   1       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         task 0 sends to task 1 and waits to receive a return message     if then        dest   1        source   1        call MPI_SEND        call MPI_RECV         task 1 waits for task 0 message then returns a message     else if then        dest   0        source   0        call MPI_RECV        call MPI_SEND     endif         query recieve Stat variable and print message details     call MPI_GET_COUNT     print  ,  Task  ,rank,   Received , count,  charfrom task ,                stat,  with tag ,stat       call MPI_FINALIZE       end
Nearest neighbor exchange in a ring topology


     C Language _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34       include  mpi h       include  stdio h        main       int numtasks, rank, next, prev, buf 2 , tag1 1, tag2 2      MPI_Request reqs 4        required variable for non_blocking calls     MPI_Status stats 4        required variable for Waitall routine       MPI_Init      MPI_Comm_size      MPI_Comm_rank              determine left and right neighbors     prev   rank_1      next   rank 1      if  prev   numtasks _ 1      if    next   0           post non_blocking receives and sends for neighbors     MPI_Irecv      MPI_Irecv        MPI_Isend      MPI_Isend                do some work while sends receives progress in background          wait for all non_blocking operations to complete     MPI_Waitall                continue _ do more work       MPI_Finalize       



     Fortran _ Non_blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40      program ringtopo     include  mpif h        integer numtasks, rank, next, prev, buf, tag1, tag2, ierr     integer reqs    required variable for non_blocking calls      integer stats    required variable for WAITALL routine      tag1   1     tag2   2       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         determine left and right neighbors      prev   rank _ 1     next   rank   1     if then        prev   numtasks _ 1     endif     if then        next   0     endif         post non_blocking receives and sends for neighbors      call MPI_IRECV, 1, MPI_INTEGER, prev, tag1, MPI_COMM_WORLD, reqs, ierr      call MPI_IRECV, 1, MPI_INTEGER, next, tag2, MPI_COMM_WORLD, reqs, ierr        call MPI_ISEND, ierr      call MPI_ISEND, ierr             do some work while sends receives progress in background         wait for all non_blocking operations to complete      call MPI_WAITALL             continue _ do more work       call MPI_FINALIZE       end
Login to the LC workshop cluster, if you are not already logged in
Using your  Hello World  MPI program from Exercise 1, add MPI blocking point_to_point routines to send and receive messages
Successfully compile your program
Successfully run your program _ several different ways
Try the same thing with nonblocking send receive routines
Perform a scatter operation on the rows of an array

     C Language _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33       include  mpi h       include  stdio h       define SIZE 4       main       int numtasks, rank, sendcount, recvcount, source      float sendbuf SIZE  SIZE             1 0, 2 0, 3 0, 4 0 ,        5 0, 6 0, 7 0, 8 0 ,        9 0, 10 0, 11 0, 12 0 ,        13 0, 14 0, 15 0, 16 0          float recvbuf SIZE         MPI_Init      MPI_Comm_rank      MPI_Comm_size        if            define source task and elements to send receive, then perform collective scatter       source   1        sendcount   SIZE        recvcount   SIZE        MPI_Scatter sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,                   MPI_FLOAT,source,MPI_COMM_WORLD           printf  rank   d  Results   f  f  f  f n ,rank,recvbuf 0 ,              recvbuf 1 ,recvbuf 2 ,recvbuf 3                else       printf        MPI_Finalize       



     Fortran _ Collective Communications Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program scatter     include  mpif h        integer SIZE     parameter     integer numtasks, rank, sendcount, recvcount, source, ierr     real 4 sendbuf, recvbuf         Fortran stores this array in column major order, so the        scatter will actually scatter columns, not rows      data sendbuf  1 0, 2 0, 3 0, 4 0,                     5 0, 6 0, 7 0, 8 0,                     9 0, 10 0, 11 0, 12 0,                     13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then          define source task and elements to send receive, then perform collective scatter        source   1        sendcount   SIZE        recvcount   SIZE        call MPI_SCATTER sendbuf, sendcount, MPI_REAL, recvbuf, recvcount, MPI_REAL,                           source, MPI_COMM_WORLD, ierr           print  ,  rank   ,rank,  Results   ,recvbuf        else        print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       end


rank  0  Results  1 000000 2 000000 3 000000 4 000000  rank  1  Results  5 000000 6 000000 7 000000 8 000000  rank  2  Results  9 000000 10 000000 11 000000 12 000000  rank  3  Results  13 000000 14 000000 15 000000 16 000000

As previously mentioned, MPI predefines its primitive data types  C Data Types Fortran Data Types   MPI_CHAR  MPI_WCHAR  MPI_SHORT  MPI_INT  MPI_LONG  MPI_LONG_LONG_INT   MPI_LONG_LONG      MPI_SIGNED_CHAR  MPI_UNSIGNED_CHAR  MPI_UNSIGNED_SHORT  MPI_UNSIGNED_LONG  MPI_UNSIGNED  MPI_FLOAT  MPI_DOUBLE  MPI_LONG_DOUBLE     MPI_C_COMPLEX  MPI_C_FLOAT_COMPLEX  MPI_C_DOUBLE_COMPLEX  MPI_C_LONG_DOUBLE_COMPLEX      MPI_C_BOOL  MPI_LOGICAL  MPI_C_LONG_DOUBLE_COMPLEX     MPI_INT8_T   MPI_INT16_T  MPI_INT32_T   MPI_INT64_T      MPI_UINT8_T   MPI_UINT16_T   MPI_UINT32_T   MPI_UINT64_T  MPI_BYTE  MPI_PACKED     MPI_CHARACTER  MPI_INTEGER  MPI_INTEGER1   MPI_INTEGER2  MPI_INTEGER4  MPI_REAL  MPI_REAL2   MPI_REAL4  MPI_REAL8  MPI_DOUBLE_PRECISION  MPI_COMPLEX  MPI_DOUBLE_COMPLEX  MPI_LOGICAL  MPI_BYTE  MPI_PACKED
MPI also provides facilities for you to define your own data structures based upon sequences of the MPI primitive data types  Such user defined structures are called derived data types 
Primitive data types are contiguous  Derived data types allow you to specify non_contiguous data in a convenient manner and to treat it as though it was contiguous 
MPI provides several methods for constructing derived data types  Contiguous Vector Indexed Struct
Contiguous
Vector
Indexed
Struct
Create a data type representing a row of an array and distribute a different row to all processes 

     C Language _ Contiguous Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43       include  mpi h       include  stdio h       define SIZE 4       main       int numtasks, rank, source 0, dest, tag 1, i      float a SIZE  SIZE           1 0, 2 0, 3 0, 4 0,        5 0, 6 0, 7 0, 8 0,        9 0, 10 0, 11 0, 12 0,        13 0, 14 0, 15 0, 16 0       float b SIZE         MPI_Status stat      MPI_Datatype rowtype       required variable       MPI_Init      MPI_Comm_rank      MPI_Comm_size           create contiguous derived data type     MPI_Type_contiguous      MPI_Type_commit        if             task 0 sends one element of rowtype to all tasks        if             for              MPI_Send                          all tasks receive rowtype data from task 0        MPI_Recv         printf  rank   d  b   3 1f  3 1f  3 1f  3 1f n ,               rank,b 0 ,b 1 ,b 2 ,b 3                 else        printf           free datatype when done using it     MPI_Type_free      MPI_Finalize       



     Fortran _ Contiguous Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46      program contiguous     include  mpif h        integer SIZE     parameter     integer numtasks, rank, source, dest, tag, i,  ierr     real 4 a, b     integer stat     integer columntype     required variable     tag   1         Fortran stores this array in column major order     data a   1 0, 2 0, 3 0, 4 0,                5 0, 6 0, 7 0, 8 0,                9 0, 10 0, 11 0, 12 0,                 13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         create contiguous derived data type     call MPI_TYPE_CONTIGUOUS     call MPI_TYPE_COMMIT         if then          task 0 sends one element of columntype to all tasks        if then           do i 0, numtasks_1           call MPI_SEND, 1, columntype, i, tag, MPI_COMM_WORLD,ierr            end do        endif            all tasks receive columntype data from task 0        source   0        call MPI_RECV        print  ,  rank   ,rank,  b   ,b     else        print  ,  Must specify ,SIZE,  processors   Terminating        endif         free datatype when done using it     call MPI_TYPE_FREE     call MPI_FINALIZE       end


rank  0  b  1 0 2 0 3 0 4 0  rank  1  b  5 0 6 0 7 0 8 0  rank  2  b  9 0 10 0 11 0 12 0  rank  3  b  13 0 14 0 15 0 16 0
Create a data type representing a column of an array and distribute different columns to all processes 

     C Language _ Vector Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44       include  mpi h       include  stdio h       define SIZE 4       main       int numtasks, rank, source 0, dest, tag 1, i      float a SIZE  SIZE            1 0, 2 0, 3 0, 4 0,          5 0, 6 0, 7 0, 8 0,         9 0, 10 0, 11 0, 12 0,       13 0, 14 0, 15 0, 16 0       float b SIZE          MPI_Status stat      MPI_Datatype columntype       required variable         MPI_Init      MPI_Comm_rank      MPI_Comm_size              create vector derived data type     MPI_Type_vector      MPI_Type_commit        if             task 0 sends one element of columntype to all tasks        if             for               MPI_Send                           all tasks receive columntype data from task 0        MPI_Recv         printf  rank   d  b   3 1f  3 1f  3 1f  3 1f n ,               rank,b 0 ,b 1 ,b 2 ,b 3                 else        printf           free datatype when done using it     MPI_Type_free      MPI_Finalize       



     Fortran _ Vector Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46      program vector     include  mpif h        integer SIZE     parameter     integer numtasks, rank, source, dest, tag, i,  ierr     real 4 a, b     integer stat     integer rowtype     required variable     tag   1         Fortran stores this array in column major order     data a   1 0, 2 0, 3 0, 4 0,                5 0, 6 0, 7 0, 8 0,                 9 0, 10 0, 11 0, 12 0,                13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         create vector derived data type     call MPI_TYPE_VECTOR     call MPI_TYPE_COMMIT         if then          task 0 sends one element of rowtype to all tasks        if then           do i 0, numtasks_1           call MPI_SEND, 1, rowtype, i, tag, MPI_COMM_WORLD, ierr            end do        endif            all tasks receive rowtype data from task 0        source   0        call MPI_RECV        print  ,  rank   ,rank,  b   ,b     else        print  ,  Must specify ,SIZE,  processors   Terminating        endif         free datatype when done using it     call MPI_TYPE_FREE     call MPI_FINALIZE       end


rank  0  b  1 0 5 0 9 0 13 0  rank  1  b  2 0 6 0 10 0 14 0  rank  2  b  3 0 7 0 11 0 15 0  rank  3  b  4 0 8 0 12 0 16 0
Create a datatype by extracting variable portions of an array and distribute to all tasks 

     C Language _ Indexed Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43       include  mpi h       include  stdio h       define NELEMENTS 6       main       int numtasks, rank, source 0, dest, tag 1, i      int blocklengths 2 , displacements 2       float a 16            1 0, 2 0, 3 0, 4 0, 5 0, 6 0, 7 0, 8 0,         9 0, 10 0, 11 0, 12 0, 13 0, 14 0, 15 0, 16 0       float b NELEMENTS          MPI_Status stat      MPI_Datatype indextype       required variable       MPI_Init      MPI_Comm_rank      MPI_Comm_size        blocklengths 0    4      blocklengths 1    2      displacements 0    5      displacements 1    12              create indexed derived data type     MPI_Type_indexed      MPI_Type_commit        if         for            task 0 sends one element of indextype to all tasks          MPI_Send                   all tasks receive indextype data from task 0     MPI_Recv      printf  rank   d  b   3 1f  3 1f  3 1f  3 1f  3 1f  3 1f n ,            rank,b 0 ,b 1 ,b 2 ,b 3 ,b 4 ,b 5                free datatype when done using it     MPI_Type_free      MPI_Finalize       



     Fortran _ Indexed Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47      program indexed     include  mpif h        integer NELEMENTS     parameter     integer numtasks, rank, source, dest, tag, i,  ierr     integer blocklengths, displacements     real 4 a, b     integer stat     integer indextype     required variable     tag   1       data a   1 0, 2 0, 3 0, 4 0, 5 0, 6 0, 7 0, 8 0,                9 0, 10 0, 11 0, 12 0, 13 0, 14 0, 15 0, 16 0         call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       blocklengths  4     blocklengths  2     displacements  5     displacements  12         create indexed derived data type     call MPI_TYPE_INDEXED 2, blocklengths, displacements, MPI_REAL,                             indextype, ierr      call MPI_TYPE_COMMIT         if then          task 0 sends one element of indextype to all tasks        do i 0, numtasks_1        call MPI_SEND        end do     endif         all tasks receive indextype data from task 0     source   0     call MPI_RECV b, NELEMENTS, MPI_REAL, source, tag, MPI_COMM_WORLD,                     stat, ierr      print  ,  rank   ,rank,  b   ,b         free datatype when done using it     call MPI_TYPE_FREE     call MPI_FINALIZE       end


rank  0  b  6 0 7 0 8 0 9 0 13 0 14 0  rank  1  b  6 0 7 0 8 0 9 0 13 0 14 0  rank  2  b  6 0 7 0 8 0 9 0 13 0 14 0  rank  3  b  6 0 7 0 8 0 9 0 13 0 14 0
Create a data type that represents a particle and distribute an array of such particles to all processes 

     C Language _ Struct Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66       include  mpi h       include  stdio h       define NELEM 25       main       int numtasks, rank, source 0, dest, tag 1, i        typedef struct         float x, y, z        float velocity        int  n, type                   Particle      Particle     p NELEM , particles NELEM       MPI_Datatype particletype, oldtypes 2        required variables     int          blockcounts 2            MPI_Aint type used to be consistent with syntax of        MPI_Type_extent routine     MPI_Aint    offsets 2 , extent        MPI_Status stat        MPI_Init      MPI_Comm_rank      MPI_Comm_size            setup description of the 4 MPI_FLOAT fields x, y, z, velocity     offsets 0    0      oldtypes 0    MPI_FLOAT      blockcounts 0    4           setup description of the 2 MPI_INT fields n, type        need to first figure offset by getting size of MPI_FLOAT     MPI_Type_extent      offsets 1    4   extent      oldtypes 1    MPI_INT      blockcounts 1    2           define structured type and commit it     MPI_Type_struct      MPI_Type_commit           task 0 initializes the particle array and then sends it to each task     if         for            particles i  x   i   1 0           particles i  y   i   _1 0           particles i  z   i   1 0            particles i  velocity   0 25           particles i  n   i           particles i  type   i   2                    for           MPI_Send                    all tasks receive particletype data     MPI_Recv        printf  rank   d    3 2f  3 2f  3 2f  3 2f  d  d n , rank,p 3  x,          p 3  y,p 3  z,p 3  velocity,p 3  n,p 3  type            free datatype when done using it     MPI_Type_free      MPI_Finalize       



     Fortan _ Struct Derived Data Type Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63      program struct     include  mpif h        integer NELEM     parameter     integer numtasks, rank, source, dest, tag, i,  ierr     integer stat       type Particle     sequence     real 4 x, y, z, velocity     integer n, type     end type Particle       type p, particles     integer particletype, oldtypes    required variables     integer blockcounts, offsets, extent     tag   1       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         setup description of the 4 MPI_REAL fields x, y, z, velocity     offsets  0     oldtypes  MPI_REAL     blockcounts  4         setup description of the 2 MPI_INTEGER fields n, type        need to first figure offset by getting size of MPI_REAL     call MPI_TYPE_EXTENT     offsets  4   extent     oldtypes  MPI_INTEGER     blockcounts  2         define structured type and commit it      call MPI_TYPE_STRUCT 2, blockcounts, offsets, oldtypes,                            particletype, ierr      call MPI_TYPE_COMMIT           task 0 initializes the particle array and then sends it to each task     if then        do i 0, NELEM_1        particles  Particle          end do          do i 0, numtasks_1        call MPI_SEND particles, NELEM, particletype, i, tag,                        MPI_COMM_WORLD, ierr         end do     endif         all tasks receive particletype data     source   0     call MPI_RECV p, NELEM, particletype, source, tag,                     MPI_COMM_WORLD, stat, ierr        print  ,  rank   ,rank,  p   ,p         free datatype when done using it     call MPI_TYPE_FREE     call MPI_FINALIZE     end


rank  0   3 00 _3 00 3 00 0 25 3 1  rank  2   3 00 _3 00 3 00 0 25 3 1  rank  1   3 00 _3 00 3 00 0 25 3 1  rank  3   3 00 _3 00 3 00 0 25 3 1
Create two different process groups for separate collective communications exchange  Requires creating new communicators also 

     C Language _ Group and Communicator Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43       include  mpi h       include  stdio h       define NPROCS 8       main       int        rank, new_rank, sendbuf, recvbuf, numtasks,                ranks1 4   0,1,2,3 , ranks2 4   4,5,6,7       MPI_Group  orig_group, new_group       required variables     MPI_Comm   new_comm       required variable       MPI_Init      MPI_Comm_rank      MPI_Comm_size        if         printf        MPI_Finalize        exit                sendbuf   rank           extract the original group handle     MPI_Comm_group            divide tasks into two distinct groups based upon rank     if         MPI_Group_incl              else         MPI_Group_incl                   create new new communicator and then perform collective communications     MPI_Comm_create      MPI_Allreduce           get rank in new group     MPI_Group_rank       printf        MPI_Finalize       



     Fortran _ Group and Communicator Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42      program group     include  mpif h        integer NPROCS     parameter     integer rank, new_rank, sendbuf, recvbuf, numtasks     integer ranks1, ranks2, ierr     integer orig_group, new_group, new_comm     required variables     data ranks1  0, 1, 2, 3 , ranks2  4, 5, 6, 7        call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE       if then       print  ,  Must specify NPROCS   ,NPROCS,  Terminating         call MPI_FINALIZE       stop     endif       sendbuf   rank         extract the original group handle     call MPI_COMM_GROUP         divide tasks into two distinct groups based upon rank     if then        call MPI_GROUP_INCL     else         call MPI_GROUP_INCL     endif         create new new communicator and then perform collective communications     call MPI_COMM_CREATE     call MPI_ALLREDUCE         get rank in new group     call MPI_GROUP_RANK     print  ,  rank   ,rank,  newrank   ,new_rank,  recvbuf   , recvbuf       call MPI_FINALIZE     end


rank  7 newrank  3 recvbuf  22  rank  0 newrank  0 recvbuf  6  rank  1 newrank  1 recvbuf  6  rank  2 newrank  2 recvbuf  6  rank  6 newrank  2 recvbuf  22  rank  3 newrank  3 recvbuf  6  rank  4 newrank  0 recvbuf  22  rank  5 newrank  1 recvbuf  22
Create a 4 x 4 Cartesian topology from 16 processors and have each process exchange its rank with four neighbors 

     C Language _ Cartesian Virtual Topology Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54       include  mpi h       include  stdio h       define SIZE 16      define UP    0      define DOWN  1      define LEFT  2      define RIGHT 3       main       int numtasks, rank, source, dest, outbuf, i, tag 1,         inbuf 4   MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL, ,         nbrs 4 , dims 2   4,4 ,         periods 2   0,0 , reorder 0, coords 2         MPI_Request reqs 8       MPI_Status stats 8       MPI_Comm cartcomm       required variable       MPI_Init      MPI_Comm_size        if             create cartesian virtual topology, get rank, coordinates, neighbor ranks        MPI_Cart_create         MPI_Comm_rank         MPI_Cart_coords         MPI_Cart_shift         MPI_Cart_shift           printf   d  d  d  d n ,               rank,coords 0 ,coords 1 ,nbrs UP ,nbrs DOWN ,nbrs LEFT ,               nbrs RIGHT             outbuf   rank              exchange data with 4 neighbors        for             dest   nbrs i             source   nbrs i             MPI_Isend  outbuf, 1, MPI_INT, dest, tag,                      MPI_COMM_WORLD,  reqs i              MPI_Irecv  inbuf i , 1, MPI_INT, source, tag,                      MPI_COMM_WORLD,  reqs i 4                         MPI_Waitall              printf   d  d  d  d n ,               rank,inbuf UP ,inbuf DOWN ,inbuf LEFT ,inbuf RIGHT           else        printf           MPI_Finalize       



     Fortran _ Cartesian Virtual Topology Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58      program cartesian     include  mpif h        integer SIZE, UP, DOWN, LEFT, RIGHT     parameter     parameter     parameter     parameter     parameter     integer numtasks, rank, source, dest, outbuf, i, tag, ierr,               inbuf, nbrs, dims, coords, periods, reorder     integer stats, reqs     integer cartcomm     required variable     data inbuf  MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL ,            dims  4,4 , tag  1 , periods  0,0 , reorder  0         call MPI_INIT     call MPI_COMM_SIZE         if then          create cartesian virtual topology, get rank, coordinates, neighbor ranks        call MPI_CART_CREATE MPI_COMM_WORLD, 2, dims, periods, reorder,                               cartcomm, ierr         call MPI_COMM_RANK        call MPI_CART_COORDS        call MPI_CART_SHIFT, nbrs, ierr         call MPI_CART_SHIFT, nbrs, ierr           writerank,coords,coords,nbrs,nbrs,                      nbrs,nbrs            exchange data with 4 neighbors        outbuf   rank        do i 1,4           dest   nbrs           source   nbrs           call MPI_ISEND outbuf, 1, MPI_INTEGER, dest, tag,                           MPI_COMM_WORLD, reqs, ierr            call MPI_IRECV, 1, MPI_INTEGER, source, tag,                           MPI_COMM_WORLD, reqs, ierr         enddo          call MPI_WAITALL          writerank,inbuf       else       print  ,  Must specify ,SIZE,  processors   Terminating        endif       call MPI_FINALIZE       20 format  rank   ,I3,  coords   ,I2,I2,                   neighbors   ,I3,I3,I3,I3       30 format  rank   ,I3,                   ,                   inbuf   ,I3,I3,I3,I3         end


rank    0 coords   0 0 neighbors   _1  4 _1  1  rank    0                  inbuf   _1  4 _1  1  rank    8 coords   2 0 neighbors    4 12 _1  9  rank    8                  inbuf    4 12 _1  9  rank    1 coords   0 1 neighbors   _1  5  0  2  rank    1                  inbuf   _1  5  0  2  rank   13 coords   3 1 neighbors    9 _1 12 14  rank   13                  inbuf    9 _1 12 14            rank    3 coords   0 3 neighbors   _1  7  2 _1  rank    3                  inbuf   _1  7  2 _1  rank   11 coords   2 3 neighbors    7 15 10 _1  rank   11                  inbuf    7 15 10 _1  rank   10 coords   2 2 neighbors    6 14  9 11  rank   10                  inbuf    6 14  9 11  rank    9 coords   2 1 neighbors    5 13  8 10  rank    9                  inbuf    5 13  8 10
Login to the LC workshop cluster, if you are not already logged in
Following the Exercise 3 instructions will take you through all sorts of MPI programs _ pick any all that are of interest 
The intention is review the codes and see what s happening _ not just compile and run 
Several codes provide serial examples for a comparison with the parallel MPI versions 
Check out the  bug  programs 
Author  Blaise Barney, Livermore Computing 
MPI Standard documents  http   www mpi_forum org docs 
 Using MPI , Gropp, Lusk and Skjellum  MIT Press, 1994 
MPI Tutorials  www mcs anl gov research projects mpi tutorial
Livermore Computing specific information  Linux Clusters Overview tutorial computing llnl gov tutorials linux_clusters Using the Sequoia Vulcan BG Q Systems tutorial computing llnl gov tutorials bgq
 A User s Guide to MPI , Peter S  Pacheco  Department of Mathematics, University of San Francisco 
Linux Clusters Overview tutorial computing llnl gov tutorials linux_clusters
Using the Sequoia Vulcan BG Q Systems tutorial computing llnl gov tutorials bgq
These man pages were derived from the MVAPICH 0 9 implementation of MPI and may differ from the man pages of other implementations 
Not all MPI routines are shown
    deprecated in MPI_2 0, replaced in MPI_3 0
The complete MPI_3 standard defines over 430 routines  Environment Management Routines MPI_Abort MPI_Errhandler_create  MPI_Errhandler_free MPI_Errhandler_get  MPI_Errhandler_set  MPI_Error_class MPI_Error_string MPI_Finalize MPI_Get_processor_name MPI_Get_version MPI_Init MPI_Initialized MPI_Wtick MPI_Wtime     Point_to_Point Communication Routines MPI_Bsend MPI_Bsend_init MPI_Buffer_attach MPI_Buffer_detach MPI_Cancel MPI_Get_count MPI_Get_elements MPI_Ibsend MPI_Iprobe MPI_Irecv MPI_Irsend MPI_Isend MPI_Issend MPI_Probe MPI_Recv MPI_Recv_init MPI_Request_free MPI_Rsend MPI_Rsend_init MPI_Send MPI_Send_init MPI_Sendrecv MPI_Sendrecv_replace MPI_Ssend MPI_Ssend_init MPI_Start MPI_Startall MPI_Test MPI_Test_cancelled MPI_Testall MPI_Testany MPI_Testsome MPI_Wait MPI_Waitall MPI_Waitany MPI_Waitsome Collective Communication Routines MPI_Allgather MPI_Allgatherv MPI_Allreduce MPI_Alltoall MPI_Alltoallv MPI_Barrier MPI_Bcast MPI_Gather MPI_Gatherv MPI_Op_create MPI_Op_free MPI_Reduce MPI_Reduce_scatter MPI_Scan MPI_Scatter MPI_Scatterv Process Group Routines MPI_Group_compare MPI_Group_difference MPI_Group_excl MPI_Group_free MPI_Group_incl MPI_Group_intersection MPI_Group_range_excl MPI_Group_range_incl MPI_Group_rank MPI_Group_size MPI_Group_translate_ranks MPI_Group_union Communicators Routines MPI_Comm_compare MPI_Comm_create MPI_Comm_dup MPI_Comm_free MPI_Comm_group MPI_Comm_rank MPI_Comm_remote_group MPI_Comm_remote_size MPI_Comm_size MPI_Comm_split MPI_Comm_test_inter MPI_Intercomm_create MPI_Intercomm_merge       Derived Types Routines MPI_Type_commit MPI_Type_contiguous MPI_Type_extent  MPI_Type_free MPI_Type_hindexed  MPI_Type_hvector  MPI_Type_indexed MPI_Type_lb MPI_Type_size MPI_Type_struct  MPI_Type_ub  MPI_Type_vector Virtual Topology Routines MPI_Cart_coords MPI_Cart_create MPI_Cart_get MPI_Cart_map MPI_Cart_rank MPI_Cart_shift MPI_Cart_sub MPI_Cartdim_get MPI_Dims_create MPI_Graph_create MPI_Graph_get MPI_Graph_map MPI_Graph_neighbors MPI_Graph_neighbors_count MPI_Graphdims_get MPI_Topo_test Miscellaneous Routines MPI_Address  MPI_Attr_delete  MPI_Attr_get  MPI_Attr_put  MPI_Keyval_create  MPI_Keyval_free  MPI_Pack MPI_Pack_size MPI_Pcontrol MPI_Unpack    
Task 0 pings task 1 and awaits return ping

     C Language _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35       include  mpi h       include  stdio h        main       int numtasks, rank, dest, source, rc, count, tag 1        char inmsg, outmsg  x       MPI_Status Stat       required variable for receive routines       MPI_Init      MPI_Comm_size      MPI_Comm_rank           task 0 sends to task 1 and waits to receive a return message     if         dest   1        source   1        MPI_Send        MPI_Recv                    task 1 waits for task 0 message then returns a message     else if         dest   0        source   0        MPI_Recv        MPI_Send                   query recieve Stat variable and print message details     MPI_Get_count      printffrom task  d with tag  d  n ,            rank, count, Stat MPI_SOURCE, Stat MPI_TAG         MPI_Finalize       



     Fortran _ Blocking Message Passing Example  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36      program ping     include  mpif h        integer numtasks, rank, dest, source, count, tag, ierr     integer stat    required variable for receive routines     character inmsg, outmsg     outmsg    x      tag   1       call MPI_INIT     call MPI_COMM_RANK     call MPI_COMM_SIZE         task 0 sends to task 1 and waits to receive a return message     if then        dest   1        source   1        call MPI_SEND        call MPI_RECV         task 1 waits for task 0 message then returns a message     else if then        dest   0        source   0        call MPI_RECV        call MPI_SEND     endif         query recieve Stat variable and print message details     call MPI_GET_COUNT     print  ,  Task  ,rank,   Received , count,  charfrom task ,                stat,  with tag ,stat       call MPI_FINALIZE       end
