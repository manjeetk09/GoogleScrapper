Google s self driving cars and robots get a lot of press  but the company s real future is in machine learning  the technology that enables computers to get smarter and more personal.
   Eric Schmidt  Google Chairman .
We are probably living in the most defining period of human history.
 The period when computing moved from large mainframes to PCs to cloud.
 But what makes it defining is not what has happened  but what is coming our way in years to come.
What makes this period exciting for some one like me is the democratization of the tools and techniques  which followed the boost in computing.
 Today  as a data scientist  I can build data crunching machines with complex algorithms for a few dollors per hour.
 But  reaching here wasn t easy  I had my dark days and nights.
.
The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world.
 Through this guide  I will enable you to work on machine learning problems and gain from experience.
 I am providing a high level understanding about various machine learning algorithms along with R   Python codes to run them.
 These should be sufficient to get your hands dirty.
.
I have deliberately skipped the statistics behind these techniques  as you don t need to understand them at the start.
 So  if you are looking for statistical understanding of these algorithms  you should look elsewhere.
 But  if you are looking to equip yourself to start building machine learning project  you are in for a treat.
 .
 .
How it works  This algorithm consist of a target   outcome variable  or dependent variable  which is to be predicted from a given set of predictors  independent variables .
 Using these set of variables  we generate a function that map inputs to desired outputs.
 The training process continues until the model achieves a desired level of accuracy on the training data.
 Examples of Supervised Learning  Regression  Decision Tree  Random Forest  KNN  Logistic Regression etc.
 .
How it works  In this algorithm  we do not have any target or outcome variable to predict   estimate.
  It is used for clustering population in different groups  which is widely used for segmenting customers in different groups for specific intervention.
 Examples of Unsupervised Learning  Apriori algorithm  K means.
 .
How it works   Using this algorithm  the machine is trained to make specific decisions.
 It works this way  the machine is exposed to an environment where it trains itself continually using trial and error.
 This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions.
 Example of Reinforcement Learning  Markov Decision Process.
Here is the list of commonly used machine learning algorithms.
 These algorithms can be applied to almost any data problem .
It is used to estimate real values  cost of houses  number of calls  total sales etc.
  based on continuous variable s .
 Here  we establish relationship between independent and dependent variables by fitting a best line.
 This best fit line is known as regression line and represented by a linear equation Y  a  X   b.
The best way to understand linear regression is to relive this experience of childhood.
 Let us say  you ask a child in fifth grade to arrange people in his class by increasing order of weight  without asking them their weights  What do you think the child will do  He   she would likely look  visually analyze  at the height and build of people and arrange them using a combination of these visible parameters.
 This is linear regression in real life  The child has actually figured out that height and build would be correlated to the weight by a relationship  which looks like the equation above.
In this equation .
These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.
Look at the below example.
 Here we have identified the best fit line having linear equation y 0.
2811x 13.
9.
 Now using this equation  we can find the weight  knowing the height of a person.
.
Linear Regression is of mainly two types  Simple Linear Regression and Multiple Linear Regression.
 Simple Linear Regression is characterized by one independent variable.
 And  Multiple Linear Regression as the name suggests  is characterized by multiple  more than 1  independent variables.
 While finding best fit line  you can fit a polynomial or curvilinear regression.
 And these are known as polynomial or curvilinear regression.
Python Code.
R Code.
 .
Don t get confused by its name  It is a classification not a regression algorithm.
 It is used to estimate discrete values   Binary values like 0 1  yes no  true false   based on given set of independent variable s .
 In simple words  it predicts the probability of occurrence of an event by fitting data to a logit function.
 Hence  it is also known as logit regression.
 Since  it predicts the probability  its output values lies between 0 and 1  as expected .
Again  let us try and understand this through a simple example.
Let s say your friend gives you a puzzle to solve.
 There are only 2 outcome scenarios   either you solve it or you don t.
 Now imagine  that you are being given wide range of puzzles   quizzes in an attempt to understand which subjects you are good at.
 The outcome to this study would be something like this   if you are given a trignometry based tenth grade problem  you are 70  likely to solve it.
 On the other hand  if it is grade fifth history question  the probability of getting an answer is only 30 .
 This is what Logistic Regression provides you.
Coming to the math  the log odds of the outcome is modeled as a linear combination of the predictor variables.
Above  p is the probability of presence of the characteristic of interest.
 It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors  like in ordinary regression .
Now  you may ask  why take a log  For the sake of simplicity  let s just say that this is one of the best mathematical way to replicate a step function.
 I can go in more details  but that will beat the purpose of this article.
Python Code.
R Code.
 .
There are many different steps that could be tried in order to improve the model .
 .
This is one of my favorite algorithm and I use it quite frequently.
 It is a type of supervised learning algorithm that is mostly used for classification problems.
 Surprisingly  it works for both categorical and continuous dependent variables.
 In this algorithm  we split the population into two or more homogeneous sets.
 This is done based on most significant attributes  independent variables to make as distinct groups as possible.
 For more details  you can read  Decision Tree Simplified.
.
source  statsexchange.
In the image above  you can see that population is classified into four different groups based on multiple attributes to identify  if they will play or not .
 To split the population into different heterogeneous groups  it uses various techniques like Gini  Information Gain  Chi square  entropy.
The best way to understand how decision tree works  is to play Jezzball   a classic game from Microsoft  image below .
 Essentially  you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.
.
So  every time you split the room with a wall  you are trying to create 2 different populations with in the same room.
 Decision trees work in very similar fashion by dividing a population in as different groups as possible.
More  Simplified Version of Decision Tree Algorithms.
R Code.
 .
It is a classification method.
 In this algorithm  we plot each data item as a point in n dimensional space  where n is number of features you have  with the value of each feature being the value of a particular coordinate.
For example  if we only had two features like Height and Hair length of an individual  we d first plot these two variables in two dimensional space where each point has two co ordinates  these co ordinates are known as Support Vectors .
.
Now  we will find some line that splits the data between the two differently classified groups of data.
 This will be the line such that the distances from the closest point in each of the two groups will be farthest away.
.
In the example shown above  the line which splits the data into two differently classified groups is the black line  since the two closest points are the farthest apart from the line.
 This line is our classifier.
 Then  depending on where the testing data lands on either side of the line  that s what class we can classify the new data as.
More  Simplified Version of Support Vector Machine.
Think of this algorithm as playing JezzBall in n dimensional space.
 The tweaks in the game are .
 .
R Code.
 .
It is a classification technique based on Bayes  theorem with an assumption of independence between predictors.
 In simple terms  a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.
 For example  a fruit may be considered to be an apple if it is red  round  and about 3 inches in diameter.
 Even if these features depend on each other or upon the existence of the other features  a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.
Naive Bayesian model is easy to build and particularly useful for very large data sets.
 Along with simplicity  Naive Bayes is known to outperform even highly sophisticated classification methods.
Bayes theorem provides a way of calculating posterior probability P c x  from P c   P x  and P x c .
 Look at the equation below .
Here .
Example  Let s understand it using an example.
 Below I have a training data set of weather and corresponding target variable  Play .
 Now  we need to classify whether players will play or not based on weather condition.
 Let s follow the below steps to perform it.
Step 1  Convert the data set to frequency table.
Step 2  Create Likelihood table by finding the probabilities like Overcast probability   0.
29 and probability of playing is 0.
64.
.
Step 3  Now  use Naive Bayesian equation to calculate the posterior probability for each class.
 The class with the highest posterior probability is the outcome of prediction.
Problem  Players will pay if weather is sunny  is this statement is correct .
We can solve it using above discussed method  so P Yes   Sunny    P  Sunny   Yes    P Yes    P  Sunny .
Here we have P  Sunny  Yes    3 9   0.
33  P Sunny    5 14   0.
36  P  Yes   9 14   0.
64.
Now  P  Yes   Sunny    0.
33   0.
64   0.
36   0.
60  which has higher probability.
Naive Bayes uses a similar method to predict the probability of different class based on various attributes.
 This algorithm is mostly used in text classification and with problems having multiple classes.
R Code.
 .
It can be used for both classification and regression problems.
 However  it is more widely used in classification problems in the industry.
 K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors.
 The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.
These distance functions can be Euclidean  Manhattan  Minkowski and Hamming distance.
 First three functions are used for continuous function and fourth one  Hamming  for categorical variables.
 If K   1  then the case is simply assigned to the class of its nearest neighbor.
 At times  choosing K turns out to be a challenge while performing KNN modeling.
More  Introduction to k nearest neighbors   Simplified.
.
KNN can easily be mapped to our real lives.
 If you want to learn about a person  of whom you have no information  you might like to find out about his close friends and the circles he moves in and gain access to his her information .
Things to consider before selecting KNN .
R Code.
 .
It is a type of unsupervised algorithm which  solves the clustering problem.
 Its procedure follows a simple and easy  way to classify a given data set through a certain number of  clusters  assume k clusters .
 Data points inside a cluster are homogeneous and heterogeneous to peer groups.
Remember figuring out shapes from ink blots  k means is somewhat similar this activity.
 You look at the shape and spread to decipher how many different clusters   population are present .
.
How K means forms cluster .
How to determine value of K .
In K means  we have clusters and each cluster has its own centroid.
 Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster.
 Also  when the sum of square values for all the clusters are added  it becomes total within sum of square value for the cluster solution.
We know that as the number of cluster increases  this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k  and then much more slowly after that.
 Here  we can find the optimum number of cluster.
.
R Code.
 .
Random Forest is a trademark term for an ensemble of decision trees.
 In Random Forest  we ve collection of decision trees  so known as  Forest  .
 To classify a new object based on attributes  each tree gives a classification and we say the tree  votes  for that class.
 The forest chooses the classification having the most votes  over all the trees in the forest .
Each tree is planted   grown as follows .
For more details on this algorithm  comparing with decision tree and tuning model parameters  I would suggest you to read these articles .
Introduction to Random forest   Simplified.
Comparing a CART model to Random Forest  Part 1 .
Comparing a Random Forest to a CART model  Part 2 .
Tuning the parameters of your Random Forest model.
Python.
R Code.
 .
In the last 4 5 years  there has been an exponential increase in data capturing at every possible stages.
 Corporates  Government Agencies  Research organisations are not only coming with new sources but also they are capturing data in great detail.
For example  E commerce companies are capturing more details about customer like their demographics  web crawling history  what they like or dislike  purchase history  feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.
As a data scientist  the data we are offered also consist of many features  this sounds good for building good robust model but there is a challenge.
 How d you identify highly significant variable s  out 1000 or 2000  In such cases  dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree  Random Forest  PCA  Factor Analysis  Identify based on correlation matrix  missing value ratio and others.
To know more about this algorithms  you can read  Beginners Guide To Learn Dimension Reduction Techniques .
 .
GBM   AdaBoost are boosting algorithms used when we deal with plenty of data to make a prediction with high prediction power.
 Boosting is an ensemble learning algorithm which combines the prediction of several base estimators in order to improve robustness over a single estimator.
 It combines multiple weak or average predictors to a build strong predictor.
 These boosting algorithms always work well in data science competitions like Kaggle  AV Hackathon  CrowdAnalytix.
More  Know about Gradient and AdaBoost in detail.
GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the difference between these two algorithms.
By now  I am sure  you would have an idea of commonly used machine learning algorithms.
 My sole intention behind writing this article and providing the codes in R and Python is to get you started right away.
 If you are keen to master machine learning  start right away.
 Take up problems  develop a physical understanding of the process  apply these codes and see the fun .
Did you find this article useful   Share your views and opinions in the comments section below.
Awesowe compilation   Thank you.
Thank you very much  A Very useful and excellent compilation.
 I have already bookmarked this page.
Straight  Informative and effective   Thank you.
Good Summary airticle.
Super Compilation .
Wonderful  Really helpful.
Very nicely done  Thanks for this.
Thank you  Well presented article.
Thank you  Well presented.
Hello .
Superb information in just one blog.
 Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated.
Hello .
Superb information in just one blog.
 Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated .
    is used to select the variables that you ll be using for a particular model.
  label .
     Uses all your Attributes  label Att1   Att2     Uses only Att1 and Att2 to create the model.
Enjoyed the simplicity.
 Thanks for the effort.
Great Article  Helps a lot  as naive in Machine Learning.
Hi All .
Thanks for the comment  .
Very good summary.
Thank  One simple point.
 The reason for taking the log p  1 p   in Logistic Regression is to make the equation linear  I.
e.
  easy to solve.
Thanks Dalila   .
That s not the reason for taking the log.
 The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes.
 First of all the assumption of linearity or otherwise introduces bias.
 However  logistic regression being a parametric model some bias is inevitable.
 The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason.
Now coming to the choice of log  it is just a convention.
 Basically  once we have decided to go with a linear model  in the case of one attribute we model the probability by.
p x    f  ax b .
such that p  infinity  0 and p infinity  0.
 It so happens that this is satisfied by.
p x    exp ax b    1   exp ax b  .
which can be re written as.
log p x   1 p x     a x  b.
While I am at it  it may be useful to talk about another point.
 One should ask is why we don t use least square method.
 The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process.
 For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method.
 So deep down linear regression and logistic regression both use maximum likelihood estimates.
 Its just that they are max likelihoods according to different distributions.
Nice summary   Huzefa  you shouldn t replace the     in the R code  it basically means  as a function of .
 You can also keep the  .
  right after  it stands for  all other variables in the dataset provided .
 If you want to be explicit  you can write y   x1   x2     where x1  x2 .
.
 are the names of the columns of your data.
frame or data.
table.
 Further note on formula specification  by default R adds an intercept  so that y   x is equivalent to y   1   x  you can remove it via y   0   x.
 Interactions are specified with either    which also adds the two variables  or    which only adds the interaction term .
 y   x1   x2 is equivalent to y   x1   x2   x1   x2.
 Hope this helps .
You did a Wonderful job  This is really helpful.
 Thanks .
I took the Stanford Coursera ML class  but have not used it  and I found this to be an incredibly useful summary.
 I appreciate the real world analogues  such as your mention of Jezzball.
 And showing the brief code snips is terrific.
This is very easy and helpful than any other courses I have completed.
 simple.
 clear.
 To the point.
You Sir are a gentleman and a scholar .
Hi Sunil .
This is really superb tutorial along with good examples and codes which is surely much helpful.
 Just  can you add Neural Network here in simple terms with example and code.
Errata   fit    kmeans X  3    5 cluster solution It s a 3 cluster solution.
Well done  Thank you .
This is a great resource overall and surely the product of a lot of work.
Just a note as I go through this  your comment on Logistic Regression not actually being regression is in fact wrong.
 It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability.
 it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression.
 As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example.
I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1.
 Thanks for the great article overall.
Very Nice.
  .
Thank you.
.
 reallu helpful article.
I wanted to know if I can use rattle instead of writing the R code explicitly.
Thank you.
 Very nice and useful article.
This is such a wonderful article.
Informative and easy to follow.
 I ve recently started following several pages like this one and this is the best material ive seen yet.
One of the best content ever read regarding algorithms.
Thank you so much for this article.
Cool stuff  I just can t get the necessary libraries .
looks sgood article.
 Do I need any data to do the examples .
Good Article.
I have to thank you for this informative summary.
 Really useful .
Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it.
 Cooking recipes like these are the ones that place people in Drew Conway s danger zone  https   www.
quora.
com In the data science venn diagram why is the common region of Hacking Skills and Substantive Expertise considered as danger zone   thus making programmers the worst data analysts  let alone scientists  that requires another mindset completely .
 I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background.
 Otherwise you could end up like Google  Target  Telefonica  or Google  again  and become a poster boy for  The Big Flops of Big Data .
Do you have a better article Please share .
Great article.
 It really summarize some of the most important topics on machine learning.
 But as asked above I would like to present thedevmasters.
com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends.
awesome   recommended this article to all my friends.
Very succinct description of some important algorithms.
 Thanks.
 I d like to point out a mistake in the SVM section.
 You say  where each point has two co ordinates  these co ordinates are known as Support Vectors  .
 This is not correct  the coordinates are just features.
 Its the points lying on the margin that are called the  support vectors .
 These are the points that  support  the margin i.
e.
 define it  as opposed to a weighted average of all points for instance.
 .
Thank you for this wonderful article it s proven helpful.
Thank you very much  A Very useful and excellent compilation.
Very good information interms of initial knowledge Note one warning  many methods can be fitted into a particular problem  but result might not be what you wish.
 Hence you must always compare models  understand residuals profile and how prediction really predicts.
 In that sense  analysis of data is never ending.
 In R  use summary  plot and check for assumptions validity .
The amazing article.
 I m new in data analysis.
 It s very useful and easy to understand.
Thanks .
This is really good article  also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know   what and where to apply in machine learning .
a very very helpful tutorial.
 Thanks a lot you guys.
The amazing article.
Analytics Vidhya   I am loving it.
Good article.
 thank you for explaining with python.
very useful compilation.
 Thanks .
Very precise quick tutorial for those who want to gain insight of machine learning.
great.
Very useful and informative.
 Thanks for sharing it.
Superb .
great summary  Thank you.
Great article.
 It would have become even better if you had some test data with each code snippet.
 Add metrics and hyper parameter tunning for each of these models.
Thanks for the  jezzball  example.
 You made my day .
Nicely complied.
 Every explanation is crystal clear and very easy to digest.
 Thanks for sharing knowledge.
Perfect  It s exactly what I was looking for  Thanks for the explanation and thanks for sharing your knowledge with us .
Very useful summary.
 Thank you.
Very informative article.
 For a person new to machine learning  this article gives a good starting point.
Good article.
 thank you for explaining with python.
Good article.
 thank you for explaining with python.
  Kareermatrix.
Hi Friends  i m new person to these machine learning algorithms.
 i have some questions.
  1  we have so many ML algorithms.
 but how can we choose the algorithms which one is suitable for my data set  2  How does these algorithms works.
  3  why only these particular algorithms.
  why not others.
 .
Nice Article.
.
 Thanks for your effort.
hello.
I have to implement machine learning algorithms in python so could you help me in this.
 any body provide me the proper code for any algorithm.
All programe has error named Error in model.
frame.
default formula   as.
list y train    .
  data   x    invalid type  list  for variable  as.
list y train  .
What is that  .
I think that  y train  is data frame and it cannot be converted directly to list with  as.
list  command.
 Try this instead.
y train    as.
list as.
data.
frame t y train   .
See if this works for you.
Very nice summary .
Can you tell how to get machine learning problems for practice .
Analytics Vidhya has some practice datasets.
 Check Analytics Vidhya hackathon.
 Also UCI machine learning repository is a phenomenal place.
 Google it and enjoy.
Do you have R codes based on caret .
Yes.
.
Receive awesome tips  guides  infographics and become expert at .
Interact with thousands of data science professionals across the globe .
.
 P.
S.
 We only publish awesome content.
 We will never share your information with anyone.
Machine learning is a branch in computer science that studies the design of algorithms that can learn.
 Typical machine learning tasks are concept learning  function learning or  predictive modeling   clustering and finding predictive patterns.
 These tasks are learned through available data that were observed through experiences or instructions  for example.
 Machine learning hopes that including the experience into its tasks will eventually improve the learning.
 The ultimate goal is to improve the learning in such a way that it becomes automatic  so that humans like ourselves don t need to interfere any more.
