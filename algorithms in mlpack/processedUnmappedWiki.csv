algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Discriminant analysis.;Linear discriminant analysis;discriminant_analysis;0.6363636363636364;0.2;1.0;1.8363636363636364;0.0;1;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Nearest neighbors .;Nearest neighbor;nearest_neighbors;0.6363636363636364;0.2;1.0;1.8363636363636364;0.0;1;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Bernoulli distribution;bernoulli_random_variable;1.0;0.0018867924528301887;1.0;2.0018867924528303;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;http://www.dataschool.io/comparing-supervised-learning-algorithms/;In the data science course that I instruct, we cover most of the data science pipeline but focus especially on machine learning.;Data science;data_science;0.8181818181818182;0.09090909090909091;1.0;1.9090909090909092;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Maximum likelihood estimation;maximum_likelihood_estimate;0.8787878787878788;0.0018867924528301887;1.0;1.880674671240709;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;Machine learning;statistical_learning;0.7272727272727273;0.0018867924528301887;1.0;1.7291595197255574;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://en.wikipedia.org/wiki/Supervised_learning;The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k_nearest neighbor algorithm, and Neural Networks .;Machine learning;learning_algorithms;0.6363636363636364;0.09090909090909091;1.0;1.7272727272727273;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Col  Dan Sulzinger says  March 1, 2016 at 1 21 am Good Article .;Good article;good_article;0.6363636363636364;0.005660377358490566;1.0;1.642024013722127;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;Frame (networking);data_frame;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.7071067811865475;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Estimation;estimate;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;SÃ¶lve;solve;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;Table (information);data_table;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.7071067811865475;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;Marine regression;regression;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.7071067811865475;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;Danger Zone (film);danger_zone;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;Poster child;poster_boy;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .;Knowledge sharing;sharing_knowledge;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Delilah;dalila;0.6363636363636364;0.0;1.0;1.6363636363636362;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Correlation and dependence;linear_relationship;0.60606060606;0.0018867924528301887;1.0;1.60794739851283;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Parametric model;parametric_model;0.5757575757581818;0.0018867924528301887;1.0;1.577644368211012;1.0;0;Sun Jun 25 19:20:46 IST 2017
algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Linear model;linear_model;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;1.0;0;Sun Jun 25 19:20:46 IST 2017
genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/;Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras July 21, 2016.;Time series;time_series_prediction;1.0;0.029411764705882353;0.42857142857142855;1.4579831932773109;0.0;0;Sun Jun 25 19:20:46 IST 2017
genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/; Recommender Systems.;Recommender system;recommender_systems;0.6363636363636364;0.029411764705882353;0.42857142857142855;1.0943468296409473;0.0;0;Sun Jun 25 19:20:46 IST 2017
quantum algorithms for supervised and unsupervised machine learning;https://en.wikipedia.org/wiki/Quantum_machine_learning;Quantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning.;Quantum machine learning;quantum_machine_learning;0.8181818181818182;0.038461538461538464;0.0;0.8566433566433567;0.0;0;Sun Jun 25 19:20:46 IST 2017
