Common classification algorithms include:
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)
Support vector machines (SVM) Neural networks Na√Øve Bayes classifier Decision trees Discriminant analysis Nearest neighbors (kNN)





Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems.. Random forest for classification and regression problems.. Support vector machines for classification problems..
The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron).
v t e
v t e
v t e
Machine learning


Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
Supervised learning is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. A test dataset is often used to validate the model. Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.










Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.
Kuber says: August 10, 2015 at 11:59 pm Awesowe compilation!! Thank you. Karthikeyan says: August 11, 2015 at 3:13 am Thank you very much, A Very useful and excellent compilation. I have already bookmarked this page. hemanth says: August 11, 2015 at 4:50 am Straight, Informative and effective!! Thank you venugopal says: August 11, 2015 at 6:05 am Good Summary airticle Dr Venugopala Rao says: August 11, 2015 at 6:27 am Super Compilation‚Ä¶ Kishor Basyal says: August 11, 2015 at 7:30 am Wonderful! Really helpful Brian Thomas says: August 11, 2015 at 9:24 am Very nicely done! Thanks for this. Tesfaye says: August 11, 2015 at 10:30 am Thank you! Well presented article. Tesfaye says: August 11, 2015 at 10:31 am Thank you! Well presented. Huzefa says: August 11, 2015 at 3:53 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated Huzefa says: August 11, 2015 at 3:54 pm Hello, Superb information in just one blog. Can anyone help me to run the codes in R what should be replaced with ‚Äú~‚Äù symbol in codes? Help is appreciated . AshuthoshGowda says: October 29, 2016 at 12:06 am ‚Äú~‚Äù is used to select the variables that you‚Äôll be using for a particular model. ‚Äúlabel~.,‚Äù ‚Äì Uses all your Attributes ‚Äúlabel~Att1 + Att2,‚Äù ‚Äì Uses only Att1 and Att2 to create the model Sudipta Basak says: August 12, 2015 at 3:35 am Enjoyed the simplicity. Thanks for the effort. Im_utm says: August 12, 2015 at 2:37 pm Great Article‚Ä¶ Helps a lot, as naive in Machine Learning. Sunil Ray says: August 14, 2015 at 7:36 am Hi All, Thanks for the comment ‚Ä¶ Dalila says: August 14, 2015 at 1:35 pm Very good summary. Thank! One simple point. The reason for taking the log(p/(1-p)) in Logistic Regression is to make the equation linear, I.e., easy to solve. Sunil Ray says: August 21, 2015 at 5:21 am Thanks Dalila‚Ä¶ üôÇ Borun Chowdhury says: April 21, 2016 at 8:48 am That‚Äôs not the reason for taking the log. The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes. First of all the assumption of linearity or otherwise introduces bias. However, logistic regression being a parametric model some bias is inevitable. The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason. Now coming to the choice of log, it is just a convention. Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p(x) = f( ax+b) such that p(-infinity)=0 and p(infinity)=0. It so happens that this is satisfied by p(x) = exp(ax+b)/ (1 + exp(ax+b)) which can be re-written as log(p(x)/(1-p(x)) = a x+ b While I am at it, it may be useful to talk about another point. One should ask is why we don‚Äôt use least square method. The reason is that a yes/no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process. For linear regression the assumption is that the residuals around the ‚Äòtrue‚Äô function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method. So deep down linear regression and logistic regression both use maximum likelihood estimates. Its just that they are max likelihoods according to different distributions. Statis says: August 19, 2015 at 12:14 am Nice summary! @Huzefa: you shouldn‚Äôt replace the ‚Äú~‚Äù in the R code, it basically means ‚Äúas a function of‚Äù. You can also keep the ‚Äú.‚Äù right after, it stands for ‚Äúall other variables in the dataset provided‚Äù. If you want to be explicit, you can write y ~ x1 + x2 + ‚Ä¶ where x1, x2 .. are the names of the columns of your data.frame or data.table. Further note on formula specification: by default R adds an intercept, so that y ~ x is equivalent to y ~ 1 + x, you can remove it via y ~ 0 + x. Interactions are specified with either * (which also adds the two variables) or : (which only adds the interaction term). y ~ x1 * x2 is equivalent to y ~ x1 + x2 + x1 : x2. Hope this helps! Chris says: August 26, 2015 at 1:01 am You did a Wonderful job! This is really helpful. Thanks! Glenn Nelson says: September 10, 2015 at 7:48 pm I took the Stanford-Coursera ML class, but have not used it, and I found this to be an incredibly useful summary. I appreciate the real-world analogues, such as your mention of Jezzball. And showing the brief code snips is terrific. Shankar Pandala says: September 15, 2015 at 12:09 pm This is very easy and helpful than any other courses I have completed. simple. clear. To the point. markpratley says: September 26, 2015 at 9:29 am You Sir are a gentleman and a scholar! whystatistics says: September 29, 2015 at 10:25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful. Just, can you add Neural Network here in simple terms with example and code. Sayan Putatunda says: November 1, 2015 at 7:00 am Errata:- fit <- kmeans(X, 3) # 5 cluster solution It`s a 3 cluster solution. Baha says: November 27, 2015 at 1:13 pm Well done, Thank you! Benjamin says: December 5, 2015 at 7:00 pm This is a great resource overall and surely the product of a lot of work. Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong. It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability. it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression. As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example. I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1. Thanks for the great article overall. Ashish Yelkar says: January 6, 2016 at 6:28 am Very Nice.!! Bansari Shah says: January 14, 2016 at 6:27 am Thank you.. reallu helpful article ayushgg92 says: January 22, 2016 at 9:54 am I wanted to know if I can use rattle instead of writing the R code explicitly Debasis says: January 22, 2016 at 10:35 am Thank you. Very nice and useful article.. Debashis says: February 16, 2016 at 8:19 am This is such a wonderful article. Anthony says: February 16, 2016 at 8:39 am Informative and easy to follow. I‚Äôve recently started following several pages like this one and this is the best material ive seen yet. Akhil says: February 17, 2016 at 3:55 am One of the best content ever read regarding algorithms. Swathi says: February 17, 2016 at 12:02 pm Thank you so much for this article N√≠colas Robles says: February 18, 2016 at 5:51 am Cool stuff! I just can‚Äôt get the necessary libraries‚Ä¶ wizzerd says: February 26, 2016 at 12:09 pm looks sgood article. Do I need any data to do the examples? Col. Dan Sulzinger says: March 1, 2016 at 1:21 am Good Article. Pansy says: March 8, 2016 at 3:04 pm I have to thank you for this informative summary. Really useful! J says: March 10, 2016 at 8:54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it. Cooking recipes like these are the ones that place people in Drew Conway‚Äôs danger zone (https://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), thus making programmers the worst data analysts (let alone scientists, that requires another mindset completely). I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background. Otherwise you could end up like Google, Target, Telefonica, or Google (again) and become a poster boy for ‚ÄúThe Big Flops of Big Data‚Äù. George says: June 17, 2016 at 1:34 pm Do you have a better article?Please share‚Ä¶ Robin White says: March 15, 2016 at 11:38 pm Great article. It really summarize some of the most important topics on machine learning. But as asked above I would like to present thedevmasters.com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends. salman ahmed says: March 19, 2016 at 8:29 am awesome , recommended this article to all my friends Borun Chowdhury says: April 21, 2016 at 8:13 am Very succinct description of some important algorithms. Thanks. I‚Äôd like to point out a mistake in the SVM section. You say ‚Äúwhere each point has two co-ordinates (these co-ordinates are known as Support Vectors)‚Äù. This is not correct, the coordinates are just features. Its the points lying on the margin that are called the ‚Äòsupport vectors‚Äô. These are the points that ‚Äòsupport‚Äô the margin i.e. define it (as opposed to a weighted average of all points for instance.) Isaac says: May 24, 2016 at 2:29 am Thank you for this wonderful article‚Ä¶it‚Äôs proven helpful. Payal gour says: June 11, 2016 at 5:52 am Thank you very much, A Very useful and excellent compilation. nd says: June 17, 2016 at 7:39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish. Hence you must always compare models, understand residuals profile and how prediction really predicts. In that sense, analysis of data is never ending. In R, use summary, plot and check for assumptions validity . Dung Dinh says: June 17, 2016 at 10:24 am The amazing article. I‚Äôm new in data analysis. It‚Äôs very useful and easy to understand. Thanks, sabarikannan says: June 30, 2016 at 5:35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning‚Ä¶. ankita srivastava says: July 5, 2016 at 8:07 am a very very helpful tutorial. Thanks a lot you guys. Haiyan says: July 7, 2016 at 7:41 am The amazing article Namala Santosh Kumar says: July 15, 2016 at 2:33 am Analytics Vidhya ‚Äì I am loving it Mohammed Abdul Kaleem says: July 16, 2016 at 8:21 am Good article. thank you for explaining with python. Jacques Gouimenou says: August 14, 2016 at 1:57 pm very useful compilation. Thanks! Baseer says: August 18, 2016 at 5:11 am Very precise quick tutorial for those who want to gain insight of machine learning vishwas says: August 24, 2016 at 4:59 pm great Ali Kazim says: August 28, 2016 at 11:41 pm Very useful and informative. Thanks for sharing it. JS says: September 3, 2016 at 7:42 pm Superb! Denis says: September 4, 2016 at 10:53 am great summary, Thank you sanjiv says: September 8, 2016 at 4:29 am Great article. It would have become even better if you had some test data with each code snippet. Add metrics and hyper parameter tunning for each of these models Faizan says: September 13, 2016 at 9:17 am Thanks for the ‚Äújezzball‚Äù example. You made my day! Satya Swarup Dani says: September 27, 2016 at 10:41 am Nicely complied. Every explanation is crystal clear and very easy to digest. Thanks for sharing knowledge. Emerson Moizes says: October 5, 2016 at 10:58 am Perfect! It‚Äôs exactly what I was looking for! Thanks for the explanation and thanks for sharing your knowledge with us! Valery says: October 11, 2016 at 8:16 am Very useful summary. Thank you. Malini Ramamurthy says: October 14, 2016 at 5:53 am Very informative article. For a person new to machine learning, this article gives a good starting point. Shubham says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. Anand Rai says: October 21, 2016 at 7:23 pm Good article. thank you for explaining with python. ‚Äî Kareermatrix ramesh says: October 23, 2016 at 12:35 pm Hi Friends, i‚Äôm new person to these machine learning algorithms. i have some questions.? 1) we have so many ML algorithms. but how can we choose the algorithms which one is suitable for my data set? 2) How does these algorithms works.? 3) why only these particular algorithms.? why not others.? Indra says: November 3, 2016 at 11:55 pm Nice Article.. Thanks for your effort RAVI RATHORE says: November 8, 2016 at 1:44 pm hello I have to implement machine learning algorithms in python so could you help me in this. any body provide me the proper code for any algorithm. Raghav says: November 10, 2016 at 8:09 pm All programe has error named Error in model.frame.default(formula = as.list(y_train) ~ ., data = x, : invalid type (list) for variable ‚Äòas.list(y_train)‚Äô What is that ? Gaurav says: December 8, 2016 at 1:54 pm I think that ‚Äúy_train‚Äù is data frame and it cannot be converted directly to list with ‚Äúas.list‚Äù command. Try this instead y_train <- as.list(as.data.frame(t(y_train))) See if this works for you. Suman says: December 9, 2016 at 6:43 am Very nice summary! Can you tell how to get machine learning problems for practice? NSS says: January 3, 2017 at 6:20 am Analytics Vidhya has some practice datasets. Check Analytics Vidhya hackathon. Also UCI machine learning repository is a phenomenal place. Google it and enjoy. YB says: December 26, 2016 at 2:46 pm Do you have R codes based on caret? NSS says: January 3, 2017 at 6:21 am Yes.





Common terms:
This documentation is for scikit-learn version 0.18.2 ‚Äî Other versions
This documentation is for scikit-learn version 0.18.2 ‚Äî Other versions
This documentation is for scikit-learn version 0.18.2 ‚Äî Other versions
If you use the software, please consider citing scikit-learn. 1. Supervised learning.
If you use the software, please consider citing scikit-learn. 1. Supervised learning.
If you use the software, please consider citing scikit-learn. 1. Supervised learning.
Home Installation Documentation Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation Examples
Home Installation Documentation Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation Examples
Home Installation Documentation Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation Examples
Home Installation Documentation Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation Examples
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
Scikit-learn 0.18 (stable) Tutorials User guide API FAQ Contributing Scikit-learn 0.19-dev (development) Scikit-learn 0.17 Scikit-learn 0.16 Scikit-learn 0.15 PDF documentation
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1.1. Ordinary Least Squares Complexity
1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation
1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation
1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection
1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection
1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection
1.1.8.1. Mathematical formulation
1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD
1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.2.1. Details of the algorithm
1.1.15.3.1. Theoretical considerations
1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms
1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details
1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems
1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems
1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems
1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel
1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel
1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel
1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel
1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR
1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR
1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details
1.5.6.1. SGD
1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes
1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes
1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size
1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size
1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size
1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size
1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size
1.6.5.1. Nearest Shrunken Centroid
1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing
1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data
1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data
1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data
1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset
1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset
1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References
1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details
1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP)
1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP)
1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP)
1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting
1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting
1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting
1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.10.7.1. Classification criteria 1.10.7.2. Regression criteria
1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding
1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding
1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding
1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding
1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding
1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding
1.11.3.1. Usage
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.5.1. Loss Functions
1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling
1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling
1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence
1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage
1.11.5.1.1. Usage
1.11.5.3.1. Usage
1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification
1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification
1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification
1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification
1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification
1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification
1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning
1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning
1.12.3.1. Multiclass learning
1.12.4.1. Multiclass learning
1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline
1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline
1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline
1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline
1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline
1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection
1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection
1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection
1.14.1. Label Propagation
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes
1.1.15.2.1. Details of the algorithm
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
1.1. Generalized Linear Models 1.1.1. Ordinary Least Squares 1.1.1.1. Ordinary Least Squares Complexity 1.1.2. Ridge Regression 1.1.2.1. Ridge Complexity 1.1.2.2. Setting the regularization parameter: generalized Cross-Validation 1.1.3. Lasso 1.1.3.1. Setting regularization parameter 1.1.3.1.1. Using cross-validation 1.1.3.1.2. Information-criteria based model selection 1.1.4. Multi-task Lasso 1.1.5. Elastic Net 1.1.6. Multi-task Elastic Net 1.1.7. Least Angle Regression 1.1.8. LARS Lasso 1.1.8.1. Mathematical formulation 1.1.9. Orthogonal Matching Pursuit (OMP) 1.1.10. Bayesian Regression 1.1.10.1. Bayesian Ridge Regression 1.1.10.2. Automatic Relevance Determination - ARD 1.1.11. Logistic regression 1.1.12. Stochastic Gradient Descent - SGD 1.1.13. Perceptron 1.1.14. Passive Aggressive Algorithms 1.1.15. Robustness regression: outliers and modeling errors 1.1.15.1. Different scenario and useful concepts 1.1.15.2. RANSAC: RANdom SAmple Consensus 1.1.15.2.1. Details of the algorithm 1.1.15.3. Theil-Sen estimator: generalized-median-based estimator 1.1.15.3.1. Theoretical considerations 1.1.15.4. Huber Regression 1.1.15.5. Notes 1.1.16. Polynomial regression: extending linear models with basis functions 1.2. Linear and Quadratic Discriminant Analysis 1.2.1. Dimensionality reduction using Linear Discriminant Analysis 1.2.2. Mathematical formulation of the LDA and QDA classifiers 1.2.3. Mathematical formulation of LDA dimensionality reduction 1.2.4. Shrinkage 1.2.5. Estimation algorithms 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.4.1. Classification 1.4.1.1. Multi-class classification 1.4.1.2. Scores and probabilities 1.4.1.3. Unbalanced problems 1.4.2. Regression 1.4.3. Density estimation, novelty detection 1.4.4. Complexity 1.4.5. Tips on Practical Use 1.4.6. Kernel functions 1.4.6.1. Custom Kernels 1.4.6.1.1. Using Python functions as kernels 1.4.6.1.2. Using the Gram matrix 1.4.6.1.3. Parameters of the RBF Kernel 1.4.7. Mathematical formulation 1.4.7.1. SVC 1.4.7.2. NuSVC 1.4.7.3. SVR 1.4.8. Implementation details 1.5. Stochastic Gradient Descent 1.5.1. Classification 1.5.2. Regression 1.5.3. Stochastic Gradient Descent for sparse data 1.5.4. Complexity 1.5.5. Tips on Practical Use 1.5.6. Mathematical formulation 1.5.6.1. SGD 1.5.7. Implementation details 1.6. Nearest Neighbors 1.6.1. Unsupervised Nearest Neighbors 1.6.1.1. Finding the Nearest Neighbors 1.6.1.2. KDTree and BallTree Classes 1.6.2. Nearest Neighbors Classification 1.6.3. Nearest Neighbors Regression 1.6.4. Nearest Neighbor Algorithms 1.6.4.1. Brute Force 1.6.4.2. K-D Tree 1.6.4.3. Ball Tree 1.6.4.4. Choice of Nearest Neighbors Algorithm 1.6.4.5. Effect of leaf_size 1.6.5. Nearest Centroid Classifier 1.6.5.1. Nearest Shrunken Centroid 1.6.6. Approximate Nearest Neighbors 1.6.6.1. Locality Sensitive Hashing Forest 1.6.6.2. Mathematical description of Locality Sensitive Hashing 1.7. Gaussian Processes 1.7.1. Gaussian Process Regression (GPR) 1.7.2. GPR examples 1.7.2.1. GPR with noise-level estimation 1.7.2.2. Comparison of GPR and Kernel Ridge Regression 1.7.2.3. GPR on Mauna Loa CO2 data 1.7.3. Gaussian Process Classification (GPC) 1.7.4. GPC examples 1.7.4.1. Probabilistic predictions with GPC 1.7.4.2. Illustration of GPC on the XOR dataset 1.7.4.3. Gaussian process classification (GPC) on iris dataset 1.7.5. Kernels for Gaussian Processes 1.7.5.1. Gaussian Process Kernel API 1.7.5.2. Basic kernels 1.7.5.3. Kernel operators 1.7.5.4. Radial-basis function (RBF) kernel 1.7.5.5. Mat√©rn kernel 1.7.5.6. Rational quadratic kernel 1.7.5.7. Exp-Sine-Squared kernel 1.7.5.8. Dot-Product kernel 1.7.5.9. References 1.7.6. Legacy Gaussian Processes 1.7.6.1. An introductory regression example 1.7.6.2. Fitting Noisy Data 1.7.6.3. Mathematical formulation 1.7.6.3.1. The initial assumption 1.7.6.3.2. The best linear unbiased prediction (BLUP) 1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP) 1.7.6.4. Correlation Models 1.7.6.5. Regression Models 1.7.6.6. Implementation details 1.8. Cross decomposition 1.9. Naive Bayes 1.9.1. Gaussian Naive Bayes 1.9.2. Multinomial Naive Bayes 1.9.3. Bernoulli Naive Bayes 1.9.4. Out-of-core naive Bayes model fitting 1.10. Decision Trees 1.10.1. Classification 1.10.2. Regression 1.10.3. Multi-output problems 1.10.4. Complexity 1.10.5. Tips on practical use 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART 1.10.7. Mathematical formulation 1.10.7.1. Classification criteria 1.10.7.2. Regression criteria 1.11. Ensemble methods 1.11.1. Bagging meta-estimator 1.11.2. Forests of randomized trees 1.11.2.1. Random Forests 1.11.2.2. Extremely Randomized Trees 1.11.2.3. Parameters 1.11.2.4. Parallelization 1.11.2.5. Feature importance evaluation 1.11.2.6. Totally Random Trees Embedding 1.11.3. AdaBoost 1.11.3.1. Usage 1.11.4. Gradient Tree Boosting 1.11.4.1. Classification 1.11.4.2. Regression 1.11.4.3. Fitting additional weak-learners 1.11.4.4. Controlling the tree size 1.11.4.5. Mathematical formulation 1.11.4.5.1. Loss Functions 1.11.4.6. Regularization 1.11.4.6.1. Shrinkage 1.11.4.6.2. Subsampling 1.11.4.7. Interpretation 1.11.4.7.1. Feature importance 1.11.4.7.2. Partial dependence 1.11.5. VotingClassifier 1.11.5.1. Majority Class Labels (Majority/Hard Voting) 1.11.5.1.1. Usage 1.11.5.2. Weighted Average Probabilities (Soft Voting) 1.11.5.3. Using the VotingClassifier with GridSearch 1.11.5.3.1. Usage 1.12. Multiclass and multilabel algorithms 1.12.1. Multilabel classification format 1.12.2. One-Vs-The-Rest 1.12.2.1. Multiclass learning 1.12.2.2. Multilabel learning 1.12.3. One-Vs-One 1.12.3.1. Multiclass learning 1.12.4. Error-Correcting Output-Codes 1.12.4.1. Multiclass learning 1.12.5. Multioutput regression 1.12.6. Multioutput classification 1.13. Feature selection 1.13.1. Removing features with low variance 1.13.2. Univariate feature selection 1.13.3. Recursive feature elimination 1.13.4. Feature selection using SelectFromModel 1.13.4.1. L1-based feature selection 1.13.4.2. Randomized sparse models 1.13.4.3. Tree-based feature selection 1.13.5. Feature selection as part of a pipeline 1.14. Semi-Supervised 1.14.1. Label Propagation 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) 1.17.1. Multi-layer Perceptron 1.17.2. Classification 1.17.3. Regression 1.17.4. Regularization 1.17.5. Algorithms 1.17.6. Complexity 1.17.7. Mathematical formulation 1.17.8. Tips on Practical Use 1.17.9. More control with warm_start
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
Machine learning algorithms can be divided into 3 broad categories‚Ää‚Äî‚Ääsupervised learning, unsupervised learning, and reinforcement learning.Supervised learning¬†is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances.¬†Unsupervised learning¬†is useful in cases where the challenge is to discover implicit relationships in a given¬†unlabeled¬†dataset (items are not pre-assigned).¬†Reinforcement learning¬†falls between these 2 extremes‚Ää‚Äî‚Ääthere is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn‚Äôt learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
