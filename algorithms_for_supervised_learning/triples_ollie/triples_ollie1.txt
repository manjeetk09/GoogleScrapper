Common classification algorithms include.
No extractions found.

Support vector machines .
No extractions found.

Neural networks.
No extractions found.

Na ve Bayes classifier.
No extractions found.

Decision trees.
No extractions found.

Discriminant analysis.
No extractions found.

Nearest neighbors .
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

Some popular examples of supervised machine learning algorithms are.
No extractions found.

 Linear regression for regression problems.
No extractions found.

 Random forest for classification and regression problems.
No extractions found.

 Support vector machines for classification problems.
No extractions found.

The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k_nearest neighbor algorithm, and Neural Networks .
No extractions found.

v.
No extractions found.

t.
No extractions found.

e.
No extractions found.

Machine learning.
No extractions found.

.
No extractions found.

.
No extractions found.

Supervised learning is a type of machine learning algorithm that uses a known dataset to make predictions.
0.962: (Supervised learning; is a type of; machine learning algorithm that uses a known dataset to make predictions)
0.768: (Supervised learning; is; a type of machine learning algorithm)
0.62: (a known dataset; be uses by; machine learning algorithm)

 The training dataset includes input data and response values.
0.782: (The training dataset; includes; input data and response values)

 From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset.
0.803: (the supervised learning algorithm; seeks to build; a model that can make predictions of the response values for a new dataset)
0.708: (predictions of the response values; can be make for; a new dataset)
0.456: (the supervised learning algorithm; seeks from; it)

 A test dataset is often used to validate the model.
0.761: (A test dataset; to validate; the model)

 Using larger training datasets often yield models with higher predictive power that can generalize well for new datasets.
0.762: (larger training datasets; often yield; models)

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

Kuber says  August 10, 2015 at 11 59 pm Awesowe compilation   Thank you .
0.638: (August 10 , 2015; Thank; you)[attrib=Kuber says]

Karthikeyan says  August 11, 2015 at 3 13 am Thank you very much, A Very useful and excellent compilation  I have already bookmarked this page .
0.827: (Karthikeyan; says in; August 11 , 2015)
0.799: (Karthikeyan; says at; 3 13)
0.661: (I; have already bookmarked; this page)

hemanth says  August 11, 2015 at 4 50 am Straight, Informative and effective   Thank you.
0.803: (hemanth; says in; August 11 , 2015)
0.638: (Informative and effective; Thank; you)[enabler=at 4 50 am Straight;attrib=hemanth says]

venugopal says  August 11, 2015 at 6 05 am Good Summary airticle.
0.803: (venugopal; says in; August 11 , 2015)[enabler=at 6 05 am Good Summary airticle]

Dr Venugopala Rao says  August 11, 2015 at 6 27 am Super Compilation .
0.827: (Dr Venugopala Rao; says in; August 11 , 2015)[enabler=at 6 27 am Super Compilation]

Kishor Basyal says  August 11, 2015 at 7 30 am Wonderful  Really helpful.
No extractions found.

Brian Thomas says  August 11, 2015 at 9 24 am Very nicely done  Thanks for this .
0.827: (Brian Thomas; says in; August 11 , 2015)
0.799: (Brian Thomas; says at; 9 24)

Tesfaye says  August 11, 2015 at 10 30 am Thank you  Well presented article .
0.827: (Tesfaye; says in; August 11 , 2015)
0.799: (Tesfaye; says at; 10 30)

Tesfaye says  August 11, 2015 at 10 31 am Thank you  Well presented .
0.827: (Tesfaye; says in; August 11 , 2015)
0.799: (Tesfaye; says at; 10 31)

Huzefa says  August 11, 2015 at 3 53 pm Hello, Superb information in just one blog  Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated.
0.668: (me; to run; the codes what should be replaced with symbol in codes Help)

Huzefa says  August 11, 2015 at 3 54 pm Hello, Superb information in just one blog  Can anyone help me to run the codes in R what should be replaced with     symbol in codes  Help is appreciated   AshuthoshGowda says  October 29, 2016 at 12 06 am     is used to select the variables that you ll be using for a particular model   label  ,    Uses all your Attributes  label Att1   Att2,    Uses only Att1 and Att2 to create the model.
0.723: (October 29 , 2016; to select; the variables that you ll be using for a particular model label)
0.718: (me; to run; the codes what should be replaced with symbol in codes Help)
0.628: (Att2; Att2 to create; the model)
0.606: (you; that ll be using for; a particular model label)

Sudipta Basak says  August 12, 2015 at 3 35 am Enjoyed the simplicity  Thanks for the effort .
0.827: (Sudipta Basak; says in; August 12 , 2015)
0.799: (Sudipta Basak; says at; 3 35)
0.784: (Sudipta Basak; am Enjoyed the simplicity Thanks for; the effort)[attrib=Basak says]

Im_utm says  August 12, 2015 at 2 37 pm Great Article  Helps a lot, as naive in Machine Learning .
No extractions found.

Sunil Ray says  August 14, 2015 at 7 36 am Hi All, Thanks for the comment  .
No extractions found.

Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .
0.856: (the probability; is governed by; a step function whose argument is linear in the attributes First of all the assumption of linearity or otherwise introduces bias However)
0.818: (higher bias and one; would not like to do so without; good reason Now coming to the choice of log)
0.722: (whose argument; is linear in; the attributes)
0.661: (whose argument; is; linear)
0.649: (linear regression and logistic regression; estimates; Its just that they are max)
0.631: (we; have decided to go with; a linear model)
0.62: (we; be model the probability by; p)
0.62: (it; may be; useful)
0.585: (some bias; is; inevitable)
0.581: (good reason; Now coming to; the choice of log)
0.533: (we; estimate; the probability)[enabler=that a yes no choice is a Bernoulli random variable and thus]
0.519: (linear; First of all the assumption linearity otherwise introduces However; bias)
0.515: (the log; be taking in; Logistic Regression)
0.511: (April 21 , 2016; am s; not the reason)
0.499: (Its just that they are max; be estimates by; a normal distribution and the maximum likelihood estimate)
0.488: (good summary Thank; is to make; the equation linear)
0.451: (it; be model the probability by; p)
0.444: (It; be model the probability by; p)
0.442: (I; am at; it)
0.439: (we; be model the probability fsuch; that p 0)
0.431: (it; be model the probability in; the case of one attribute)
0.424: (It; be model the probability in; the case of one attribute)
0.424: (we; be model the probability in; the case of one attribute)
0.375: (a yes no choice; is thus; a Bernoulli random)
0.283: (it; be model the probability fsuch; that p 0)
0.277: (It; be model the probability fsuch; that p 0)
0.247: (bias; be First of all the assumption linearity otherwise introduces However by; a step function)
0.153: (It; is just Basically once; its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason Now coming to the choice of log , it is just a convention Basically , once we have decided to go with a linear model , in the case of one attribute we model)

Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .
0.847: (you; can remove it via; y 0 x Interactions)[enabler=of You can also keep the right after , it stands for all other variables in the dataset provided If you want to be explicit , you can write y x1 x2 where x1 , x2 are the names of the columns of your data frame or data table Further note on formula specification by default R adds an intercept , so that y x is equivalent to y 1 x]
0.828: (the right; stands for; all other variables)
0.805: (you; can remove; it)[enabler=of You can also keep the right after , it stands for all other variables in the dataset provided If you want to be explicit , you can write y x1 x2 where x1 , x2 are the names of the columns of your data frame or data table Further note on formula specification by default R adds an intercept , so that y x is equivalent to y 1 x]
0.8: (you; can write; y x1 x2 where x1 , x2 are the names of the columns of your data frame)
0.782: (Statis; says in; August 19 , 2015)
0.749: (it; basically means as; a function)
0.749: (it; stands for; all other variables)
0.749: (Statis; says at; 12 14)
0.728: (y x1 x2 where x1 , x2 are the names of the columns of your data frame; can be write by; default R)
0.653: (you; can write y x1 x2 where x1 , x2 are the names of the columns of your data frame or data table Further note on formula specification by; default R)
0.576: (You; can also keep in; the dataset)
0.545: (it; can be remove via; y 0 x Interactions)[enabler=of You can also keep the right after , it stands for all other variables in the dataset provided If you want to be explicit , you can write y x1 x2 where x1 , x2 are the names of the columns of your data frame or data table Further note on formula specification by default R adds an intercept , so that y x is equivalent to y 1 x]
0.289: (y x; so is equivalent to; 1 x)
0.282: (y x; so is; equivalent)

Chris says  August 26, 2015 at 1 01 am You did a Wonderful job  This is really helpful  Thanks .
0.782: (You; am did; a Wonderful job This is really helpful Thanks)
0.681: (Chris; says in; August 26 , 2015)
0.64: (Chris; says at; 1 01)

Glenn Nelson says  September 10, 2015 at 7 48 pm I took the Stanford_Coursera ML class, but have not used it, and I found this to be an incredibly useful summary  I appreciate the real_world analogues, such as your mention of Jezzball  And showing the brief code snips is terrific .
0.761: (I; found; this to be an incredibly useful summary I appreciate the real_world analogues , such as your mention of Jezzball And showing the brief code snips is terrific)
0.751: (Glenn Nelson; found; this to be an incredibly useful summary I appreciate the real_world analogues , such as your mention of Jezzball And showing the brief code snips is terrific)
0.672: (I; took; the Stanford_Coursera ML class)
0.604: (I; appreciate; the real_world analogues)
0.518: (I; says in; Glenn Nelson)
0.427: (I; is; terrific)
0.394: (I; says in; I)

Shankar Pandala says  September 15, 2015 at 12 09 pm This is very easy and helpful than any other courses I have completed  simple  clear  To the point .
0.694: (I; have completed; simple clear)

markpratley says  September 26, 2015 at 9 29 am You Sir are a gentleman and a scholar .
No extractions found.

whystatistics says  September 29, 2015 at 10 25 am Hi Sunil, This is really superb tutorial along with good examples and codes which is surely much helpful  Just, can you add Neural Network here in simple terms with example and code .
0.814: (you; can add Neural Network here in; simple terms)[attrib=September 29 , 2015 says]
0.727: (Neural Network; can be add here in; simple terms)[attrib=September 29 , 2015 says]
0.668: (you; can add here; Neural Network)[attrib=September 29 , 2015 says]
0.635: (Sunil; can add Neural Network here in; simple terms)[attrib=September 29 , 2015 says]
0.445: (Sunil; can add here; Neural Network)[attrib=September 29 , 2015 says]

Sayan Putatunda says  November 1, 2015 at 7 00 am Errata _ fit  _ kmeans  5 cluster solution It s a 3 cluster solution .
0.827: (Sayan Putatunda; says in; November 1 , 2015)
0.799: (Sayan Putatunda; says at; 7 00)
0.667: (Errata _ fit;  ; kmeans 5 cluster solution It s a 3 cluster solution)
0.624: (It; s; a 3 cluster solution)
0.564: (a 3 cluster solution; be s by; kmeans 5 cluster solution)

Baha says  November 27, 2015 at 1 13 pm Well done, Thank you .
No extractions found.

Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .
0.848: (it; falls under; the umbrella of Generalized Libear Models)
0.84: (a matter of fact; falls under; the umbrella of Generalized Libear Models)
0.736: (a continuous variable; be bound between; 0 and 1)
0.731: (not actually being regression; is outputs to; a continuous variable bound between 0 and 1 that we regard as probability)
0.68: (a matter of fact; falls as; the glm R package)
0.679: (it; falls as; the glm R package)
0.629: (not actually being regression; is outputs in; fact wrong It maps)
0.563: (a note as I go through this , your comment on Logistic Regression; be the product of; a lot of work)
0.485: (a note; be your comment on; Logistic Regression)[enabler=as I go through this]
0.402: (we; that regard as; probability it makes classification easy but that is still an extra step)
0.297: (richer than 0 or 1 Thanks; be requires overall by; an extra step)

Ashish Yelkar says  January 6, 2016 at 6 28 am Very Nice   .
0.827: (Ashish Yelkar; says in; January 6 , 2016)
0.799: (Ashish Yelkar; says at; 6 28)

Bansari Shah says  January 14, 2016 at 6 27 am Thank you   reallu helpful article.
0.827: (Bansari Shah; says in; January 14 , 2016)
0.799: (Bansari Shah; says at; 6 27)
0.718: (you; am Thank reallu; helpful article)[attrib=Bansari Shah says]

ayushgg92 says  January 22, 2016 at 9 54 am I wanted to know if I can use rattle instead of writing the R code explicitly.
0.753: (ayushgg92; says in; January 22 , 2016)
0.73: (ayushgg92; says at; 9 54)
0.467: (I; says in; ayushgg92)
0.083: (I; can use; rattle)

Debasis says  January 22, 2016 at 10 35 am Thank you  Very nice and useful article  .
0.803: (Debasis; says in; January 22 , 2016)
0.783: (Debasis; says at; 10 35)

Debashis says  February 16, 2016 at 8 19 am This is such a wonderful article .
0.603: (February 16; am is; such a wonderful article)

Anthony says  February 16, 2016 at 8 39 am Informative and easy to follow  I ve recently started following several pages like this one and this is the best material ive seen yet .
0.771: (I; ve recently started following; several pages)

Akhil says  February 17, 2016 at 3 55 am One of the best content ever read regarding algorithms .
No extractions found.

Swathi says  February 17, 2016 at 12 02 pm Thank you so much for this article.
0.51: (February 17; Thank so much this article; you)[attrib=Swathi says]

N colas Robles says  February 18, 2016 at 5 51 am Cool stuff  I just can t get the necessary libraries .
0.678: (I; just can t get; the necessary libraries)

wizzerd says  February 26, 2016 at 12 09 pm looks sgood article  Do I need any data to do the examples .
0.756: (I; need; any data to do the examples)
0.639: (February 26; says in; wizzerd)

Col  Dan Sulzinger says  March 1, 2016 at 1 21 am Good Article .
0.819: (Col Dan Sulzinger; says in; March 1 , 2016 at 1 21)

Pansy says  March 8, 2016 at 3 04 pm I have to thank you for this informative summary  Really useful .
0.632: (I; have to thank; you)[enabler=for this informative summary Really useful;attrib=Pansy says]

J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .
0.749: (you; to Otherwise could end up like; Google , Target , Telefonica , or Google)
0.681: (you; not to jump could into; statistical learning)
0.664: (you; to could says in; June 17 , 2016)
0.635: (I; highly recommend; anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background Otherwise you could end up like Google , Target , Telefonica , or Google and become a poster boy for The Big Flops of Big Data George says June 17 , 2016 at 1 34 pm Do you have a better article Please share)
0.623: (you; to could says at; 1 34 pm)
0.593: (you; not to jump could without; proper statistical background)
0.579: (you; Do have; a better article)
0.572: (a poster boy; to could says in; June 17 , 2016)
0.542: (a poster boy; to could says at; 1 34 pm)
0.521: (it; does not mention; any measure of performance)
0.52: (J; says in; March 10)
0.496: (a poster boy; to Otherwise could end up like; Google , Target , Telefonica , or Google)
0.436: (a poster boy; not to jump could into; statistical learning)
0.345: (a poster boy; not to jump could without; proper statistical background)

Robin White says  March 15, 2016 at 11 38 pm Great article  It really summarize some of the most important topics on machine learning  But as asked above I would like to present thedevmasters com as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends .
0.708: (Robin White; says in; March 15)
0.649: (It; really summarize some of the most important topics on; machine learning)
0.622: (a company; to learn; more depth about machine learning with great professors and a sense of community)
0.606: (I; would like to present; thedevmasters com)[enabler=as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends]
0.504: (I; to present; thedevmasters com)[enabler=as a company with a really good course to learn more depth about machine learning with great professors and a sense of community that is always helping itself to continue learning even after the course ends]

salman ahmed says  March 19, 2016 at 8 29 am awesome , recommended this article to all my friends.
0.773: (this article; be recommended to; all my friends)

Borun Chowdhury says  April 21, 2016 at 8 13 am Very succinct description of some important algorithms  Thanks  I d like to point out a mistake in the SVM section  You say  where each point has two co_ordinates    This is not correct, the coordinates are just features  Its the points lying on the margin that are called the  support vectors   These are the points that  support  the margin i e  define it .
0.827: (Borun Chowdhury; says in; April 21 , 2016)
0.799: (Borun Chowdhury; says at; 8 13)
0.68: (each point; has; two co_ordinates This is not correct)
0.493: (I; d to point out; a mistake You say where each point has two co_ordinates This is not correct , the coordinates are just features Its the points)
0.401: (e; define; it)

Isaac says  May 24, 2016 at 2 29 am Thank you for this wonderful article it s proven helpful .
0.827: (Isaac; says in; May 24 , 2016)
0.799: (Isaac; says at; 2 29)
0.789: (you; be Thank for; this wonderful article it s proven helpful)
0.446: (it; s proven in; this wonderful article)

Payal gour says  June 11, 2016 at 5 52 am Thank you very much, A Very useful and excellent compilation .
0.803: (Payal gour; says in; June 11 , 2016)
0.783: (Payal gour; says at; 5 52)

nd says  June 17, 2016 at 7 39 am Very good information interms of initial knowledge Note one warning, many methods can be fitted into a particular problem, but result might not be what you wish  Hence you must always compare models, understand residuals profile and how prediction really predicts  In that sense, analysis of data is never ending  In R, use summary, plot and check for assumptions validity  .
0.889: (many methods; can be fitted into; a particular problem)
0.763: (prediction; really predicts in; that sense)
0.763: (residuals; really predicts in; that sense)
0.701: (analysis of data; is never ending in; R, use summary , plot and check)
0.656: (you; Hence must always compare; models)
0.633: (nd; says in; June 17 , 2016)
0.604: (nd; says at; 7 39)

Dung Dinh says  June 17, 2016 at 10 24 am The amazing article  I m new in data analysis  It s very useful and easy to understand  Thanks,.
0.827: (Dung Dinh; says in; June 17 , 2016)
0.799: (Dung Dinh; says at; 10 24)
0.634: (I; m; new)

sabarikannan says  June 30, 2016 at 5 35 am This is really good article, also if you would have explain about Anomaly dection algorithm that will really helpful for everyone to know , what and where to apply in machine learning  .
0.753: (sabarikannan; says in; June 30 , 2016)
0.73: (sabarikannan; says at; 5 35)
0.232: (you; would have explain about; Anomaly dection algorithm)

ankita srivastava says  July 5, 2016 at 8 07 am a very very helpful tutorial  Thanks a lot you guys .
No extractions found.

Haiyan says  July 7, 2016 at 7 41 am The amazing article.
No extractions found.

Namala Santosh Kumar says  July 15, 2016 at 2 33 am Analytics Vidhya   I am loving it.
0.765: (Namala Santosh Kumar; says in; July 15)
0.458: (I; am loving; it)

Mohammed Abdul Kaleem says  July 16, 2016 at 8 21 am Good article  thank you for explaining with python .
No extractions found.

Jacques Gouimenou says  August 14, 2016 at 1 57 pm very useful compilation  Thanks .
No extractions found.

Baseer says  August 18, 2016 at 5 11 am Very precise quick tutorial for those who want to gain insight of machine learning.
0.799: (Baseer; says at; 5 11)
0.765: (Baseer; says in; August 18)

vishwas says  August 24, 2016 at 4 59 pm great.
0.735: (vishwas; says in; August 24)

Ali Kazim says  August 28, 2016 at 11 41 pm Very useful and informative  Thanks for sharing it .
0.765: (Ali Kazim; says in; August 28)

JS says  September 3, 2016 at 7 42 pm Superb .
0.591: (JS; says in; September 3)

Denis says  September 4, 2016 at 10 53 am great summary, Thank you.
0.71: (September 4 , 2016; Thank; you)[attrib=Denis says]

sanjiv says  September 8, 2016 at 4 29 am Great article  It would have become even better if you had some test data with each code snippet  Add metrics and hyper parameter tunning for each of these models.
0.618: (It; would have become even better had; some test data)[enabler=with each code snippet Add metrics and hyper parameter tunning for each of these models]
0.594: (each code snippet; Add; metrics and hyper parameter tunning)
0.165: (you; would have become even better had; some test data)[enabler=with each code snippet Add metrics and hyper parameter tunning for each of these models]

Faizan says  September 13, 2016 at 9 17 am Thanks for the  jezzball  example  You made my day .
0.884: (September 13 , 2016; am Thanks for; the jezzball example)[attrib=Faizan says]
0.79: (You; made in; my day)

Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .
0.774: (Every explanation; to digest; Thanks)
0.735: (Every explanation; is; clear and very easy)

Emerson Moizes says  October 5, 2016 at 10 58 am Perfect  It s exactly what I was looking for  Thanks for the explanation and thanks for sharing your knowledge with us .
0.827: (Emerson Moizes; says in; October 5 , 2016)
0.799: (Emerson Moizes; says at; 10 58)
0.684: (your knowledge; be sharing with; us)
0.395: (I; exactly what was looking for; Thanks)

Valery says  October 11, 2016 at 8 16 am Very useful summary  Thank you .
0.827: (Valery; says in; October 11 , 2016)[enabler=at 8 16 am Very useful summary Thank you]
0.659: (useful summary; Thank; you)

Malini Ramamurthy says  October 14, 2016 at 5 53 am Very informative article  For a person new to machine learning, this article gives a good starting point .
0.827: (Malini Ramamurthy; says in; October 14 , 2016)
0.799: (Malini Ramamurthy; says at; 5 53)
0.772: (this article; gives; a good starting point)
0.488: (a good starting point; be gives by; informative article)

Shubham says  October 21, 2016 at 7 23 pm Good article  thank you for explaining with python .
0.692: (October 21; thank; you)[attrib=Shubham says]

Anand Rai says  October 21, 2016 at 7 23 pm Good article  thank you for explaining with python    Kareermatrix.
0.692: (October 21; thank; you)[attrib=Anand Rai says]

ramesh says  October 23, 2016 at 12 35 pm Hi Friends, i m new person to these machine learning algorithms  i have some questions   1  we have so many ML algorithms  but how can we choose the algorithms which one is suitable for my data set  2  How does these algorithms works   3  why only these particular algorithms   why not others  .
0.674: (one; is; suitable)
0.552: (one; which is suitable for; my data set 2 How does these algorithms works 3 why only these particular algorithms)
0.525: (we; can choose; the algorithms which one is suitable for my data)
0.37: (October 23; i have; some questions 1 we have so many ML algorithms)[attrib=ramesh says]
0.332: (2016; be October 23 i; m new person learning algorithms)

Indra says  November 3, 2016 at 11 55 pm Nice Article   Thanks for your effort.
0.764: (Indra; says in; November 3)

RAVI RATHORE says  November 8, 2016 at 1 44 pm hello I have to implement machine learning algorithms in python so could you help me in this  any body provide me the proper code for any algorithm .
0.747: (any body; provide me; the proper code)
0.689: (you; could help; me)
0.647: (I; have to implement; machine learning algorithms in python so)[attrib=RAVI RATHORE says]
0.502: (any body; be the proper code for; any algorithm)
0.462: (machine; learning algorithms so in; python)
0.459: (algorithms; be learning so in; python)

Raghav says  November 10, 2016 at 8 09 pm All programe has error named Error in model frame default   , data   x,   invalid type for variable  as list  What is that   Gaurav says  December 8, 2016 at 1 54 pm I think that  y_train  is data frame and it cannot be converted directly to list with  as list  command  Try this instead y_train  _ as list   See if this works for you .
0.883: (it; cannot be converted directly to; list)[attrib=I think]
0.803: (it; cannot be converted directly with; list command)[attrib=I think]
0.794: (that Gaurav; says in; December 8 , 2016)
0.699: (error; be named in; model frame default)
0.445: (y_train; is cannot; data frame)

Suman says  December 9, 2016 at 6 43 am Very nice summary  Can you tell how to get machine learning problems for practice  NSS says  January 3, 2017 at 6 20 am Analytics Vidhya has some practice datasets  Check Analytics Vidhya hackathon  Also UCI machine learning repository is a phenomenal place  Google it and enjoy .
0.706: (January 3 , 2017; says at; 6 20)
0.697: (Analytics Vidhya hackathon; Also is a phenomenal place google; it)
0.697: (UCI machine learning repository; Also is a phenomenal place google; it)
0.585: (Analytics Vidhya hackathon; Also is; a phenomenal place)
0.57: (UCI machine learning repository; Also is; a phenomenal place)
0.556: (NSS; says at; 6 20)

YB says  December 26, 2016 at 2 46 pm Do you have R codes based on caret  NSS says  January 3, 2017 at 6 21 am Yes .
0.846: (you; have; R codes based on caret NSS)
0.74: (R codes; be based on; caret NSS)

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

.
No extractions found.

Common terms.
No extractions found.

This documentation is for scikit_learn version 0.
0.854: (This documentation; is for; scikit_learn version)

18.
No extractions found.

2   Other versions.
No extractions found.

If you use the software, please consider citing scikit_learn.
0.19: (you; use; the software)

 1.
No extractions found.

 Supervised learning.
No extractions found.

Home.
No extractions found.

Installation.
No extractions found.

Documentation Scikit_learn 0 18 Tutorials User guide API FAQ Contributing Scikit_learn 0 19_dev Scikit_learn 0 17 Scikit_learn 0 16 Scikit_learn 0 15 PDF documentation.
No extractions found.

Examples.
No extractions found.

Scikit_learn 0 18 .
No extractions found.

Tutorials.
No extractions found.

User guide.
No extractions found.

API.
No extractions found.

FAQ.
No extractions found.

Contributing.
No extractions found.

.
No extractions found.

Scikit_learn 0 19_dev .
No extractions found.

Scikit_learn 0 17.
No extractions found.

Scikit_learn 0 16.
No extractions found.

Scikit_learn 0 15.
No extractions found.

PDF documentation.
No extractions found.

1 1  Generalized Linear Models 1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity 1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation 1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection 1 1 4  Multi_task Lasso 1 1 5  Elastic Net 1 1 6  Multi_task Elastic Net 1 1 7  Least Angle Regression 1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation 1 1 9  Orthogonal Matching Pursuit 1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD 1 1 11  Logistic regression 1 1 12  Stochastic Gradient Descent _ SGD 1 1 13  Perceptron 1 1 14  Passive Aggressive Algorithms 1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes 1 1 16  Polynomial regression  extending linear models with basis functions.
0.729: (1 1 16 Polynomial regression; extending linear models with; basis functions)
0.681: (Cross_Validation 1 1 3 Lasso 1 1 3 1; Setting; regularization parameter 1 1 3 1 1 Using cross_validation 1 1 3 1 2 Information_criteria based model selection 1 1 4 Multi_task Lasso 1 1 5 Elastic Net 1 1 6 Multi_task Elastic Net 1 1 7 Least Angle Regression 1 1 8 LARS Lasso 1 1 8 1 Mathematical formulation 1 1 9 Orthogonal Matching Pursuit 1 1 10 Bayesian Regression 1 1 10 1 Bayesian Ridge Regression 1 1 10 2 Automatic Relevance Determination _ ARD 1 1 11 Logistic regression 1 1 12 Stochastic Gradient Descent _ SGD 1 1 13 Perceptron 1 1 14 Passive Aggressive Algorithms 1 1 15 Robustness regression outliers and modeling errors 1 1 15 1 Different scenario and useful concepts 1 1 15 2 RANSAC RANdom SAmple Consensus 1 1 15 2 1 Details of the algorithm 1 1 15 3 Theil_Sen estimator generalized_median_based estimator 1 1 15 3 1 Theoretical considerations 1 1 15 4 Huber Regression 1 1 15 5 Notes 1 1 16 Polynomial regression)
0.674: (linear models; be extending with; basis functions)
0.624: (1 1 2 1 Ridge Complexity 1 1 2 2; Setting; the regularization parameter generalized Cross_Validation 1 1 3 Lasso 1 1 3 1 Setting regularization parameter 1 1 3 1 1)

1 2  Linear and Quadratic Discriminant Analysis 1 2 1  Dimensionality reduction using Linear Discriminant Analysis 1 2 2  Mathematical formulation of the LDA and QDA classifiers 1 2 3  Mathematical formulation of LDA dimensionality reduction 1 2 4  Shrinkage 1 2 5  Estimation algorithms.
No extractions found.

1 3  Kernel ridge regression.
No extractions found.

1 4  Support Vector Machines 1 4 1  Classification 1 4 1 1  Multi_class classification 1 4 1 2  Scores and probabilities 1 4 1 3  Unbalanced problems 1 4 2  Regression 1 4 3  Density estimation, novelty detection 1 4 4  Complexity 1 4 5  Tips on Practical Use 1 4 6  Kernel functions 1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel 1 4 7  Mathematical formulation 1 4 7 1  SVC 1 4 7 2  NuSVC 1 4 7 3  SVR 1 4 8  Implementation details.
0.607: (Python functions; be Using as; kernels)
0.502: (Kernel functions; Using Python functions as; kernels)
0.499: (2 Regression 1 4 3 Density estimation; be novelty detection on; Practical Use)

1 5  Stochastic Gradient Descent 1 5 1  Classification 1 5 2  Regression 1 5 3  Stochastic Gradient Descent for sparse data 1 5 4  Complexity 1 5 5  Tips on Practical Use 1 5 6  Mathematical formulation 1 5 6 1  SGD 1 5 7  Implementation details.
No extractions found.

1 6  Nearest Neighbors 1 6 1  Unsupervised Nearest Neighbors 1 6 1 1  Finding the Nearest Neighbors 1 6 1 2  KDTree and BallTree Classes 1 6 2  Nearest Neighbors Classification 1 6 3  Nearest Neighbors Regression 1 6 4  Nearest Neighbor Algorithms 1 6 4 1  Brute Force 1 6 4 2  K_D Tree 1 6 4 3  Ball Tree 1 6 4 4  Choice of Nearest Neighbors Algorithm 1 6 4 5  Effect of leaf_size 1 6 5  Nearest Centroid Classifier 1 6 5 1  Nearest Shrunken Centroid 1 6 6  Approximate Nearest Neighbors 1 6 6 1  Locality Sensitive Hashing Forest 1 6 6 2  Mathematical description of Locality Sensitive Hashing.
No extractions found.

1 7  Gaussian Processes 1 7 1  Gaussian Process Regression 1 7 2  GPR examples 1 7 2 1  GPR with noise_level estimation 1 7 2 2  Comparison of GPR and Kernel Ridge Regression 1 7 2 3  GPR on Mauna Loa CO2 data 1 7 3  Gaussian Process Classification 1 7 4  GPC examples 1 7 4 1  Probabilistic predictions with GPC 1 7 4 2  Illustration of GPC on the XOR dataset 1 7 4 3  Gaussian process classification on iris dataset 1 7 5  Kernels for Gaussian Processes 1 7 5 1  Gaussian Process Kernel API 1 7 5 2  Basic kernels 1 7 5 3  Kernel operators 1 7 5 4  Radial_basis function kernel 1 7 5 5  Mat rn kernel 1 7 5 6  Rational quadratic kernel 1 7 5 7  Exp_Sine_Squared kernel 1 7 5 8  Dot_Product kernel 1 7 5 9  References 1 7 6  Legacy Gaussian Processes 1 7 6 1  An introductory regression example 1 7 6 2  Fitting Noisy Data 1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor 1 7 6 4  Correlation Models 1 7 6 5  Regression Models 1 7 6 6  Implementation details.
No extractions found.

1 8  Cross decomposition.
No extractions found.

1 9  Naive Bayes 1 9 1  Gaussian Naive Bayes 1 9 2  Multinomial Naive Bayes 1 9 3  Bernoulli Naive Bayes 1 9 4  Out_of_core naive Bayes model fitting.
0.366: (4 Out_of_core naive Bayes model; fitting; Bernoulli Naive Bayes)

1 10  Decision Trees 1 10 1  Classification 1 10 2  Regression 1 10 3  Multi_output problems 1 10 4  Complexity 1 10 5  Tips on practical use 1 10 6  Tree algorithms  ID3, C4 5, C5 0 and CART 1 10 7  Mathematical formulation 1 10 7 1  Classification criteria 1 10 7 2  Regression criteria.
No extractions found.

1 11  Ensemble methods 1 11 1  Bagging meta_estimator 1 11 2  Forests of randomized trees 1 11 2 1  Random Forests 1 11 2 2  Extremely Randomized Trees 1 11 2 3  Parameters 1 11 2 4  Parallelization 1 11 2 5  Feature importance evaluation 1 11 2 6  Totally Random Trees Embedding 1 11 3  AdaBoost 1 11 3 1  Usage 1 11 4  Gradient Tree Boosting 1 11 4 1  Classification 1 11 4 2  Regression 1 11 4 3  Fitting additional weak_learners 1 11 4 4  Controlling the tree size 1 11 4 5  Mathematical formulation 1 11 4 5 1  Loss Functions 1 11 4 6  Regularization 1 11 4 6 1  Shrinkage 1 11 4 6 2  Subsampling 1 11 4 7  Interpretation 1 11 4 7 1  Feature importance 1 11 4 7 2  Partial dependence 1 11 5  VotingClassifier 1 11 5 1  Majority Class Labels 1 11 5 1 1  Usage 1 11 5 2  Weighted Average Probabilities 1 11 5 3  Using the VotingClassifier with GridSearch 1 11 5 3 1  Usage.
0.72: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using the VotingClassifier with; GridSearch 1 11 5 3 1 Usage)
0.665: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using; the VotingClassifier)
0.661: (the VotingClassifier; be Using with; GridSearch 1 11 5 3 1 Usage)

1 12  Multiclass and multilabel algorithms 1 12 1  Multilabel classification format 1 12 2  One_Vs_The_Rest 1 12 2 1  Multiclass learning 1 12 2 2  Multilabel learning 1 12 3  One_Vs_One 1 12 3 1  Multiclass learning 1 12 4  Error_Correcting Output_Codes 1 12 4 1  Multiclass learning 1 12 5  Multioutput regression 1 12 6  Multioutput classification.
0.644: (algorithms 1 12 1 Multilabel classification; be multilabel format; 1 12 2 One_Vs_The_Rest)

1 13  Feature selection 1 13 1  Removing features with low variance 1 13 2  Univariate feature selection 1 13 3  Recursive feature elimination 1 13 4  Feature selection using SelectFromModel 1 13 4 1  L1_based feature selection 1 13 4 2  Randomized sparse models 1 13 4 3  Tree_based feature selection 1 13 5  Feature selection as part of a pipeline.
0.577: (SelectFromModel 1 13 4 1 L1_based feature selection; be using as; part of a pipeline)
0.333: (1 13 3 Recursive feature elimination 1 13 4 Feature selection; using SelectFromModel 1 13 4 1 L1 based feature selection as; part of a pipeline)

1 14  Semi_Supervised 1 14 1  Label Propagation.
No extractions found.

1 15  Isotonic regression.
No extractions found.

1 16  Probability calibration.
No extractions found.

1 17  Neural network models 1 17 1  Multi_layer Perceptron 1 17 2  Classification 1 17 3  Regression 1 17 4  Regularization 1 17 5  Algorithms 1 17 6  Complexity 1 17 7  Mathematical formulation 1 17 8  Tips on Practical Use 1 17 9  More control with warm_start.
No extractions found.

1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity.
No extractions found.

1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation.
0.59: (1 1 2 1 Ridge Complexity 1 1 2 2; Setting; the regularization parameter)

1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection.
0.575: (Lasso 1 1 3 1; Setting; regularization parameter 1 1 3 1 1)

1 1 4  Multi_task Lasso.
No extractions found.

1 1 5  Elastic Net.
No extractions found.

1 1 6  Multi_task Elastic Net.
No extractions found.

1 1 7  Least Angle Regression.
No extractions found.

1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation.
No extractions found.

1 1 9  Orthogonal Matching Pursuit .
No extractions found.

1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD.
No extractions found.

1 1 11  Logistic regression.
No extractions found.

1 1 12  Stochastic Gradient Descent _ SGD.
No extractions found.

1 1 13  Perceptron.
No extractions found.

1 1 14  Passive Aggressive Algorithms.
No extractions found.

1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes.
No extractions found.

1 1 16  Polynomial regression  extending linear models with basis functions.
0.741: (1 1 16 Polynomial regression; extending linear models with; basis functions)
0.674: (linear models; be extending with; basis functions)

1 1 1 1  Ordinary Least Squares Complexity.
No extractions found.

1 1 2 1  Ridge Complexity.
No extractions found.

1 1 2 2  Setting the regularization parameter  generalized Cross_Validation.
0.604: (1 1 2 2; Setting; the regularization parameter)

1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection.
No extractions found.

1 1 3 1 1  Using cross_validation.
No extractions found.

1 1 3 1 2  Information_criteria based model selection.
No extractions found.

1 1 8 1  Mathematical formulation.
No extractions found.

1 1 10 1  Bayesian Ridge Regression.
No extractions found.

1 1 10 2  Automatic Relevance Determination _ ARD.
No extractions found.

1 1 15 1  Different scenario and useful concepts.
No extractions found.

1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm.
No extractions found.

1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations.
No extractions found.

1 1 15 4  Huber Regression.
No extractions found.

1 1 15 5  Notes.
No extractions found.

1 1 15 2 1  Details of the algorithm.
No extractions found.

1 1 15 3 1  Theoretical considerations.
No extractions found.

1 2 1  Dimensionality reduction using Linear Discriminant Analysis.
No extractions found.

1 2 2  Mathematical formulation of the LDA and QDA classifiers.
No extractions found.

1 2 3  Mathematical formulation of LDA dimensionality reduction.
No extractions found.

1 2 4  Shrinkage.
No extractions found.

1 2 5  Estimation algorithms.
No extractions found.

1 4 1  Classification 1 4 1 1  Multi_class classification 1 4 1 2  Scores and probabilities 1 4 1 3  Unbalanced problems.
No extractions found.

1 4 2  Regression.
No extractions found.

1 4 3  Density estimation, novelty detection.
No extractions found.

1 4 4  Complexity.
No extractions found.

1 4 5  Tips on Practical Use.
No extractions found.

1 4 6  Kernel functions 1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel.
No extractions found.

1 4 7  Mathematical formulation 1 4 7 1  SVC 1 4 7 2  NuSVC 1 4 7 3  SVR.
No extractions found.

1 4 8  Implementation details.
No extractions found.

1 4 1 1  Multi_class classification.
No extractions found.

1 4 1 2  Scores and probabilities.
No extractions found.

1 4 1 3  Unbalanced problems.
No extractions found.

1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel.
0.681: (1 4 6 1 1; Using Python functions as; kernels)
0.607: (Python functions; be Using as; kernels)

1 4 6 1 1  Using Python functions as kernels.
0.741: (1 4 6 1 1; Using Python functions as; kernels)
0.661: (Python functions; be Using as; kernels)

1 4 6 1 2  Using the Gram matrix.
No extractions found.

1 4 6 1 3  Parameters of the RBF Kernel.
No extractions found.

1 4 7 1  SVC.
No extractions found.

1 4 7 2  NuSVC.
No extractions found.

1 4 7 3  SVR.
No extractions found.

1 5 1  Classification.
No extractions found.

1 5 2  Regression.
No extractions found.

1 5 3  Stochastic Gradient Descent for sparse data.
No extractions found.

1 5 4  Complexity.
No extractions found.

1 5 5  Tips on Practical Use.
No extractions found.

1 5 6  Mathematical formulation 1 5 6 1  SGD.
No extractions found.

1 5 7  Implementation details.
No extractions found.

1 5 6 1  SGD.
No extractions found.

1 6 1  Unsupervised Nearest Neighbors 1 6 1 1  Finding the Nearest Neighbors 1 6 1 2  KDTree and BallTree Classes.
No extractions found.

1 6 2  Nearest Neighbors Classification.
No extractions found.

1 6 3  Nearest Neighbors Regression.
No extractions found.

1 6 4  Nearest Neighbor Algorithms 1 6 4 1  Brute Force 1 6 4 2  K_D Tree 1 6 4 3  Ball Tree 1 6 4 4  Choice of Nearest Neighbors Algorithm 1 6 4 5  Effect of leaf_size.
No extractions found.

1 6 5  Nearest Centroid Classifier 1 6 5 1  Nearest Shrunken Centroid.
No extractions found.

1 6 6  Approximate Nearest Neighbors 1 6 6 1  Locality Sensitive Hashing Forest 1 6 6 2  Mathematical description of Locality Sensitive Hashing.
No extractions found.

1 6 1 1  Finding the Nearest Neighbors.
0.658: (1 6 1 1; Finding; the Nearest Neighbors)

1 6 1 2  KDTree and BallTree Classes.
No extractions found.

1 6 4 1  Brute Force.
No extractions found.

1 6 4 2  K_D Tree.
No extractions found.

1 6 4 3  Ball Tree.
No extractions found.

1 6 4 4  Choice of Nearest Neighbors Algorithm.
No extractions found.

1 6 4 5  Effect of leaf_size.
No extractions found.

1 6 5 1  Nearest Shrunken Centroid.
No extractions found.

1 6 6 1  Locality Sensitive Hashing Forest.
No extractions found.

1 6 6 2  Mathematical description of Locality Sensitive Hashing.
No extractions found.

1 7 1  Gaussian Process Regression .
No extractions found.

1 7 2  GPR examples 1 7 2 1  GPR with noise_level estimation 1 7 2 2  Comparison of GPR and Kernel Ridge Regression 1 7 2 3  GPR on Mauna Loa CO2 data.
No extractions found.

1 7 3  Gaussian Process Classification .
No extractions found.

1 7 4  GPC examples 1 7 4 1  Probabilistic predictions with GPC 1 7 4 2  Illustration of GPC on the XOR dataset 1 7 4 3  Gaussian process classification on iris dataset.
No extractions found.

1 7 5  Kernels for Gaussian Processes 1 7 5 1  Gaussian Process Kernel API 1 7 5 2  Basic kernels 1 7 5 3  Kernel operators 1 7 5 4  Radial_basis function kernel 1 7 5 5  Mat rn kernel 1 7 5 6  Rational quadratic kernel 1 7 5 7  Exp_Sine_Squared kernel 1 7 5 8  Dot_Product kernel 1 7 5 9  References.
No extractions found.

1 7 6  Legacy Gaussian Processes 1 7 6 1  An introductory regression example 1 7 6 2  Fitting Noisy Data 1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor 1 7 6 4  Correlation Models 1 7 6 5  Regression Models 1 7 6 6  Implementation details.
No extractions found.

1 7 2 1  GPR with noise_level estimation.
No extractions found.

1 7 2 2  Comparison of GPR and Kernel Ridge Regression.
No extractions found.

1 7 2 3  GPR on Mauna Loa CO2 data.
No extractions found.

1 7 4 1  Probabilistic predictions with GPC.
No extractions found.

1 7 4 2  Illustration of GPC on the XOR dataset.
No extractions found.

1 7 4 3  Gaussian process classification on iris dataset.
No extractions found.

1 7 5 1  Gaussian Process Kernel API.
No extractions found.

1 7 5 2  Basic kernels.
No extractions found.

1 7 5 3  Kernel operators.
No extractions found.

1 7 5 4  Radial_basis function kernel.
No extractions found.

1 7 5 5  Mat rn kernel.
No extractions found.

1 7 5 6  Rational quadratic kernel.
No extractions found.

1 7 5 7  Exp_Sine_Squared kernel.
No extractions found.

1 7 5 8  Dot_Product kernel.
No extractions found.

1 7 5 9  References.
No extractions found.

1 7 6 1  An introductory regression example.
No extractions found.

1 7 6 2  Fitting Noisy Data.
No extractions found.

1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor .
No extractions found.

1 7 6 4  Correlation Models.
No extractions found.

1 7 6 5  Regression Models.
No extractions found.

1 7 6 6  Implementation details.
No extractions found.

1 7 6 3 1  The initial assumption.
No extractions found.

1 7 6 3 2  The best linear unbiased prediction .
No extractions found.

1 7 6 3 3  The empirical best linear unbiased predictor .
No extractions found.

1 9 1  Gaussian Naive Bayes.
No extractions found.

1 9 2  Multinomial Naive Bayes.
No extractions found.

1 9 3  Bernoulli Naive Bayes.
No extractions found.

1 9 4  Out_of_core naive Bayes model fitting.
No extractions found.

1 10 1  Classification.
No extractions found.

1 10 2  Regression.
No extractions found.

1 10 3  Multi_output problems.
No extractions found.

1 10 4  Complexity.
No extractions found.

1 10 5  Tips on practical use.
No extractions found.

1 10 6  Tree algorithms  ID3, C4 5, C5 0 and CART.
No extractions found.

1 10 7  Mathematical formulation 1 10 7 1  Classification criteria 1 10 7 2  Regression criteria.
No extractions found.

1 10 7 1  Classification criteria.
No extractions found.

1 10 7 2  Regression criteria.
No extractions found.

1 11 1  Bagging meta_estimator.
No extractions found.

1 11 2  Forests of randomized trees 1 11 2 1  Random Forests 1 11 2 2  Extremely Randomized Trees 1 11 2 3  Parameters 1 11 2 4  Parallelization 1 11 2 5  Feature importance evaluation 1 11 2 6  Totally Random Trees Embedding.
No extractions found.

1 11 3  AdaBoost 1 11 3 1  Usage.
No extractions found.

1 11 4  Gradient Tree Boosting 1 11 4 1  Classification 1 11 4 2  Regression 1 11 4 3  Fitting additional weak_learners 1 11 4 4  Controlling the tree size 1 11 4 5  Mathematical formulation 1 11 4 5 1  Loss Functions 1 11 4 6  Regularization 1 11 4 6 1  Shrinkage 1 11 4 6 2  Subsampling 1 11 4 7  Interpretation 1 11 4 7 1  Feature importance 1 11 4 7 2  Partial dependence.
No extractions found.

1 11 5  VotingClassifier 1 11 5 1  Majority Class Labels 1 11 5 1 1  Usage 1 11 5 2  Weighted Average Probabilities 1 11 5 3  Using the VotingClassifier with GridSearch 1 11 5 3 1  Usage.
0.72: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using the VotingClassifier with; GridSearch 1 11 5 3 1 Usage)
0.665: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using; the VotingClassifier)
0.661: (the VotingClassifier; be Using with; GridSearch 1 11 5 3 1 Usage)

1 11 2 1  Random Forests.
No extractions found.

1 11 2 2  Extremely Randomized Trees.
No extractions found.

1 11 2 3  Parameters.
No extractions found.

1 11 2 4  Parallelization.
No extractions found.

1 11 2 5  Feature importance evaluation.
No extractions found.

1 11 2 6  Totally Random Trees Embedding.
No extractions found.

1 11 3 1  Usage.
No extractions found.

1 11 4 1  Classification.
No extractions found.

1 11 4 2  Regression.
No extractions found.

1 11 4 3  Fitting additional weak_learners.
No extractions found.

1 11 4 4  Controlling the tree size.
No extractions found.

1 11 4 5  Mathematical formulation 1 11 4 5 1  Loss Functions.
No extractions found.

1 11 4 6  Regularization 1 11 4 6 1  Shrinkage 1 11 4 6 2  Subsampling.
No extractions found.

1 11 4 7  Interpretation 1 11 4 7 1  Feature importance 1 11 4 7 2  Partial dependence.
No extractions found.

1 11 4 5 1  Loss Functions.
No extractions found.

1 11 4 6 1  Shrinkage.
No extractions found.

1 11 4 6 2  Subsampling.
No extractions found.

1 11 4 7 1  Feature importance.
No extractions found.

1 11 4 7 2  Partial dependence.
No extractions found.

1 11 5 1  Majority Class Labels 1 11 5 1 1  Usage.
No extractions found.

1 11 5 2  Weighted Average Probabilities .
No extractions found.

1 11 5 3  Using the VotingClassifier with GridSearch 1 11 5 3 1  Usage.
0.732: (1 11 5 3; Using the VotingClassifier with; GridSearch 1 11 5 3 1 Usage)
0.678: (1 11 5 3; Using; the VotingClassifier)
0.661: (the VotingClassifier; be Using with; GridSearch 1 11 5 3 1 Usage)

1 11 5 1 1  Usage.
No extractions found.

1 11 5 3 1  Usage.
No extractions found.

1 12 1  Multilabel classification format.
No extractions found.

1 12 2  One_Vs_The_Rest 1 12 2 1  Multiclass learning 1 12 2 2  Multilabel learning.
No extractions found.

1 12 3  One_Vs_One 1 12 3 1  Multiclass learning.
No extractions found.

1 12 4  Error_Correcting Output_Codes 1 12 4 1  Multiclass learning.
No extractions found.

1 12 5  Multioutput regression.
No extractions found.

1 12 6  Multioutput classification.
No extractions found.

1 12 2 1  Multiclass learning.
No extractions found.

1 12 2 2  Multilabel learning.
No extractions found.

1 12 3 1  Multiclass learning.
No extractions found.

1 12 4 1  Multiclass learning.
No extractions found.

1 13 1  Removing features with low variance.
No extractions found.

1 13 2  Univariate feature selection.
No extractions found.

1 13 3  Recursive feature elimination.
No extractions found.

1 13 4  Feature selection using SelectFromModel 1 13 4 1  L1_based feature selection 1 13 4 2  Randomized sparse models 1 13 4 3  Tree_based feature selection.
No extractions found.

1 13 5  Feature selection as part of a pipeline.
No extractions found.

1 13 4 1  L1_based feature selection.
No extractions found.

1 13 4 2  Randomized sparse models.
No extractions found.

1 13 4 3  Tree_based feature selection.
No extractions found.

1 14 1  Label Propagation.
No extractions found.

1 17 1  Multi_layer Perceptron.
No extractions found.

1 17 2  Classification.
No extractions found.

1 17 3  Regression.
No extractions found.

1 17 4  Regularization.
No extractions found.

1 17 5  Algorithms.
No extractions found.

1 17 6  Complexity.
No extractions found.

1 17 7  Mathematical formulation.
No extractions found.

1 17 8  Tips on Practical Use.
No extractions found.

1 17 9  More control with warm_start.
No extractions found.

1 1  Generalized Linear Models 1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity 1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation 1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection 1 1 4  Multi_task Lasso 1 1 5  Elastic Net 1 1 6  Multi_task Elastic Net 1 1 7  Least Angle Regression 1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation 1 1 9  Orthogonal Matching Pursuit 1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD 1 1 11  Logistic regression 1 1 12  Stochastic Gradient Descent _ SGD 1 1 13  Perceptron 1 1 14  Passive Aggressive Algorithms 1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes 1 1 16  Polynomial regression  extending linear models with basis functions.
0.729: (1 1 16 Polynomial regression; extending linear models with; basis functions)
0.681: (Cross_Validation 1 1 3 Lasso 1 1 3 1; Setting; regularization parameter 1 1 3 1 1 Using cross_validation 1 1 3 1 2 Information_criteria based model selection 1 1 4 Multi_task Lasso 1 1 5 Elastic Net 1 1 6 Multi_task Elastic Net 1 1 7 Least Angle Regression 1 1 8 LARS Lasso 1 1 8 1 Mathematical formulation 1 1 9 Orthogonal Matching Pursuit 1 1 10 Bayesian Regression 1 1 10 1 Bayesian Ridge Regression 1 1 10 2 Automatic Relevance Determination _ ARD 1 1 11 Logistic regression 1 1 12 Stochastic Gradient Descent _ SGD 1 1 13 Perceptron 1 1 14 Passive Aggressive Algorithms 1 1 15 Robustness regression outliers and modeling errors 1 1 15 1 Different scenario and useful concepts 1 1 15 2 RANSAC RANdom SAmple Consensus 1 1 15 2 1 Details of the algorithm 1 1 15 3 Theil_Sen estimator generalized_median_based estimator 1 1 15 3 1 Theoretical considerations 1 1 15 4 Huber Regression 1 1 15 5 Notes 1 1 16 Polynomial regression)
0.674: (linear models; be extending with; basis functions)
0.624: (1 1 2 1 Ridge Complexity 1 1 2 2; Setting; the regularization parameter generalized Cross_Validation 1 1 3 Lasso 1 1 3 1 Setting regularization parameter 1 1 3 1 1)

1 2  Linear and Quadratic Discriminant Analysis 1 2 1  Dimensionality reduction using Linear Discriminant Analysis 1 2 2  Mathematical formulation of the LDA and QDA classifiers 1 2 3  Mathematical formulation of LDA dimensionality reduction 1 2 4  Shrinkage 1 2 5  Estimation algorithms.
No extractions found.

1 3  Kernel ridge regression.
No extractions found.

1 4  Support Vector Machines 1 4 1  Classification 1 4 1 1  Multi_class classification 1 4 1 2  Scores and probabilities 1 4 1 3  Unbalanced problems 1 4 2  Regression 1 4 3  Density estimation, novelty detection 1 4 4  Complexity 1 4 5  Tips on Practical Use 1 4 6  Kernel functions 1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel 1 4 7  Mathematical formulation 1 4 7 1  SVC 1 4 7 2  NuSVC 1 4 7 3  SVR 1 4 8  Implementation details.
0.607: (Python functions; be Using as; kernels)
0.502: (Kernel functions; Using Python functions as; kernels)
0.499: (2 Regression 1 4 3 Density estimation; be novelty detection on; Practical Use)

1 5  Stochastic Gradient Descent 1 5 1  Classification 1 5 2  Regression 1 5 3  Stochastic Gradient Descent for sparse data 1 5 4  Complexity 1 5 5  Tips on Practical Use 1 5 6  Mathematical formulation 1 5 6 1  SGD 1 5 7  Implementation details.
No extractions found.

1 6  Nearest Neighbors 1 6 1  Unsupervised Nearest Neighbors 1 6 1 1  Finding the Nearest Neighbors 1 6 1 2  KDTree and BallTree Classes 1 6 2  Nearest Neighbors Classification 1 6 3  Nearest Neighbors Regression 1 6 4  Nearest Neighbor Algorithms 1 6 4 1  Brute Force 1 6 4 2  K_D Tree 1 6 4 3  Ball Tree 1 6 4 4  Choice of Nearest Neighbors Algorithm 1 6 4 5  Effect of leaf_size 1 6 5  Nearest Centroid Classifier 1 6 5 1  Nearest Shrunken Centroid 1 6 6  Approximate Nearest Neighbors 1 6 6 1  Locality Sensitive Hashing Forest 1 6 6 2  Mathematical description of Locality Sensitive Hashing.
No extractions found.

1 7  Gaussian Processes 1 7 1  Gaussian Process Regression 1 7 2  GPR examples 1 7 2 1  GPR with noise_level estimation 1 7 2 2  Comparison of GPR and Kernel Ridge Regression 1 7 2 3  GPR on Mauna Loa CO2 data 1 7 3  Gaussian Process Classification 1 7 4  GPC examples 1 7 4 1  Probabilistic predictions with GPC 1 7 4 2  Illustration of GPC on the XOR dataset 1 7 4 3  Gaussian process classification on iris dataset 1 7 5  Kernels for Gaussian Processes 1 7 5 1  Gaussian Process Kernel API 1 7 5 2  Basic kernels 1 7 5 3  Kernel operators 1 7 5 4  Radial_basis function kernel 1 7 5 5  Mat rn kernel 1 7 5 6  Rational quadratic kernel 1 7 5 7  Exp_Sine_Squared kernel 1 7 5 8  Dot_Product kernel 1 7 5 9  References 1 7 6  Legacy Gaussian Processes 1 7 6 1  An introductory regression example 1 7 6 2  Fitting Noisy Data 1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor 1 7 6 4  Correlation Models 1 7 6 5  Regression Models 1 7 6 6  Implementation details.
No extractions found.

1 8  Cross decomposition.
No extractions found.

1 9  Naive Bayes 1 9 1  Gaussian Naive Bayes 1 9 2  Multinomial Naive Bayes 1 9 3  Bernoulli Naive Bayes 1 9 4  Out_of_core naive Bayes model fitting.
0.366: (4 Out_of_core naive Bayes model; fitting; Bernoulli Naive Bayes)

1 10  Decision Trees 1 10 1  Classification 1 10 2  Regression 1 10 3  Multi_output problems 1 10 4  Complexity 1 10 5  Tips on practical use 1 10 6  Tree algorithms  ID3, C4 5, C5 0 and CART 1 10 7  Mathematical formulation 1 10 7 1  Classification criteria 1 10 7 2  Regression criteria.
No extractions found.

1 11  Ensemble methods 1 11 1  Bagging meta_estimator 1 11 2  Forests of randomized trees 1 11 2 1  Random Forests 1 11 2 2  Extremely Randomized Trees 1 11 2 3  Parameters 1 11 2 4  Parallelization 1 11 2 5  Feature importance evaluation 1 11 2 6  Totally Random Trees Embedding 1 11 3  AdaBoost 1 11 3 1  Usage 1 11 4  Gradient Tree Boosting 1 11 4 1  Classification 1 11 4 2  Regression 1 11 4 3  Fitting additional weak_learners 1 11 4 4  Controlling the tree size 1 11 4 5  Mathematical formulation 1 11 4 5 1  Loss Functions 1 11 4 6  Regularization 1 11 4 6 1  Shrinkage 1 11 4 6 2  Subsampling 1 11 4 7  Interpretation 1 11 4 7 1  Feature importance 1 11 4 7 2  Partial dependence 1 11 5  VotingClassifier 1 11 5 1  Majority Class Labels 1 11 5 1 1  Usage 1 11 5 2  Weighted Average Probabilities 1 11 5 3  Using the VotingClassifier with GridSearch 1 11 5 3 1  Usage.
0.72: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using the VotingClassifier with; GridSearch 1 11 5 3 1 Usage)
0.665: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using; the VotingClassifier)
0.661: (the VotingClassifier; be Using with; GridSearch 1 11 5 3 1 Usage)

1 12  Multiclass and multilabel algorithms 1 12 1  Multilabel classification format 1 12 2  One_Vs_The_Rest 1 12 2 1  Multiclass learning 1 12 2 2  Multilabel learning 1 12 3  One_Vs_One 1 12 3 1  Multiclass learning 1 12 4  Error_Correcting Output_Codes 1 12 4 1  Multiclass learning 1 12 5  Multioutput regression 1 12 6  Multioutput classification.
0.644: (algorithms 1 12 1 Multilabel classification; be multilabel format; 1 12 2 One_Vs_The_Rest)

1 13  Feature selection 1 13 1  Removing features with low variance 1 13 2  Univariate feature selection 1 13 3  Recursive feature elimination 1 13 4  Feature selection using SelectFromModel 1 13 4 1  L1_based feature selection 1 13 4 2  Randomized sparse models 1 13 4 3  Tree_based feature selection 1 13 5  Feature selection as part of a pipeline.
0.577: (SelectFromModel 1 13 4 1 L1_based feature selection; be using as; part of a pipeline)
0.333: (1 13 3 Recursive feature elimination 1 13 4 Feature selection; using SelectFromModel 1 13 4 1 L1 based feature selection as; part of a pipeline)

1 14  Semi_Supervised 1 14 1  Label Propagation.
No extractions found.

1 15  Isotonic regression.
No extractions found.

1 16  Probability calibration.
No extractions found.

1 17  Neural network models 1 17 1  Multi_layer Perceptron 1 17 2  Classification 1 17 3  Regression 1 17 4  Regularization 1 17 5  Algorithms 1 17 6  Complexity 1 17 7  Mathematical formulation 1 17 8  Tips on Practical Use 1 17 9  More control with warm_start.
No extractions found.

1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity.
No extractions found.

1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation.
0.59: (1 1 2 1 Ridge Complexity 1 1 2 2; Setting; the regularization parameter)

1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection.
0.575: (Lasso 1 1 3 1; Setting; regularization parameter 1 1 3 1 1)

1 1 4  Multi_task Lasso.
No extractions found.

1 1 5  Elastic Net.
No extractions found.

1 1 6  Multi_task Elastic Net.
No extractions found.

1 1 7  Least Angle Regression.
No extractions found.

1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation.
No extractions found.

1 1 9  Orthogonal Matching Pursuit .
No extractions found.

1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD.
No extractions found.

1 1 11  Logistic regression.
No extractions found.

1 1 12  Stochastic Gradient Descent _ SGD.
No extractions found.

1 1 13  Perceptron.
No extractions found.

1 1 14  Passive Aggressive Algorithms.
No extractions found.

1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes.
No extractions found.

1 1 16  Polynomial regression  extending linear models with basis functions.
0.741: (1 1 16 Polynomial regression; extending linear models with; basis functions)
0.674: (linear models; be extending with; basis functions)

1 1 15 1  Different scenario and useful concepts.
No extractions found.

1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm.
No extractions found.

1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations.
No extractions found.

1 1 15 4  Huber Regression.
No extractions found.

1 1 15 5  Notes.
No extractions found.

1 1 15 2 1  Details of the algorithm.
No extractions found.

1 1  Generalized Linear Models 1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity 1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation 1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection 1 1 4  Multi_task Lasso 1 1 5  Elastic Net 1 1 6  Multi_task Elastic Net 1 1 7  Least Angle Regression 1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation 1 1 9  Orthogonal Matching Pursuit 1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD 1 1 11  Logistic regression 1 1 12  Stochastic Gradient Descent _ SGD 1 1 13  Perceptron 1 1 14  Passive Aggressive Algorithms 1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes 1 1 16  Polynomial regression  extending linear models with basis functions.
0.729: (1 1 16 Polynomial regression; extending linear models with; basis functions)
0.681: (Cross_Validation 1 1 3 Lasso 1 1 3 1; Setting; regularization parameter 1 1 3 1 1 Using cross_validation 1 1 3 1 2 Information_criteria based model selection 1 1 4 Multi_task Lasso 1 1 5 Elastic Net 1 1 6 Multi_task Elastic Net 1 1 7 Least Angle Regression 1 1 8 LARS Lasso 1 1 8 1 Mathematical formulation 1 1 9 Orthogonal Matching Pursuit 1 1 10 Bayesian Regression 1 1 10 1 Bayesian Ridge Regression 1 1 10 2 Automatic Relevance Determination _ ARD 1 1 11 Logistic regression 1 1 12 Stochastic Gradient Descent _ SGD 1 1 13 Perceptron 1 1 14 Passive Aggressive Algorithms 1 1 15 Robustness regression outliers and modeling errors 1 1 15 1 Different scenario and useful concepts 1 1 15 2 RANSAC RANdom SAmple Consensus 1 1 15 2 1 Details of the algorithm 1 1 15 3 Theil_Sen estimator generalized_median_based estimator 1 1 15 3 1 Theoretical considerations 1 1 15 4 Huber Regression 1 1 15 5 Notes 1 1 16 Polynomial regression)
0.674: (linear models; be extending with; basis functions)
0.624: (1 1 2 1 Ridge Complexity 1 1 2 2; Setting; the regularization parameter generalized Cross_Validation 1 1 3 Lasso 1 1 3 1 Setting regularization parameter 1 1 3 1 1)

1 2  Linear and Quadratic Discriminant Analysis 1 2 1  Dimensionality reduction using Linear Discriminant Analysis 1 2 2  Mathematical formulation of the LDA and QDA classifiers 1 2 3  Mathematical formulation of LDA dimensionality reduction 1 2 4  Shrinkage 1 2 5  Estimation algorithms.
No extractions found.

1 3  Kernel ridge regression.
No extractions found.

1 4  Support Vector Machines 1 4 1  Classification 1 4 1 1  Multi_class classification 1 4 1 2  Scores and probabilities 1 4 1 3  Unbalanced problems 1 4 2  Regression 1 4 3  Density estimation, novelty detection 1 4 4  Complexity 1 4 5  Tips on Practical Use 1 4 6  Kernel functions 1 4 6 1  Custom Kernels 1 4 6 1 1  Using Python functions as kernels 1 4 6 1 2  Using the Gram matrix 1 4 6 1 3  Parameters of the RBF Kernel 1 4 7  Mathematical formulation 1 4 7 1  SVC 1 4 7 2  NuSVC 1 4 7 3  SVR 1 4 8  Implementation details.
0.607: (Python functions; be Using as; kernels)
0.502: (Kernel functions; Using Python functions as; kernels)
0.499: (2 Regression 1 4 3 Density estimation; be novelty detection on; Practical Use)

1 5  Stochastic Gradient Descent 1 5 1  Classification 1 5 2  Regression 1 5 3  Stochastic Gradient Descent for sparse data 1 5 4  Complexity 1 5 5  Tips on Practical Use 1 5 6  Mathematical formulation 1 5 6 1  SGD 1 5 7  Implementation details.
No extractions found.

1 6  Nearest Neighbors 1 6 1  Unsupervised Nearest Neighbors 1 6 1 1  Finding the Nearest Neighbors 1 6 1 2  KDTree and BallTree Classes 1 6 2  Nearest Neighbors Classification 1 6 3  Nearest Neighbors Regression 1 6 4  Nearest Neighbor Algorithms 1 6 4 1  Brute Force 1 6 4 2  K_D Tree 1 6 4 3  Ball Tree 1 6 4 4  Choice of Nearest Neighbors Algorithm 1 6 4 5  Effect of leaf_size 1 6 5  Nearest Centroid Classifier 1 6 5 1  Nearest Shrunken Centroid 1 6 6  Approximate Nearest Neighbors 1 6 6 1  Locality Sensitive Hashing Forest 1 6 6 2  Mathematical description of Locality Sensitive Hashing.
No extractions found.

1 7  Gaussian Processes 1 7 1  Gaussian Process Regression 1 7 2  GPR examples 1 7 2 1  GPR with noise_level estimation 1 7 2 2  Comparison of GPR and Kernel Ridge Regression 1 7 2 3  GPR on Mauna Loa CO2 data 1 7 3  Gaussian Process Classification 1 7 4  GPC examples 1 7 4 1  Probabilistic predictions with GPC 1 7 4 2  Illustration of GPC on the XOR dataset 1 7 4 3  Gaussian process classification on iris dataset 1 7 5  Kernels for Gaussian Processes 1 7 5 1  Gaussian Process Kernel API 1 7 5 2  Basic kernels 1 7 5 3  Kernel operators 1 7 5 4  Radial_basis function kernel 1 7 5 5  Mat rn kernel 1 7 5 6  Rational quadratic kernel 1 7 5 7  Exp_Sine_Squared kernel 1 7 5 8  Dot_Product kernel 1 7 5 9  References 1 7 6  Legacy Gaussian Processes 1 7 6 1  An introductory regression example 1 7 6 2  Fitting Noisy Data 1 7 6 3  Mathematical formulation 1 7 6 3 1  The initial assumption 1 7 6 3 2  The best linear unbiased prediction 1 7 6 3 3  The empirical best linear unbiased predictor 1 7 6 4  Correlation Models 1 7 6 5  Regression Models 1 7 6 6  Implementation details.
No extractions found.

1 8  Cross decomposition.
No extractions found.

1 9  Naive Bayes 1 9 1  Gaussian Naive Bayes 1 9 2  Multinomial Naive Bayes 1 9 3  Bernoulli Naive Bayes 1 9 4  Out_of_core naive Bayes model fitting.
0.366: (4 Out_of_core naive Bayes model; fitting; Bernoulli Naive Bayes)

1 10  Decision Trees 1 10 1  Classification 1 10 2  Regression 1 10 3  Multi_output problems 1 10 4  Complexity 1 10 5  Tips on practical use 1 10 6  Tree algorithms  ID3, C4 5, C5 0 and CART 1 10 7  Mathematical formulation 1 10 7 1  Classification criteria 1 10 7 2  Regression criteria.
No extractions found.

1 11  Ensemble methods 1 11 1  Bagging meta_estimator 1 11 2  Forests of randomized trees 1 11 2 1  Random Forests 1 11 2 2  Extremely Randomized Trees 1 11 2 3  Parameters 1 11 2 4  Parallelization 1 11 2 5  Feature importance evaluation 1 11 2 6  Totally Random Trees Embedding 1 11 3  AdaBoost 1 11 3 1  Usage 1 11 4  Gradient Tree Boosting 1 11 4 1  Classification 1 11 4 2  Regression 1 11 4 3  Fitting additional weak_learners 1 11 4 4  Controlling the tree size 1 11 4 5  Mathematical formulation 1 11 4 5 1  Loss Functions 1 11 4 6  Regularization 1 11 4 6 1  Shrinkage 1 11 4 6 2  Subsampling 1 11 4 7  Interpretation 1 11 4 7 1  Feature importance 1 11 4 7 2  Partial dependence 1 11 5  VotingClassifier 1 11 5 1  Majority Class Labels 1 11 5 1 1  Usage 1 11 5 2  Weighted Average Probabilities 1 11 5 3  Using the VotingClassifier with GridSearch 1 11 5 3 1  Usage.
0.72: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using the VotingClassifier with; GridSearch 1 11 5 3 1 Usage)
0.665: (1 11 5 2 Weighted Average Probabilities 1 11 5 3; Using; the VotingClassifier)
0.661: (the VotingClassifier; be Using with; GridSearch 1 11 5 3 1 Usage)

1 12  Multiclass and multilabel algorithms 1 12 1  Multilabel classification format 1 12 2  One_Vs_The_Rest 1 12 2 1  Multiclass learning 1 12 2 2  Multilabel learning 1 12 3  One_Vs_One 1 12 3 1  Multiclass learning 1 12 4  Error_Correcting Output_Codes 1 12 4 1  Multiclass learning 1 12 5  Multioutput regression 1 12 6  Multioutput classification.
0.644: (algorithms 1 12 1 Multilabel classification; be multilabel format; 1 12 2 One_Vs_The_Rest)

1 13  Feature selection 1 13 1  Removing features with low variance 1 13 2  Univariate feature selection 1 13 3  Recursive feature elimination 1 13 4  Feature selection using SelectFromModel 1 13 4 1  L1_based feature selection 1 13 4 2  Randomized sparse models 1 13 4 3  Tree_based feature selection 1 13 5  Feature selection as part of a pipeline.
0.577: (SelectFromModel 1 13 4 1 L1_based feature selection; be using as; part of a pipeline)
0.333: (1 13 3 Recursive feature elimination 1 13 4 Feature selection; using SelectFromModel 1 13 4 1 L1 based feature selection as; part of a pipeline)

1 14  Semi_Supervised 1 14 1  Label Propagation.
No extractions found.

1 15  Isotonic regression.
No extractions found.

1 16  Probability calibration.
No extractions found.

1 17  Neural network models 1 17 1  Multi_layer Perceptron 1 17 2  Classification 1 17 3  Regression 1 17 4  Regularization 1 17 5  Algorithms 1 17 6  Complexity 1 17 7  Mathematical formulation 1 17 8  Tips on Practical Use 1 17 9  More control with warm_start.
No extractions found.

Machine learning algorithms can be divided into 3 broad categories   supervised learning, unsupervised learning, and reinforcement learning.
0.9: (Machine learning algorithms; can be divided into; 3 broad categories)

Supervised learning is useful in cases where a property is available for a certain dataset , but is missing and needs to be predicted for other instances.
0.882: (Supervised learning; is useful in; cases where a property is available for a certain dataset)
0.851: (needs; to be predicted for; other instances)
0.824: (a property; is available for; a certain dataset)
0.778: (a property; is; available)
0.777: (Supervised learning; is; useful)

 Unsupervised learning is useful in cases where the challenge is to discover implicit relationships in a given unlabeled dataset .
0.888: (Unsupervised learning; is useful in; cases where the challenge is to discover implicit relationships in a given unlabeled dataset)
0.798: (the challenge; is to discover; implicit relationships)
0.746: (Unsupervised learning; is; useful)

 Reinforcement learning falls between these 2 extremes   there is some form of feedback available for each predictive step or action, but no precise label or error message.
No extractions found.

 Since this is an intro class, I didn t learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
0.64: (t; learn about; reinforcement learning)

