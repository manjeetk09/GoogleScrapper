MappedWiki;algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 6  Nearest Neighbors 1 6 1  Unsupervised Nearest Neighbors 1 6 1 1  Finding the Nearest Neighbors 1 6 1 2  KDTree and BallTree Classes 1 6 2  Nearest Neighbors Classification 1 6 3  Nearest Neighbors Regression 1 6 4  Nearest Neighbor Algorithms 1 6 4 1  Brute Force 1 6 4 2  K_D Tree 1 6 4 3  Ball Tree 1 6 4 4  Choice of Nearest Neighbors Algorithm 1 6 4 5  Effect of leaf_size 1 6 5  Nearest Centroid Classifier 1 6 5 1  Nearest Shrunken Centroid 1 6 6  Approximate Nearest Neighbors 1 6 6 1  Locality Sensitive Hashing Forest 1 6 6 2  Mathematical description of Locality Sensitive Hashing.;Locality-sensitive hashing;locality-sensitive_hashing;1.0;0.032679738562091505;1.0;2.0326797385620914;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/; Natural Language Processing .;Natural language processing;natural_language_processing;1.0;0.029411764705882353;1.0;2.0294117647058822;0.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/;Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras July 21, 2016.;Time series;time_series_prediction;1.0;0.029411764705882353;1.0;2.0294117647058822;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://www.sciencedirect.com/science/article/pii/S1568494612002128;Interactive genetic algorithms are effective methods of solving optimization problems with implicit criteria by incorporating a user s intelligent evaluation into traditional evolution mechanisms.;Interactive evolutionary computation;interactive_genetic_algorithms;1.0;0.015151515151515152;1.0;2.015151515151515;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning;  Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data   such algorithms overcome following strictly static program instructions by making data_driven predictions or decisions,.;Computational learning theory;computational_learning_theory;1.0;0.009174311926605505;1.0;2.0091743119266052;0.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Bernoulli distribution;bernoulli_random_variable;1.0;0.0018867924528301887;1.0;2.0018867924528303;1.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Supervised learning is a type of machine learning algorithm that uses a known dataset to make predictions.;Outline of machine learning;machine_learning_algorithms;0.9393939393939394;0.05263157894736842;1.0;1.9920255183413078;1.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Maximum likelihood estimation;maximum_likelihood_estimate;0.8787878787878788;0.0018867924528301887;1.0;1.880674671240709;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Discriminant analysis.;Linear discriminant analysis;discriminant_analysis;0.6363636363636364;0.2;1.0;1.8363636363636364;0.0;1;Wed Jun 28 04:14:40 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Normal distribution;normal_distribution;0.8181818181818182;0.0018867924528301887;1.0;1.8200686106346484;1.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html;Machine learning algorithms can be divided into 3 broad categories   supervised learning, unsupervised learning, and reinforcement learning.;Unsupervised learning;unsupervised_learning;0.6969696969690908;0.09090909090909091;1.0;1.7878787878781819;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html;Machine learning algorithms can be divided into 3 broad categories   supervised learning, unsupervised learning, and reinforcement learning.;Reinforcement learning;reinforcement_learning;0.6969696969690908;0.06060606060606061;1.0;1.7575757575751514;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;Machine learning;statistical_learning;0.7272727272727273;0.0018867924528301887;1.0;1.7291595197255574;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://link.springer.com/chapter/10.1007/978-94-009-0643-3_86; The difficulty of obtaining a nearly globally optimal set of weights in a reasonable time is overcome by using a global optimization method.;Reasonable time;reasonable_time;0.6363636363636364;0.05263157894736842;1.0;1.6889952153110048;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://link.springer.com/chapter/10.1007/978-94-009-0643-3_86; It maintains a set of points at every iteration and permits a parallel search of the global minimum.;Maxima and minima;global_minimum;0.6363636363636364;0.05263157894736842;1.0;1.6889952153110048;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;https://stackoverflow.com/questions/20752232/is-a-genetic-algorithm-a-form-of-unsupervised-learning;Unsupervised Genetic Algorithm Deployed for Intrusion Detection, .;Intrusion detection system;intrusion_detection;0.6363636363636364;0.05;1.0;1.6863636363636365;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 1  Generalized Linear Models 1 1 1  Ordinary Least Squares 1 1 1 1  Ordinary Least Squares Complexity 1 1 2  Ridge Regression 1 1 2 1  Ridge Complexity 1 1 2 2  Setting the regularization parameter  generalized Cross_Validation 1 1 3  Lasso 1 1 3 1  Setting regularization parameter 1 1 3 1 1  Using cross_validation 1 1 3 1 2  Information_criteria based model selection 1 1 4  Multi_task Lasso 1 1 5  Elastic Net 1 1 6  Multi_task Elastic Net 1 1 7  Least Angle Regression 1 1 8  LARS Lasso 1 1 8 1  Mathematical formulation 1 1 9  Orthogonal Matching Pursuit 1 1 10  Bayesian Regression 1 1 10 1  Bayesian Ridge Regression 1 1 10 2  Automatic Relevance Determination _ ARD 1 1 11  Logistic regression 1 1 12  Stochastic Gradient Descent _ SGD 1 1 13  Perceptron 1 1 14  Passive Aggressive Algorithms 1 1 15  Robustness regression  outliers and modeling errors 1 1 15 1  Different scenario and useful concepts 1 1 15 2  RANSAC  RANdom SAmple Consensus 1 1 15 2 1  Details of the algorithm 1 1 15 3  Theil_Sen estimator  generalized_median_based estimator 1 1 15 3 1  Theoretical considerations 1 1 15 4  Huber Regression 1 1 15 5  Notes 1 1 16  Polynomial regression  extending linear models with basis functions.;Basis function;basis_functions;0.6363636363636364;0.032679738562091505;1.0;1.6690433749257279;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;algorithms for supervised learning;http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html; Reinforcement learning falls between these 2 extremes   there is some form of feedback available for each predictive step or action, but no precise label or error message.;Error message;error_messages;0.6363636363636364;0.030303030303030304;1.0;1.6666666666666665;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/;Develop Your First Neural Network in Python With Keras Step_By_Step May 24, 2016.;Artificial neural network;neural_network;0.6363636363636364;0.029411764705882353;1.0;1.6657754010695187;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/; Graphical Models.;Graphical model;graphical_model;0.6363636363636364;0.029411764705882353;1.0;1.6657754010695187;0.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/; Recommender Systems.;Recommender system;recommender_systems;0.6363636363636364;0.029411764705882353;1.0;1.6657754010695187;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://link.springer.com/article/10.1007/BF00993043; This allows for an easy utilization of inference rules of the well_known inductive learning methodology, which replace the traditional domain_independent operators and make the search task_specific.;List of rules of inference;inference_rules;0.6363636363636364;0.02564102564102564;1.0;1.662004662004662;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://www.sciencedirect.com/science/article/pii/S1568494612002128;Interactive genetic algorithms are effective methods of solving optimization problems with implicit criteria by incorporating a user s intelligent evaluation into traditional evolution mechanisms.;Effective method;effective_method;0.6363636363636364;0.015151515151515152;1.0;1.6515151515151514;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://www.sciencedirect.com/science/article/pii/S1568494612002128; Incorporated with the principles of the improved semi_supervised learning, the opportunities of applying and updating the surrogate model are determined by its confidence degree in estimation, and the informative individuals reevaluated by the user are selected according to the concept of learning from mistakes.;Surrogate model;surrogate_model_;0.6363636363636364;0.015151515151515152;1.0;1.6515151515151514;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;http://www.sciencedirect.com/science/article/pii/S1568494612002128; In this algorithm, a population with many individuals is adopted to efficiently explore the search space.;optimization;search_space;0.6363636363636364;0.015151515151515152;1.0;1.6515151515151514;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning;Machine learning is the subfield of computer science that, according to Arthur Samuel in 1959, gives  computers the ability to learn without being explicitly programmed.;Computer science;computer_science;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning;  Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data   such algorithms overcome following strictly static program instructions by making data_driven predictions or decisions,.;Artificial intelligence;artificial_intelligence;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning;A genetic algorithm is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem.;Natural selection;natural_selection;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning; Vice versa, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.;Evolutionary algorithm;evolutionary_algorithm;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;1.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning; Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible  example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition , learning to rank, and computer vision.;Data breach;data_breach;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning; The program is provided feedback in terms of rewards and punishments as it navigates its problem space.;Problem domain;problem_space;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;genetic algorithm for supervised learning;https://en.wikipedia.org/wiki/Machine_learning; Vice versa, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.;List of Latin phrases (V);vice_versa;0.6363636363636364;0.009174311926605505;1.0;1.645537948290242;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;User guide.;User guide;user_guide;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 4 3  Density estimation, novelty detection.;Novelty detection;novelty_detection;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;http://scikit-learn.org/stable/supervised_learning.html;1 4 6 1 2  Using the Gram matrix.;Gramian matrix;gram_matrix;0.6363636363636364;0.006535947712418301;1.0;1.6428995840760545;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Col  Dan Sulzinger says  March 1, 2016 at 1 21 am Good Article .;Good article;good_article;0.6363636363636364;0.005660377358490566;1.0;1.642024013722127;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;Frame (networking);data_frame;0.6363636363636364;0.0037735849056603774;1.0;1.6401372212692968;0.7071067811865475;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Step function;step_function;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;1.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Benjamin says  December 5, 2015 at 7 00 pm This is a great resource overall and surely the product of a lot of work  Just a note as I go through this, your comment on Logistic Regression not actually being regression is in fact wrong  It maps outputs to a continuous variable bound between 0 and 1 that we regard as probability  it makes classification easy but that is still an extra step that requires the choice of a threshold which is not the main aim of Logistic Regression  As a matter of fact it falls under the umbrella of Generalized Libear Models as the glm R package hints it in your code example  I thought this was interesting to note so as not to forget that logistic regression output is richer than 0 or 1  Thanks for the great article overall .;Marine regression;regression_;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dung Dinh says  June 17, 2016 at 10 24 am The amazing article  I m new in data analysis  It s very useful and easy to understand  Thanks,.;Data analysis;data_analysis;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;sanjiv says  September 8, 2016 at 4 29 am Great article  It would have become even better if you had some test data with each code snippet  Add metrics and hyper parameter tunning for each of these models.;Text corpus;text_data;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Estimation;estimate;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;SÃ¶lve;solve;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Statis says  August 19, 2015 at 12 14 am Nice summary   Huzefa  you shouldn t replace the     in the R code, it basically means  as a function of   You can also keep the     right after, it stands for  all other variables in the dataset provided   If you want to be explicit, you can write y   x1   x2     where x1, x2    are the names of the columns of your data frame or data table  Further note on formula specification  by default R adds an intercept, so that y   x is equivalent to y   1   x, you can remove it via y   0   x  Interactions are specified with either   or     y   x1   x2 is equivalent to y   x1   x2   x1   x2  Hope this helps .;Table (information);data_table;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.7071067811865475;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;Danger Zone (film);danger_zone;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;J says  March 10, 2016 at 8 54 pm Somewhat irresponsible article since it does not mention any measure of performance and only gives cooking recipes without understanding what algorithm does what and the stats behind it  Cooking recipes like these are the ones that place people in Drew Conway s danger zone , thus making programmers the worst data analysts   I highly recommend anyone wishing to enter into this brave new world not to jump into statistical learning without proper statistical background  Otherwise you could end up like Google, Target, Telefonica, or Google and become a poster boy for  The Big Flops of Big Data   George says  June 17, 2016 at 1 34 pm Do you have a better article Please share .;Poster child;poster_boy;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Satya Swarup Dani says  September 27, 2016 at 10 41 am Nicely complied  Every explanation is crystal clear and very easy to digest  Thanks for sharing knowledge .;Knowledge sharing;sharing_knowledge;0.6363636363636364;0.0018867924528301887;1.0;1.6382504288164665;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://www.sciencedirect.com/science/article/pii/S1568494612002128; Incorporated with the principles of the improved semi_supervised learning, the opportunities of applying and updating the surrogate model are determined by its confidence degree in estimation, and the informative individuals reevaluated by the user are selected according to the concept of learning from mistakes.;Learning;learning;0.6363636363636364;0.0;1.0;1.6363636363636362;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Delilah;dalila;0.6363636363636364;0.0;1.0;1.6363636363636362;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Correlation and dependence;linear_relationship;0.60606060606;0.0018867924528301887;1.0;1.60794739851283;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Parametric model;parametric_model;0.5757575757581818;0.0018867924528301887;1.0;1.577644368211012;1.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Linearity;linear_;0.5757575757563637;0.0;1.0;1.5757575757563638;1.0;0;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;ramesh says  October 23, 2016 at 12 35 pm Hi Friends, i m new person to these machine learning algorithms  i have some questions   1  we have so many ML algorithms  but how can we choose the algorithms which one is suitable for my data set  2  How does these algorithms works   3  why only these particular algorithms   why not others  .;Algorithm;algorithms;0.5636363636363636;0.0037735849056603774;1.0;1.567409948542024;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;algorithms for supervised learning;https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/;Dalila says  August 14, 2015 at 1 35 pm Very good summary  Thank  One simple point  The reason for taking the log  in Logistic Regression is to make the equation linear, I e , easy to solve  Sunil Ray says  August 21, 2015 at 5 21 am Thanks Dalila    Borun Chowdhury says  April 21, 2016 at 8 48 am That s not the reason for taking the log  The underlying assumption in logistic regression is that the probability is governed by a step function whose argument is linear in the attributes  First of all the assumption of linearity or otherwise introduces bias  However, logistic regression being a parametric model some bias is inevitable  The reason to choose a linear relationship is not because its easy to solve but because a higher order polynomial introduces higher bias and one would not like to do so without good reason  Now coming to the choice of log, it is just a convention  Basically, once we have decided to go with a linear model, in the case of one attribute we model the probability by p  fsuch that p 0 and p 0  It so happens that this is satisfied by p  exp    which can be re_written as log     a x  b While I am at it, it may be useful to talk about another point  One should ask is why we don t use least square method  The reason is that a yes no choice is a Bernoulli random variable and thus we estimate the probability according to maximum likelihood wrt Bernoulli process  For linear regression the assumption is that the residuals around the  true  function are distributed according to a normal distribution and the maximum likelihood estimate for a normal distribution amounts to the least square method  So deep down linear regression and logistic regression both use maximum likelihood estimates  Its just that they are max likelihoods according to different distributions .;Linear model;linear_model;0.5454545454545454;0.0018867924528301887;1.0;1.5473413379073757;1.0;0;Wed Jun 28 04:14:40 IST 2017
UnmappedWiki;quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html; What Quantum Computing Means to Data Mining explains the most relevant concepts of machine learning, quantum mechanics, and quantum information theory, and contrasts classical learning algorithms to their quantum counterparts.;Quantum information;quantum_information_theory;0.9393939393939394;0.06666666666666667;0.3243243243243243;1.3303849303849304;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Decision trees.;Decision tree;decision_tree;0.6363636363636364;0.2;0.45945945945945943;1.2958230958230958;0.0;1;Wed Jun 28 04:14:38 IST 2017
MappedWiki;algorithms for supervised learning;https://www.mathworks.com/discovery/supervised-learning.html;Nearest neighbors .;Nearest neighbor;nearest_neighbor_;0.6363636363636364;0.2;0.45945945945945943;1.2958230958230958;0.0;1;Wed Jun 28 04:14:38 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://suanfazu.com/t/is-a-genetic-algorithm-a-form-of-unsupervised-learning/2882;Genetic Algorithms can be used for both supervised and unsupervised learning, e.;Genetic algorithm;genetic_algorithms;0.6363636363636364;0.3333333333333333;0.27927927927927926;1.248976248976249;1.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html;Quantum Machine Learning.;Quantum machine learning;quantum_machine_learning;1.0;0.06666666666666667;0.12162162162162161;1.1882882882882884;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;genetic algorithm for supervised learning;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/; Computer Vision .;Computer vision;computer_vision;0.6363636363636364;0.029411764705882353;0.45945945945945943;1.1252348605289781;0.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;quantum algorithms for supervised and unsupervised machine learning;https://en.wikipedia.org/wiki/Quantum_machine_learning;Quantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning.;Quantum mechanics;quantum_physics;0.7272727272727273;0.038461538461538464;0.3243243243243243;1.09005859005859;0.0;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;quantum algorithms for supervised and unsupervised machine learning;https://en.wikipedia.org/wiki/Quantum_machine_learning; Quantum machine learning algorithms can use the advantages of quantum computation in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a quantum computer.;Quantum computing;quantum_computation;0.6363636363636364;0.038461538461538464;0.3243243243243243;0.9991494991494991;0.7071067811865475;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;quantum algorithms for supervised and unsupervised machine learning;http://machinelearningmastery.com/machine-learning-with-quantum-computers/;I ve had quantum computing on my mind and another tech talk went by titled Quantum Machine Learning and I had to jump on it.;Technobabble;tech_talk;0.6363636363636364;0.02;0.3243243243243243;0.9806879606879606;0.7071067811865475;0;Wed Jun 28 04:14:40 IST 2017
MappedWiki;quantum algorithms for supervised and unsupervised machine learning;http://peterwittek.com/book.html; Rebentrost,  Quantum algorithms for supervised and unsupervised machine learning,  arXiv.;Quantum algorithm;quantum_algorithms;0.6363636363636364;0.06666666666666667;0.12162162162162161;0.8246519246519246;0.0;0;Wed Jun 28 04:14:38 IST 2017
UnmappedWiki;spectral algorithms for supervised learning;http://www.cs.columbia.edu/~scohen/naacl13tutorial/; CCA is an early method from statistics for dimensionality reduction.;Dimensionality reduction;dimensionality_reduction;0.6363636363636364;0.034482758620689655;0.05405405405405406;0.72490044903838;1.0;0;Wed Jun 28 04:14:40 IST 2017
