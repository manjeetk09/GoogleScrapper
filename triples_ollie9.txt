Read this introductory list of contemporary machine learning algorithms of importance that every engineer should understand.
No extractions found.

By James Le  New Story Charity.
No extractions found.

.
No extractions found.

It is no doubt that the sub field of machine learning   artificial intelligence has increasingly gained more popularity in the past couple of years.
0.802: (the sub field of machine learning artificial intelligence; has increasingly gained more popularity in; the past couple of years)
0.682: (the sub field of machine learning artificial intelligence; has increasingly gained; more popularity)
0.646: (It; is; no doubt)

 As Big Data is the hottest trend in the tech industry at the moment  machine learning is incredibly powerful to make predictions or calculated suggestions based on large amounts of data.
0.842: (predictions or calculated suggestions; be based on; large amounts of data)
0.778: (the hottest trend; is incredibly; powerful)
0.443: (powerful; be the hottest trend in; the tech industry)
0.351: (powerful; be the hottest trend at; the moment machine learning)

 Some of the most common examples of machine learning are Netflix s algorithms to make movie suggestions based on movies you have watched in the past or Amazon s algorithms that recommend books based on books you have bought before.
0.837: (you; have watched in; the past)
0.79: (Netflix; s; algorithms to make movie suggestions)
0.789: (movie suggestions; be based on; movies you have watched in the past or Amazon s algorithms)
0.734: (books; be based on; books you have bought before)
0.73: (Amazon; s; algorithms that recommend books)

So if you want to learn more about machine learning  how do you start  For me  my first introduction is when I took an Artificial Intelligence class when I was studying abroad in Copenhagen.
0.672: (I; took; an Artificial Intelligence class)[enabler=when I was studying abroad in Copenhagen]
0.609: (I; was studying abroad in; Copenhagen)

 My lecturer is a full time Applied Math and CS professor at the Technical University of Denmark  in which his research areas are logic and artificial  focusing primarily on the use of logic to model human like planning  reasoning and problem solving.
0.752: (CS; be professor at; the Technical University of Denmark)
0.736: (My lecturer; is; a full time)

 The class was a mix of discussion of theory core concepts and hands on problem solving.
0.936: (The class; was a mix of; discussion of theory core concepts)
0.87: (The class; was a mix of discussion of theory core concepts on; problem)
0.738: (The class; was; a mix of discussion of theory core concepts)

 The textbook that we used is one of the AI classics  Peter Norvig s Artificial Intelligence   A Modern Approach  in which we covered major topics including intelligent agents  problem solving by searching  adversarial search  probability theory  multi agent systems  social AI  philosophy ethics future of AI.
0.795: (The textbook that we used; is; one of the AI classics)
0.7: (Peter Norvig; s; Artificial Intelligence)
0.672: (we; covered; major topics)
0.564: (Artificial Intelligence; be s by; the AI classics)

 At the end of the class  in a team of 3  we implemented simple search based agents solving transportation tasks in a virtual environment as a programming project.
0.746: (based agents; solving transportation tasks in; a virtual environment)
0.707: (transportation tasks; be solving in; a virtual environment)
0.674: (transportation tasks; be solving as; a programming project)
0.663: (based agents; solving transportation tasks as; a programming project)
0.607: (we; implemented; simple search)

I have learned a tremendous amount of knowledge thanks to that class  and decided to keep learning about this specialized topic.
0.696: (I; have learned; a tremendous amount of knowledge)

 In the last few weeks  I have been multiple tech talks in San Francisco on deep learning  neural networks  data architecture   and a Machine Learning conference with a lot of well known professionals in the field.
0.872: (I; have been multiple tech talks in; San Francisco)
0.797: (I; have been multiple tech talks on; deep learning neural networks data architecture)
0.702: (I; have been multiple tech talks in; the last few weeks)
0.505: (multiple; have been tech talks in; San Francisco)
0.446: (multiple; have been tech talks on; deep learning neural networks data architecture)
0.273: (multiple; have been tech talks in; the last few weeks)

 Most importantly  I enrolled inUdacity s Intro to Machine Learning online course in the beginning of June and has just finished it a few days ago.
0.828: (inUdacity; s Intro has online; course)
0.82: (inUdacity; s Intro has in; the beginning of June)
0.815: (inUdacity; s Intro has to; Machine Learning)
0.518: (inUdacity; s has; Intro)

 In this post  I want to share some of the most common machine learning algorithms that I learned from the course.
0.638: (I; that learned from; the course)
0.45: (I; want in; this post)

Machine learning algorithms can be divided into 3 broad categories   supervised learning  unsupervised learning  and reinforcement learning.
0.914: (Machine learning algorithms; can be divided into; supervised learning unsupervised learning and reinforcement learning)

Supervised learning is useful in cases where a property  label  is available for a certain dataset  training set   but is missing and needs to be predicted for other instances.
0.851: (needs; to be predicted for; other instances)
0.778: (a property label; is; available)
0.777: (Supervised learning; is; useful)

 Unsupervised learning is useful in cases where the challenge is to discover implicit relationships in a given unlabeled dataset  items are not pre assigned .
0.882: (Unsupervised learning; is useful in; cases where the challenge is to discover implicit relationships in a given unlabeled dataset items are not pre assigned)
0.746: (Unsupervised learning; is; useful)
0.338: (not pre; be implicit relationships in; a given unlabeled dataset items)

 Reinforcement learning falls between these 2 extremes   there is some form of feedback available for each predictive step or action  but no precise label or error message.
No extractions found.

 Since this is an intro class  I didn t learn about reinforcement learning  but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.
0.64: (t; learn about; reinforcement learning)

  1.
No extractions found.

 Decision Trees  A decision tree is a decision support tool that uses a tree like graph or model of decisions and their possible consequences  including chance event outcomes  resource costs  and utility.
0.772: (chance event; outcomes; resource costs and utility)
0.745: (A decision tree; is; a decision support tool that uses a tree like graph or model of decisions and their possible consequences including chance event outcomes resource costs and utility)
0.564: (a tree like graph or model of decisions; be uses by; a decision support tool)

 Take a look at the image to get a sense of how it looks like.
No extractions found.

.
No extractions found.

.
No extractions found.

From a business decision point of view  a decision tree is the minimum number of yes no questions that one has to ask  to assess the probability of making a correct decision  most of the time.
No extractions found.

 As a method  it allows you to approach the problem in a structured and systematic way to arrive at a logical conclusion.
0.846: (you; to approach; the problem)

2.
No extractions found.

 Na ve Bayes Classification  Na ve Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes  theorem with strong  na ve  independence assumptions between the features.
0.908: (Na ve Bayes Classification Na ve Bayes classifiers; are a family of; simple probabilistic classifiers)
0.69: (Bayes theorem; be applying with; strong)

 The featured image is the equation   with P A B  is posterior probability  P B A  is likelihood  P A  is class prior probability  and P B  is predictor prior probability.
0.879: (The featured image; is the equation with; P A B)
0.746: (The featured image; is; the equation)
0.597: (P; is predictor; prior probability)

.
No extractions found.

.
No extractions found.

Some of real world examples are .
No extractions found.

3.
No extractions found.

 Ordinary Least Squares Regression  If you know statistics  you probably have heard of linear regression before.
0.771: (you; probably have heard before of; linear regression)
0.186: (you; know; statistics you probably have heard of linear regression before)

 Least squares is a method for performing linear regression.
0.734: (Least squares; is; a method)

 You can think of linear regression as the task of fitting a straight line through a set of points.
0.914: (You; can think of; linear regression)
0.903: (You; can think as; the task fitting a straight line through a set of points)
0.683: (a straight line; be fitting through; a set of points)

 There are multiple possible strategies to do this  and  ordinary least squares  strategy go like this   You can draw a line  and then for each of the data points  measure the vertical distance between the point and the line  and add these up  the fitted line would be the one where this sum of distances is as small as possible.
0.777: (this sum of distances; is as small as; possible)
0.737: (You; can draw; a line)
0.72: (this sum of distances; is as; small)
0.181: (small; be this sum of; distances)

.
No extractions found.

.
No extractions found.

Linear refers the kind of model you are using to fit the data  while least squares refers to the kind of error metric you are minimizing over.
0.814: (Linear; refers; the kind of model)
0.667: (you; are using; least squares)

4.
No extractions found.

 Logistic Regression  Logistic regression is a powerful statistical way of modeling a binomial outcome with one or more explanatory variables.
0.802: (Logistic Regression Logistic regression; is; a powerful statistical way modeling a binomial outcome with one or more explanatory variables)
0.674: (a binomial outcome; be modeling with; one or more explanatory variables)

 It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function  which is the cumulative logistic distribution.
0.722: (It; measures; the relationship)

.
No extractions found.

.
No extractions found.

In general  regressions can be used in real world applications such as .
No extractions found.

5.
No extractions found.

 Support Vector Machines  SVM is binary classification algorithm.
0.554: (Support Vector Machines SVM; is; binary classification algorithm)

 Given a set of points of 2 types in N dimensional place  SVM generates a  N   1  dimensional hyperlane to separate those points into 2 groups.
0.754: (those points; to be separate into; 2 groups)

 Say you have some points of 2 types in a paper which are linearly separable.
0.906: (you; have; some points of 2 types)
0.772: (you; be some points of; 2 types)

 SVM will find a straight line which separates those points into 2 types and situated as far as possible from all those points.
0.74: (SVM; will find; a straight line which separates those points into 2 types and situated as far as possible from all those points)

.
No extractions found.

.
No extractions found.

In terms of scale  some of the biggest problems that have been solved using SVMs  with suitably modified implementations  are display advertising  human splice site recognition  image based gender detection  large scale image classification.
0.408: (SVMs; be using with; modified implementations)

Pages  1 2.
No extractions found.

